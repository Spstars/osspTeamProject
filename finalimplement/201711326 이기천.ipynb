{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38db4f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-18 13:46:54, start\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "2021-12-18 13:47:21, load data completed\n",
      "2021-12-18 13:47:21, start prepare vali data\n",
      "2021-12-18 13:47:41, prepare validation data, completed\n",
      "loss : tensor(0.8298, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:47:51, start first iteration validation\n",
      "2021-12-18 13:47:51, start first iteration validation\n",
      "2021-12-18 13:47:52, first iteration validation complete\n",
      "2021-12-18 13:47:52: itr0, vali: 1.06316, 0.53017, 0.68534\n",
      "2021-12-18 13:47:52, iteration 0 train complete\n",
      "loss : tensor(0.7839, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:48:06, iteration 1 train complete\n",
      "loss : tensor(0.8681, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:48:11, iteration 2 train complete\n",
      "loss : tensor(0.7994, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:48:16, iteration 3 train complete\n",
      "loss : tensor(0.7224, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:48:29, iteration 4 train complete\n",
      "loss : tensor(0.8823, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:48:33, iteration 5 train complete\n",
      "loss : tensor(0.8623, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:48:47, iteration 6 train complete\n",
      "loss : tensor(0.8351, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:48:54, iteration 7 train complete\n",
      "loss : tensor(0.8304, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:49:04, iteration 8 train complete\n",
      "loss : tensor(0.8689, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:49:10, iteration 9 train complete\n",
      "loss : tensor(0.7425, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:49:21: itr10, vali: 1.06172, 0.58190, 0.73707\n",
      "2021-12-18 13:49:22, iteration 10 train complete\n",
      "loss : tensor(0.8049, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:49:27, iteration 11 train complete\n",
      "loss : tensor(0.9661, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:49:36, iteration 12 train complete\n",
      "loss : tensor(0.8514, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:49:42, iteration 13 train complete\n",
      "loss : tensor(0.8303, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:49:54, iteration 14 train complete\n",
      "loss : tensor(0.8342, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:50:01, iteration 15 train complete\n",
      "loss : tensor(0.8414, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:50:17, iteration 16 train complete\n",
      "loss : tensor(0.7614, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:50:24, iteration 17 train complete\n",
      "loss : tensor(0.9132, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:50:30, iteration 18 train complete\n",
      "loss : tensor(0.8782, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:50:47, iteration 19 train complete\n",
      "loss : tensor(0.8480, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:51:06: itr20, vali: 1.04280, 0.59914, 0.73707\n",
      "2021-12-18 13:51:07, iteration 20 train complete\n",
      "loss : tensor(0.9178, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:51:12, iteration 21 train complete\n",
      "loss : tensor(0.7108, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:51:20, iteration 22 train complete\n",
      "loss : tensor(0.8707, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:51:34, iteration 23 train complete\n",
      "loss : tensor(0.6923, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:51:41, iteration 24 train complete\n",
      "loss : tensor(0.7105, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:51:58, iteration 25 train complete\n",
      "loss : tensor(0.8563, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:52:13, iteration 26 train complete\n",
      "loss : tensor(0.8147, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:52:19, iteration 27 train complete\n",
      "loss : tensor(0.7578, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:52:25, iteration 28 train complete\n",
      "loss : tensor(0.7019, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:52:36, iteration 29 train complete\n",
      "loss : tensor(0.7310, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:52:43: itr30, vali: 1.00322, 0.59914, 0.75000\n",
      "2021-12-18 13:52:44, iteration 30 train complete\n",
      "loss : tensor(0.7174, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:52:56, iteration 31 train complete\n",
      "loss : tensor(0.7019, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:53:02, iteration 32 train complete\n",
      "loss : tensor(0.7830, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:53:17, iteration 33 train complete\n",
      "loss : tensor(0.6994, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:53:25, iteration 34 train complete\n",
      "loss : tensor(0.6957, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:53:41, iteration 35 train complete\n",
      "loss : tensor(0.6691, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:53:47, iteration 36 train complete\n",
      "loss : tensor(0.7104, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:53:52, iteration 37 train complete\n",
      "loss : tensor(0.7525, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:54:06, iteration 38 train complete\n",
      "loss : tensor(0.6292, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:54:12, iteration 39 train complete\n",
      "loss : tensor(0.6950, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:54:28: itr40, vali: 0.99279, 0.59483, 0.74569\n",
      "2021-12-18 13:54:28, iteration 40 train complete\n",
      "loss : tensor(0.7181, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:54:34, iteration 41 train complete\n",
      "loss : tensor(0.6286, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:54:49, iteration 42 train complete\n",
      "loss : tensor(0.6413, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:55:03, iteration 43 train complete\n",
      "loss : tensor(0.5856, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:55:11, iteration 44 train complete\n",
      "loss : tensor(0.6178, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:55:16, iteration 45 train complete\n",
      "loss : tensor(0.6116, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:55:30, iteration 46 train complete\n",
      "loss : tensor(0.6731, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:55:35, iteration 47 train complete\n",
      "loss : tensor(0.7010, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:55:52, iteration 48 train complete\n",
      "loss : tensor(0.5875, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:56:07, iteration 49 train complete\n",
      "loss : tensor(0.6291, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:56:15: itr50, vali: 1.02294, 0.59483, 0.73707\n",
      "2021-12-18 13:56:15, iteration 50 train complete\n",
      "loss : tensor(0.6354, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:56:31, iteration 51 train complete\n",
      "loss : tensor(0.5511, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:56:37, iteration 52 train complete\n",
      "loss : tensor(0.6191, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:56:55, iteration 53 train complete\n",
      "loss : tensor(0.6786, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:57:02, iteration 54 train complete\n",
      "loss : tensor(0.6237, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:57:17, iteration 55 train complete\n",
      "loss : tensor(0.6099, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:57:24, iteration 56 train complete\n",
      "loss : tensor(0.6118, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:57:38, iteration 57 train complete\n",
      "loss : tensor(0.6417, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:57:44, iteration 58 train complete\n",
      "loss : tensor(0.6372, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:57:58, iteration 59 train complete\n",
      "loss : tensor(0.6950, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:58:05: itr60, vali: 1.05376, 0.59914, 0.72845\n",
      "2021-12-18 13:58:05, iteration 60 train complete\n",
      "loss : tensor(0.6374, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:58:19, iteration 61 train complete\n",
      "loss : tensor(0.6011, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:58:29, iteration 62 train complete\n",
      "loss : tensor(0.6031, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:58:46, iteration 63 train complete\n",
      "loss : tensor(0.5845, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:58:53, iteration 64 train complete\n",
      "loss : tensor(0.6724, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:59:05, iteration 65 train complete\n",
      "loss : tensor(0.6303, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:59:11, iteration 66 train complete\n",
      "loss : tensor(0.6808, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:59:24, iteration 67 train complete\n",
      "loss : tensor(0.6052, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:59:29, iteration 68 train complete\n",
      "loss : tensor(0.6326, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:59:43, iteration 69 train complete\n",
      "loss : tensor(0.6377, grad_fn=<DivBackward0>)\n",
      "2021-12-18 13:59:51: itr70, vali: 1.06957, 0.60345, 0.74569\n",
      "2021-12-18 13:59:52, iteration 70 train complete\n",
      "loss : tensor(0.5902, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:00:08, iteration 71 train complete\n",
      "loss : tensor(0.6803, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:00:13, iteration 72 train complete\n",
      "loss : tensor(0.6370, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:00:18, iteration 73 train complete\n",
      "loss : tensor(0.5706, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:00:32, iteration 74 train complete\n",
      "loss : tensor(0.6097, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:00:38, iteration 75 train complete\n",
      "loss : tensor(0.6165, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:00:53, iteration 76 train complete\n",
      "loss : tensor(0.5554, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:00:59, iteration 77 train complete\n",
      "loss : tensor(0.6595, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:01:14, iteration 78 train complete\n",
      "loss : tensor(0.6636, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:01:21, iteration 79 train complete\n",
      "loss : tensor(0.5869, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:01:41: itr80, vali: 1.08821, 0.59914, 0.73276\n",
      "2021-12-18 14:01:41, iteration 80 train complete\n",
      "loss : tensor(0.6100, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:01:46, iteration 81 train complete\n",
      "loss : tensor(0.5986, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:01:51, iteration 82 train complete\n",
      "loss : tensor(0.5922, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:02:05, iteration 83 train complete\n",
      "loss : tensor(0.6017, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:02:25, iteration 84 train complete\n",
      "loss : tensor(0.6465, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:02:32, iteration 85 train complete\n",
      "loss : tensor(0.6099, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:02:37, iteration 86 train complete\n",
      "loss : tensor(0.5767, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:02:53, iteration 87 train complete\n",
      "loss : tensor(0.6039, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:03:10, iteration 88 train complete\n",
      "loss : tensor(0.6908, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:03:17, iteration 89 train complete\n",
      "loss : tensor(0.6550, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:03:23: itr90, vali: 1.10187, 0.59914, 0.74569\n",
      "2021-12-18 14:03:23, iteration 90 train complete\n",
      "loss : tensor(0.6501, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:03:38, iteration 91 train complete\n",
      "loss : tensor(0.6178, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:03:44, iteration 92 train complete\n",
      "loss : tensor(0.6779, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:03:58, iteration 93 train complete\n",
      "loss : tensor(0.5547, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:04:21, iteration 94 train complete\n",
      "loss : tensor(0.6176, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:04:28, iteration 95 train complete\n",
      "loss : tensor(0.6207, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:04:44, iteration 96 train complete\n",
      "loss : tensor(0.6754, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:04:50, iteration 97 train complete\n",
      "loss : tensor(0.6726, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:05:04, iteration 98 train complete\n",
      "loss : tensor(0.5739, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:05:12, iteration 99 train complete\n",
      "loss : tensor(0.5114, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:05:29: itr100, vali: 1.12107, 0.60345, 0.73276\n",
      "2021-12-18 14:05:29, iteration 100 train complete\n",
      "loss : tensor(0.6170, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:05:35, iteration 101 train complete\n",
      "loss : tensor(0.6541, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:05:41, iteration 102 train complete\n",
      "loss : tensor(0.5930, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:05:55, iteration 103 train complete\n",
      "loss : tensor(0.5922, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:06:10, iteration 104 train complete\n",
      "loss : tensor(0.6029, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:06:15, iteration 105 train complete\n",
      "loss : tensor(0.6171, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:06:20, iteration 106 train complete\n",
      "loss : tensor(0.5673, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:06:33, iteration 107 train complete\n",
      "loss : tensor(0.5578, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:06:38, iteration 108 train complete\n",
      "loss : tensor(0.5721, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:06:53, iteration 109 train complete\n",
      "loss : tensor(0.5521, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:07:06: itr110, vali: 1.14512, 0.59483, 0.73276\n",
      "2021-12-18 14:07:06, iteration 110 train complete\n",
      "loss : tensor(0.5738, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:07:11, iteration 111 train complete\n",
      "loss : tensor(0.6774, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:07:16, iteration 112 train complete\n",
      "loss : tensor(0.5610, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:07:27, iteration 113 train complete\n",
      "loss : tensor(0.6099, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:07:32, iteration 114 train complete\n",
      "loss : tensor(0.6206, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:07:45, iteration 115 train complete\n",
      "loss : tensor(0.5673, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:07:50, iteration 116 train complete\n",
      "loss : tensor(0.6060, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:08:04, iteration 117 train complete\n",
      "loss : tensor(0.6332, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:08:09, iteration 118 train complete\n",
      "loss : tensor(0.6656, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:08:22, iteration 119 train complete\n",
      "loss : tensor(0.6867, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:08:29: itr120, vali: 1.15672, 0.59052, 0.72845\n",
      "2021-12-18 14:08:29, iteration 120 train complete\n",
      "loss : tensor(0.5724, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:08:41, iteration 121 train complete\n",
      "loss : tensor(0.5694, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:08:46, iteration 122 train complete\n",
      "loss : tensor(0.5507, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:08:59, iteration 123 train complete\n",
      "loss : tensor(0.6301, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:09:12, iteration 124 train complete\n",
      "loss : tensor(0.6852, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:09:18, iteration 125 train complete\n",
      "loss : tensor(0.5472, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:09:24, iteration 126 train complete\n",
      "loss : tensor(0.5629, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:09:40, iteration 127 train complete\n",
      "loss : tensor(0.6532, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:09:55, iteration 128 train complete\n",
      "loss : tensor(0.6704, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:09:59, iteration 129 train complete\n",
      "loss : tensor(0.5966, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:10:07: itr130, vali: 1.15732, 0.59483, 0.73276\n",
      "2021-12-18 14:10:07, iteration 130 train complete\n",
      "loss : tensor(0.6004, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:10:24, iteration 131 train complete\n",
      "loss : tensor(0.6601, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:10:32, iteration 132 train complete\n",
      "loss : tensor(0.5628, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:10:48, iteration 133 train complete\n",
      "loss : tensor(0.6330, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:10:52, iteration 134 train complete\n",
      "loss : tensor(0.6397, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:11:07, iteration 135 train complete\n",
      "loss : tensor(0.6437, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:11:13, iteration 136 train complete\n",
      "loss : tensor(0.5676, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:11:30, iteration 137 train complete\n",
      "loss : tensor(0.6760, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:11:35, iteration 138 train complete\n",
      "loss : tensor(0.6337, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:11:52, iteration 139 train complete\n",
      "loss : tensor(0.5925, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:11:57: itr140, vali: 1.17359, 0.59483, 0.73707\n",
      "2021-12-18 14:11:57, iteration 140 train complete\n",
      "loss : tensor(0.6240, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:12:11, iteration 141 train complete\n",
      "loss : tensor(0.6009, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:12:17, iteration 142 train complete\n",
      "loss : tensor(0.5827, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:12:31, iteration 143 train complete\n",
      "loss : tensor(0.5754, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:12:36, iteration 144 train complete\n",
      "loss : tensor(0.5688, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:12:40, iteration 145 train complete\n",
      "loss : tensor(0.4936, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:12:55, iteration 146 train complete\n",
      "loss : tensor(0.5685, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:13:08, iteration 147 train complete\n",
      "loss : tensor(0.5855, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:13:13, iteration 148 train complete\n",
      "loss : tensor(0.6175, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:13:26, iteration 149 train complete\n",
      "loss : tensor(0.5317, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:13:31: itr150, vali: 1.20178, 0.59052, 0.73276\n",
      "2021-12-18 14:13:31, iteration 150 train complete\n",
      "loss : tensor(0.6713, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:13:45, iteration 151 train complete\n",
      "loss : tensor(0.6587, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:13:51, iteration 152 train complete\n",
      "loss : tensor(0.6390, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:14:08, iteration 153 train complete\n",
      "loss : tensor(0.6559, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:14:13, iteration 154 train complete\n",
      "loss : tensor(0.6383, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:14:20, iteration 155 train complete\n",
      "loss : tensor(0.6710, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:14:37, iteration 156 train complete\n",
      "loss : tensor(0.6423, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:14:43, iteration 157 train complete\n",
      "loss : tensor(0.5820, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:14:59, iteration 158 train complete\n",
      "loss : tensor(0.5648, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:15:03, iteration 159 train complete\n",
      "loss : tensor(0.6151, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:15:20: itr160, vali: 1.18967, 0.59483, 0.75862\n",
      "2021-12-18 14:15:21, iteration 160 train complete\n",
      "loss : tensor(0.5452, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:15:25, iteration 161 train complete\n",
      "loss : tensor(0.5430, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:15:41, iteration 162 train complete\n",
      "loss : tensor(0.5886, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:15:55, iteration 163 train complete\n",
      "loss : tensor(0.5289, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:15:59, iteration 164 train complete\n",
      "loss : tensor(0.6329, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:16:13, iteration 165 train complete\n",
      "loss : tensor(0.6285, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:16:20, iteration 166 train complete\n",
      "loss : tensor(0.5736, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:16:37, iteration 167 train complete\n",
      "loss : tensor(0.5686, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:16:44, iteration 168 train complete\n",
      "loss : tensor(0.5799, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:16:50, iteration 169 train complete\n",
      "loss : tensor(0.7036, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:17:04: itr170, vali: 1.19601, 0.59052, 0.75000\n",
      "2021-12-18 14:17:04, iteration 170 train complete\n",
      "loss : tensor(0.5828, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:17:09, iteration 171 train complete\n",
      "loss : tensor(0.6278, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:17:22, iteration 172 train complete\n",
      "loss : tensor(0.6123, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:17:27, iteration 173 train complete\n",
      "loss : tensor(0.5795, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:17:42, iteration 174 train complete\n",
      "loss : tensor(0.6793, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:17:47, iteration 175 train complete\n",
      "loss : tensor(0.5760, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:18:03, iteration 176 train complete\n",
      "loss : tensor(0.6152, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:18:07, iteration 177 train complete\n",
      "loss : tensor(0.5535, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:18:24, iteration 178 train complete\n",
      "loss : tensor(0.5810, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:18:30, iteration 179 train complete\n",
      "loss : tensor(0.5621, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:18:47: itr180, vali: 1.20021, 0.59914, 0.75000\n",
      "2021-12-18 14:18:47, iteration 180 train complete\n",
      "loss : tensor(0.6288, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:18:53, iteration 181 train complete\n",
      "loss : tensor(0.6278, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:18:58, iteration 182 train complete\n",
      "loss : tensor(0.6699, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:19:11, iteration 183 train complete\n",
      "loss : tensor(0.6231, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:19:16, iteration 184 train complete\n",
      "loss : tensor(0.6319, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:19:31, iteration 185 train complete\n",
      "loss : tensor(0.5852, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:19:36, iteration 186 train complete\n",
      "loss : tensor(0.6773, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:19:48, iteration 187 train complete\n",
      "loss : tensor(0.5363, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:19:53, iteration 188 train complete\n",
      "loss : tensor(0.6811, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:19:57, iteration 189 train complete\n",
      "loss : tensor(0.6108, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:20:04: itr190, vali: 1.20909, 0.59914, 0.75000\n",
      "2021-12-18 14:20:04, iteration 190 train complete\n",
      "loss : tensor(0.6490, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:20:09, iteration 191 train complete\n",
      "loss : tensor(0.5672, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:20:16, iteration 192 train complete\n",
      "loss : tensor(0.7167, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:20:22, iteration 193 train complete\n",
      "loss : tensor(0.5535, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:20:37, iteration 194 train complete\n",
      "loss : tensor(0.6115, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:20:42, iteration 195 train complete\n",
      "loss : tensor(0.6201, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:20:57, iteration 196 train complete\n",
      "loss : tensor(0.6348, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:21:01, iteration 197 train complete\n",
      "loss : tensor(0.6571, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:21:06, iteration 198 train complete\n",
      "loss : tensor(0.6484, grad_fn=<DivBackward0>)\n",
      "2021-12-18 14:21:16, iteration 199 train complete\n",
      "2021-12-18 14:21:16, data end\n",
      "2021-12-18 14:21:16, start prepare test data\n",
      "test!!!loss!!!, test: 0.74454, vali: 0.99279\n",
      "test!!!pre1!!!, test: 0.67624, vali: 0.60345\n",
      "test!!!pre2!!!, test: 0.83859, vali: 0.75862\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "import random\n",
    "from past.builtins import xrange\n",
    "import pickle \n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import threading\n",
    "sys.path.append('../../')\n",
    "# almost similar to the original implementations\n",
    "\n",
    "losslist=[]\n",
    "\n",
    "class Dataset():\n",
    "    \"\"\"docstring for Dataset\"\"\"\n",
    "    def __init__(self,train_data=True,userhistory=[]):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.dataset = \"mv\"\n",
    "        self.model_type = \"PW\"\n",
    "        self.band_size = 20\n",
    "        #load the data\n",
    "        #data_filename = os.path.join(args.data_folder, args.dataset+'.pkl')\n",
    "        if train_data:\n",
    "            f = open(\"movie.pkl\", 'rb')\n",
    "            data_behavior = pickle.load(f) # time and user behavior\n",
    "            del data_behavior[0]\n",
    "            tmp=[]\n",
    "            # usernum=300\n",
    "            # for i in range(usernum):\n",
    "            #     tmp.append(data_behavior[i])\n",
    "            f.close()\n",
    "        else:\n",
    "            tmp=userhistory\n",
    "\n",
    "        data_behavior=tmp\n",
    "        #실구현시에는 movie의 개수임.\n",
    "        #그냥 movie를 원핫 벡터화했는데요..? 그러니까 movie1=[1.0,,0,0,0,0],movie2=[0.1,,0,0,0,0]\n",
    "        #item_feature = pickle.load(f) # identity matrix\n",
    "        item_feature = np.eye(3952)\n",
    "\n",
    "\n",
    "        self.size_item = len(item_feature)\n",
    "        self.size_user = len(data_behavior)\n",
    "        self.f_dim = len(item_feature[0])\n",
    "\n",
    "        # load the index fo train,test,valid split \n",
    "        if train_data:\n",
    "            self.train_user = range(int(usernum*0.7))\n",
    "            self.vali_user = range(int(usernum*0.7),int(usernum*0.8))\n",
    "            self.test_user = range(int(usernum*0.8),usernum)\n",
    "        else:\n",
    "            self.train_user = 0\n",
    "            self.vali_user = 0\n",
    "            self.test_user = 0\n",
    "\n",
    "        # process the data\n",
    "\n",
    "        # get the most no of suggetion for an individual at a time\n",
    "        # 영화 추천한 집합 중에서 가장 긴 것을 kmax라 한다.\n",
    "        k_max = 0\n",
    "        for d_b in data_behavior:\n",
    "            for disp in d_b[1]:\n",
    "                k_max = max(k_max, len(disp))\n",
    "\n",
    "        self.data_click = [[] for x in xrange(self.size_user)]\n",
    "        #유저가 본 영화 목록(중복을 없애서)\n",
    "        self.data_disp = [[] for x in xrange(self.size_user)]\n",
    "        self.data_time = np.zeros(self.size_user, dtype=np.int)\n",
    "        self.data_news_cnt = np.zeros(self.size_user, dtype=np.int)\n",
    "        self.feature = [[] for x in xrange(self.size_user)]\n",
    "        self.feature_click = [[] for x in xrange(self.size_user)]\n",
    "            #유저 100명에 대해서\n",
    "        for user in xrange(self.size_user):\n",
    "            print(user)\n",
    "            # (1) count number of clicks\n",
    "            click_t = 0\n",
    "            num_events = len(data_behavior[user][1])\n",
    "            click_t += num_events\n",
    "            self.data_time[user] = click_t\n",
    "            # (2)\n",
    "            #news_dict에는, movie가 인덱싱 되어있음\n",
    "            news_dict = {}\n",
    "            #feature click에 있는 리스트의 size는 [click_t,self.f_dim]이다(클릭이 15, f_dim이 3952)\n",
    "            self.feature_click[user] = np.zeros([click_t, self.f_dim])\n",
    "            click_t = 0\n",
    "            for event in xrange(num_events):\n",
    "                #그러니까, list에 있는 것중에 실제로 뽑힌 것을 찾는 과정임\n",
    "                disp_list = data_behavior[user][1][event]\n",
    "                #print(len(disp_list))\n",
    "                #[1198, 1210, 1217, 2717, 1293]\n",
    "                pick_id = data_behavior[user][2][event]\n",
    "                #뭐 뽑았니\n",
    "                #print(pick_id)  1293\n",
    "\n",
    "                for id in disp_list:\n",
    "                    #movie의 인덱싱\n",
    "                    if id not in news_dict:\n",
    "                        news_dict[id] = len(news_dict)  # for each user, movie id start from 0\n",
    "                if pick_id:\n",
    "                    id = pick_id\n",
    "                    #pick_id가 있다면(무조건 있음) 유저에, click_t(idx),news_dict(새로 인덱싱한거)\n",
    "                    self.data_click[user].append([click_t, news_dict[id]])\n",
    "                    self.feature_click[user][click_t] = item_feature[id-1]\n",
    "                for idd in disp_list:\n",
    "                    self.data_disp[user].append([click_t, news_dict[idd]])\n",
    "                click_t += 1  # splitter a event with 2 clickings to 2 events\n",
    "            #data_news_cnt[useridx]에는 disp_list의 총합이되, 중복제거\n",
    "            self.data_news_cnt[user] = len(news_dict)\n",
    "            #print(\"news_dict\",len(news_dict))\n",
    "            # feature[user]=유저가 본 무비데이터xf_dim\n",
    "            self.feature[user] = np.zeros([self.data_news_cnt[user], self.f_dim])\n",
    "            #item_feature에서 값 찾아서 movie매칭\n",
    "            #[user][news_dict[id](id들어온대로 idx한거)]\n",
    "            for id in news_dict:\n",
    "                self.feature[user][news_dict[id]] = item_feature[id-1]\n",
    "            self.feature[user] = self.feature[user].tolist()\n",
    "            self.feature_click[user] = self.feature_click[user].tolist()\n",
    "        self.max_disp_size = k_max\n",
    "        \n",
    "    def random_split_user(self):\n",
    "        # dont think this one is really necessary if the initial split is random enough\n",
    "        num_users = len(self.train_user) + len(self.vali_user) + len(self.test_user)\n",
    "        shuffle_order = np.arange(num_users)\n",
    "        np.random.shuffle(shuffle_order)\n",
    "        self.train_user = shuffle_order[0:len(self.train_user)].tolist()\n",
    "        self.vali_user = shuffle_order[len(self.train_user):len(self.train_user)+len(self.vali_user)].tolist()\n",
    "        self.test_user = shuffle_order[len(self.train_user)+len(self.vali_user):].tolist()\n",
    "\n",
    "    def data_process_for_placeholder(self, user_set):\n",
    "        #print (\"user_set\",user_set)\n",
    "        if self.model_type == 'PW':\n",
    "            \n",
    "            sec_cnt_x = 0\n",
    "            news_cnt_short_x = 0\n",
    "            news_cnt_x = 0\n",
    "            click_2d_x = []\n",
    "            disp_2d_x = []\n",
    "\n",
    "            tril_indice = []\n",
    "            tril_value_indice = []\n",
    "\n",
    "            disp_2d_split_sec = []\n",
    "            feature_clicked_x = []\n",
    "\n",
    "            disp_current_feature_x = []\n",
    "            click_sub_index_2d = []\n",
    "\n",
    "            # started with the validation set\n",
    "            #[703, 713, 723, 733, 743, 753, 763, 773, 783, 793, 803, 813, 823, 833, 843, 853, 863, 873, 883, 893, 903, 913, 923, 933, 943, 953, 963, 973, 983, 993, 1003, 1013, 1023, 1033, 1043, 1053]\n",
    "            #user_set = [703]\n",
    "            for u in user_set:\n",
    "                t_indice = []\n",
    "                #print(self.data_time[u]-1)\n",
    "                #19랑 얼마나 클릭했냐를 체크하여 짧은 쪽 선택\n",
    "                #데이터가 19넘으면 자르고, 19이하면 채우는 매커니즘\n",
    "                for kk in xrange(min(self.band_size-1, self.data_time[u]-1)):\n",
    "                    #(데이터 시간[u]-(kk+1))를 lambda로 반환\n",
    "                    # 0,kk=0이면 (0+0+1,0)\n",
    "                    #[1,0]....[6,0..]\n",
    "                    t_indice += map(lambda x: [x + kk+1 + sec_cnt_x, x + sec_cnt_x], np.arange(self.data_time[u] - (kk+1)))\n",
    "              #  for t in range(len(t_indice)):\n",
    "              #      print(t_indice[t])\n",
    "              #  print (len(t_indice)) #[] for 703\n",
    "              #  quit()\n",
    "                tril_indice += t_indice\n",
    "                tril_value_indice += map(lambda x: (x[0] - x[1] - 1), t_indice)\n",
    "               # print (\"THE Click data is \",self.data_click[u]) #THE Click data is  [[0, 0], [1, 8], [2, 14]] for u =15\n",
    "                click_2d_tmp = map(lambda x: [x[0] + sec_cnt_x, x[1]], self.data_click[u])\n",
    "                click_2d_tmp = list(click_2d_tmp)\n",
    "               # print (len(list(click_2d_tmp)))\n",
    "                #print (list(click_2d_tmp))\n",
    "                click_2d_x += click_2d_tmp\n",
    "                #print (\"tenp is \",click_2d_x,list(click_2d_tmp))  # [[0, 0], [1, 8], [2, 14]] for u15\n",
    "                #print (\"dispaly data is \", self.data_disp[u])\n",
    "                #[0,0]\n",
    "                \n",
    "\n",
    "                disp_2d_tmp = map(lambda x: [x[0] + sec_cnt_x, x[1]], self.data_disp[u])\n",
    "                disp_2d_tmp = list(disp_2d_tmp)\n",
    "                #y=[]\n",
    "                #y+=disp_2d_tmp\n",
    "                #print (disp_2d_tmp, click_2d_tmp)\n",
    "                click_sub_index_tmp = map(lambda x: disp_2d_tmp.index(x), (click_2d_tmp))\n",
    "                click_sub_index_tmp = list(click_sub_index_tmp)\n",
    "                #print (\"the mess is \",click_sub_index_tmp)\n",
    "                click_sub_index_2d += map(lambda x: x+len(disp_2d_x), click_sub_index_tmp)\n",
    "                #print (\"click_sub_index_2d\",click_sub_index_2d)\n",
    "                disp_2d_x += disp_2d_tmp\n",
    "                #print (\"disp_2d_x\",disp_2d_x) # [[0, 0]]\n",
    "                #sys.exit()\n",
    "                disp_2d_split_sec += map(lambda x: x[0] + sec_cnt_x, self.data_disp[u])\n",
    "\n",
    "                sec_cnt_x += self.data_time[u]\n",
    "                news_cnt_short_x = max(news_cnt_short_x, self.data_news_cnt[u])\n",
    "                news_cnt_x += self.data_news_cnt[u]\n",
    "                disp_current_feature_x += map(lambda x: self.feature[u][x], [idd[1] for idd in self.data_disp[u]])\n",
    "                feature_clicked_x += self.feature_click[u]\n",
    "\n",
    "                out1 ={}\n",
    "                out1['click_2d_x']=click_2d_x\n",
    "                out1['disp_2d_x']=disp_2d_x\n",
    "                out1['disp_current_feature_x']=disp_current_feature_x\n",
    "                out1['sec_cnt_x']=sec_cnt_x\n",
    "                out1['tril_indice']=tril_indice\n",
    "                out1['tril_value_indice']=tril_value_indice\n",
    "                out1['disp_2d_split_sec']=disp_2d_split_sec\n",
    "                out1['news_cnt_short_x']=news_cnt_short_x\n",
    "                out1['click_sub_index_2d']=click_sub_index_2d\n",
    "                out1['feature_clicked_x']=feature_clicked_x\n",
    "            # print (\"out\",out1['tril_value_indice'])\n",
    "#             # sys.exit()\n",
    "#             with open('user.pickle','wb') as fw:\n",
    "#                 pickle.dump(out1, fw)\n",
    "            return out1\n",
    "\n",
    "\n",
    "    def prepare_validation_data(self, num_sets, v_user):\n",
    "\n",
    "        if self.model_type == 'PW':\n",
    "            vali_thread_u = [[] for _ in xrange(num_sets)]\n",
    "            click_2d_v = [[] for _ in xrange(num_sets)]\n",
    "            disp_2d_v = [[] for _ in xrange(num_sets)]\n",
    "            feature_v = [[] for _ in xrange(num_sets)]\n",
    "            sec_cnt_v = [[] for _ in xrange(num_sets)]\n",
    "            tril_ind_v = [[] for _ in xrange(num_sets)]\n",
    "            tril_value_ind_v = [[] for _ in xrange(num_sets)]\n",
    "            disp_2d_split_sec_v = [[] for _ in xrange(num_sets)]\n",
    "            feature_clicked_v = [[] for _ in xrange(num_sets)]\n",
    "            news_cnt_short_v = [[] for _ in xrange(num_sets)]\n",
    "            click_sub_index_2d_v = [[] for _ in xrange(num_sets)]\n",
    "            for ii in xrange(len(v_user)):\n",
    "                vali_thread_u[ii % num_sets].append(v_user[ii])\n",
    "            for ii in xrange(num_sets):\n",
    "                out=self.data_process_for_placeholder(vali_thread_u[ii])\n",
    "                # print (\"out_val\",out['tril_indice'])\n",
    "                # sys.exit()\n",
    "\n",
    "                click_2d_v[ii], disp_2d_v[ii], feature_v[ii], sec_cnt_v[ii], tril_ind_v[ii], tril_value_ind_v[ii], \\\n",
    "                disp_2d_split_sec_v[ii], news_cnt_short_v[ii], click_sub_index_2d_v[ii], feature_clicked_v[ii] = out['click_2d_x'], \\\n",
    "                out['disp_2d_x'], \\\n",
    "                out['disp_current_feature_x'], \\\n",
    "                out['sec_cnt_x'], \\\n",
    "                out['tril_indice'], \\\n",
    "                out['tril_value_indice'], \\\n",
    "                out['disp_2d_split_sec'], \\\n",
    "                out['news_cnt_short_x'], \\\n",
    "                out['click_sub_index_2d'], \\\n",
    "                out['feature_clicked_x']\n",
    "\n",
    "            out2={}\n",
    "            out2['vali_thread_u']=vali_thread_u \n",
    "            out2['click_2d_v']=click_2d_v \n",
    "            out2['disp_2d_v']=disp_2d_v \n",
    "            out2['feature_v']=feature_v \n",
    "            out2['sec_cnt_v']=sec_cnt_v \n",
    "            out2['tril_ind_v']=tril_ind_v \n",
    "            out2['tril_value_ind_v']=tril_value_ind_v \n",
    "            out2['disp_2d_split_sec_v']=disp_2d_split_sec_v \n",
    "            out2['news_cnt_short_v']=news_cnt_short_v \n",
    "            out2['click_sub_index_2d_v']=click_sub_index_2d_v \n",
    "            out2['feature_clicked_v']=feature_clicked_v\n",
    "            return out2\n",
    "\n",
    "class UserModelPW(nn.Module):\n",
    "    \"\"\"docstring for UserModelPW\"\"\"\n",
    "\n",
    "    def __init__(self, f_dim):\n",
    "        super(UserModelPW, self).__init__()\n",
    "        self.f_dim = 3952\n",
    "        # self.placeholder = {}\n",
    "        self.hidden_dims = '64-64'\n",
    "        self.lr = 0.001\n",
    "        self.pw_dim = 4\n",
    "        self.band_size = 20\n",
    "        self.mlp_model = self.mlp(19760, self.hidden_dims, 1, 1e-3, act_last=False)\n",
    "        self.u_disps=[]\n",
    "\n",
    "    def mlp(self, x_shape, hidden_dims, output_dim, sd, act_last=False):\n",
    "        hidden_dims = tuple(map(int, hidden_dims.split(\"-\")))\n",
    "        # print (\"hidden_dims\",hidden_dims)\n",
    "        # print (\"imp is\",x)\n",
    "        # print (x.shape,x.dtype)\n",
    "        cur = x_shape\n",
    "        main_mod = nn.Sequential()\n",
    "        for i, h in enumerate(hidden_dims):\n",
    "            main_mod.add_module('Linear-{0}'.format(i), torch.nn.Linear(cur, h))\n",
    "            main_mod.add_module('act-{0}'.format(i), nn.ELU())\n",
    "            cur = h\n",
    "\n",
    "        if act_last:\n",
    "            main_mod.add_module(\"Linear_last\", torch.nn.Linear(cur, output_dim))\n",
    "            main_mod.add_module(\"act_last\", nn.ELU())\n",
    "            return main_mod\n",
    "        else:\n",
    "            main_mod.add_module(\"linear_last\", torch.nn.Linear(cur, output_dim))\n",
    "            return main_mod\n",
    "\n",
    "    def forward(self, inputs, is_train=False, index=None):\n",
    "        # input is a dictionaty \n",
    "        if is_train == True:\n",
    "\n",
    "            disp_current_feature = torch.tensor(inputs['disp_current_feature_x'])\n",
    "            Xs_clicked = torch.tensor(inputs['feature_clicked_x'])\n",
    "            item_size = torch.tensor(inputs['news_cnt_short_x'])\n",
    "            section_length = torch.tensor(inputs['sec_cnt_x'])\n",
    "            click_values = torch.tensor(np.ones(len(inputs['click_2d_x']), dtype=np.float32))\n",
    "            click_indices = torch.tensor(inputs['click_2d_x'])\n",
    "            disp_indices = torch.tensor(np.array(np.int64(inputs['disp_2d_x'])))\n",
    "            disp_2d_split_sec_ind = torch.tensor(inputs['disp_2d_split_sec'])\n",
    "            cumsum_tril_indices = torch.tensor(np.int64(inputs['tril_indice']))\n",
    "            cumsum_tril_value_indices = torch.tensor(np.array(inputs['tril_value_indice'], dtype=np.int64))\n",
    "            click_2d_subindex = torch.tensor(inputs['click_sub_index_2d'])\n",
    "\n",
    "        else:\n",
    "            # define the inputs for val/tst here\n",
    "            # print (\"input_val\",inputs)\n",
    "            disp_current_feature = torch.tensor(inputs['feature_v'][index])\n",
    "            Xs_clicked = torch.tensor(inputs['feature_clicked_v'][index])\n",
    "            item_size = torch.tensor(inputs['news_cnt_short_v'][index])\n",
    "            section_length = torch.tensor(inputs['sec_cnt_v'][index])\n",
    "            click_values = torch.tensor(np.ones(len(inputs['click_2d_v'][index]), dtype=np.float32))\n",
    "            click_indices = torch.tensor(inputs['click_2d_v'][index])\n",
    "            disp_indices = torch.tensor(np.array(np.int64(inputs['disp_2d_v'][index])))\n",
    "            disp_2d_split_sec_ind = torch.tensor(inputs['disp_2d_split_sec_v'][index])\n",
    "            cumsum_tril_indices = torch.tensor(np.int64(inputs['tril_ind_v'][index]))\n",
    "            cumsum_tril_value_indices = torch.tensor(np.array(inputs['tril_value_ind_v'][index], dtype=np.int64))\n",
    "            click_2d_subindex = torch.tensor(inputs['click_sub_index_2d_v'][index])\n",
    "\n",
    "        #feature_v가 9352 torch.Size([9352, 3952])\n",
    "        #유저들이 간단히 생각해서 본 영화의 총합이 9352개라는 뜻 (유저끼리는 중복있음)\n",
    "       # print(disp_current_feature.shape)\n",
    "        #2390,3952  torch.Size([2390, 3952])\n",
    "       # print(Xs_clicked.shape)\n",
    "        #null\n",
    "       # print(item_size.shape)\n",
    "        #null\n",
    "       # print(section_length.shape)\n",
    "        #click value torch.Size([2390])\n",
    "       # print(click_values.shape)\n",
    "        #torch.Size([9352, 2])\n",
    "       # print(disp_indices.shape)\n",
    "        #torch.Size([9352, 3952])\n",
    "       # print(disp_2d_split_sec_ind.shape)\n",
    "       # torch.Size([33865, 2])\n",
    "       # print(cumsum_tril_indices.shape)\n",
    "        #u_disp.shape torch.Size([9352, 1])\n",
    "       # print(cumsum_tril_value_indices.shape)\n",
    "        denseshape = [section_length, item_size]  # this wont work\n",
    "\n",
    "        click_history = [[] for _ in xrange(self.pw_dim)]\n",
    "      #  print(\"slice\")\n",
    "        # pw\n",
    "        for ii in xrange(self.pw_dim):\n",
    "            position_weight = torch.ones(size=[self.band_size]).to(dtype=torch.float64) * 0.0001\n",
    "            # print (position_weight,cumsum_tril_value_indices)\n",
    "\n",
    "            cumsum_tril_value = position_weight[cumsum_tril_value_indices]\n",
    "            # tf.gather(position_weight, self.placeholder['cumsum_tril_value_indices'])\n",
    "          #  print (\"cumsum_tril_indices\",cumsum_tril_indices)\n",
    "           # print (\"cumsum_tril_value\",cumsum_tril_value)\n",
    "\n",
    "            cumsum_tril_matrix = torch.sparse.FloatTensor(cumsum_tril_indices.t(), cumsum_tril_value,\n",
    "                                                          [section_length, section_length]).to_dense()\n",
    "            # print (\"cumsum_tril_matrix\",cumsum_tril_matrix)\n",
    "            # print (\"Xs_clicked\",Xs_clicked.dtype)\n",
    "            # feature 행렬곱하는 부분\n",
    "\n",
    "            click_history[ii] = torch.matmul(cumsum_tril_matrix, Xs_clicked.to(dtype=torch.float64))  # Xs_clicked: section by _f_dim\n",
    "\n",
    "        concat_history = torch.cat(click_history, axis=1)\n",
    "        #print(\"history: \",concat_history,concat_history.shape)\n",
    "        #[1,x]\n",
    "        disp_history_feature = concat_history[disp_2d_split_sec_ind]\n",
    "        \n",
    "        # (4) combine features\n",
    "        #[갯수,19760]\n",
    "        concat_disp_features = torch.reshape(torch.cat([disp_history_feature, disp_current_feature], axis=1),\n",
    "                                             [-1, self.f_dim * self.pw_dim + self.f_dim])\n",
    "\n",
    "        #  print(len(concat_disp_features))\n",
    "       # print(\"concat \",concat_disp_features.shape)\n",
    "        # (5) compute utility\n",
    "        # print (\"the in pu t shape s \",concat_disp_features.shape)\n",
    "        # reward\n",
    "        u_disp = self.mlp_model(concat_disp_features.float())\n",
    "       # print(\"u_disp \",u_disp.shape)\n",
    "        # net.apply(init_weights,sdv)\n",
    "        # (5) 백터로 나옴. 값은\n",
    "        # user가 본 영화 길이가 나오는듯\n",
    "        #print(u_disp)\n",
    "        exp_u_disp = torch.exp(u_disp)\n",
    "        #reward [1, 2,3,4,5]\n",
    "        #[e^1,e^2,e^3,e^4,e^5]     [[1,2,3/4,5,6]]\n",
    "        # #torch exp\n",
    "        # [1,1,2,1,1, 1,1,1,1, 1,1,1]\n",
    "        # [0,0,0,0,0  1 1 1 1 ,2,2,2]->[5,4,3]\n",
    "       # print(disp_2d_split_sec_ind)\n",
    "        sum_exp_disp_ubar_ut = segment_sum(exp_u_disp, disp_2d_split_sec_ind)\n",
    "        #U_disp에 segment_sum을 함으로서, \n",
    "      #  print (\"sum_exp_disp_ubar_ut \",sum_exp_disp_ubar_ut)\n",
    "       # print(click_2d_subindex)\n",
    "        sum_click_u_bar_ut = u_disp[click_2d_subindex]\n",
    "        #reward에서 subindex만을 고른다(실제 선택한 값의 reward)\n",
    "       # print(sum_click_u_bar_ut)\n",
    "        # (6) loss and precision\n",
    "        #print (\"click_values\",click_values.shape)\n",
    "        #print (\"click_indices\",click_indices.shape)\n",
    "       # print (\"click_indices\",click_indices)\n",
    "      #  print (\"denseshape\",denseshape.shape)\n",
    "        #error size 1\n",
    "        click_tensor = torch.sparse.FloatTensor(click_indices.t(), click_values, denseshape).to_dense()\n",
    "        #print(\"click tensor\",click_tensor.shape)\n",
    "        click_cnt = click_tensor.sum(1)\n",
    "        #reward\n",
    "        loss_sum = torch.sum(- sum_click_u_bar_ut + torch.log(sum_exp_disp_ubar_ut ))#+1\n",
    "        #\n",
    "        event_cnt = torch.sum(click_cnt)\n",
    "        loss = loss_sum / event_cnt\n",
    "       # print(torch.log(sum_exp_disp_ubar_ut + 1))\n",
    "\n",
    "        exp_disp_ubar_ut = torch.sparse.FloatTensor(disp_indices.t(), torch.reshape(exp_u_disp, (-1,)), denseshape)\n",
    "        \n",
    "        #print(\"exp_disp_ubar_ut\",exp_disp_ubar_ut.shape)\n",
    "        dense_exp_disp_util = exp_disp_ubar_ut.to_dense()\n",
    "        argmax_click = torch.argmax(click_tensor, dim=1)\n",
    "        argmax_disp = torch.argmax(dense_exp_disp_util, dim=1)\n",
    "        # 최고 2개 리턴\n",
    "        top_2_disp = torch.topk(dense_exp_disp_util, k=2, sorted=False)[1]\n",
    "\n",
    "        # print (\"argmax_click\",argmax_click.shape)\n",
    "        # for i in argmax_click:\n",
    "        #     print(i)\n",
    "        # print (\"argmax_disp\",argmax_disp.shape)\n",
    "        # print (\"argmax_disp\",argmax_disp)\n",
    "        # (\"top_2_disp\",top_2_disp,top_2_disp.shape)\n",
    "        #print(\"argmax_disp\",argmax_disp,argmax_disp.shape)\n",
    "        #sys.exit()\n",
    "        precision_1_sum = torch.sum((torch.eq(argmax_click, argmax_disp)))\n",
    "        precision_1 = precision_1_sum / event_cnt\n",
    "\n",
    "        precision_2_sum = (torch.eq(argmax_click[:, None].to(torch.int64), top_2_disp.to(torch.int64))).sum()\n",
    "        precision_2 = precision_2_sum / event_cnt\n",
    "\n",
    "        # self.lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * 0.05  # regularity\n",
    "        # weight decay can be added in the optimizer for l2 decay\n",
    "        return loss, precision_1, precision_2, loss_sum, precision_1_sum, precision_2_sum, event_cnt, u_disp,concat_disp_features\n",
    "    def embedding(self, inputs, is_train=False, index=None):\n",
    "        # input is a dictionaty \n",
    "        if is_train == True:\n",
    "\n",
    "            disp_current_feature = torch.tensor(inputs['disp_current_feature_x'])\n",
    "            Xs_clicked = torch.tensor(inputs['feature_clicked_x'])\n",
    "            item_size = torch.tensor(inputs['news_cnt_short_x'])\n",
    "            section_length = torch.tensor(inputs['sec_cnt_x'])\n",
    "            \n",
    "            disp_2d_split_sec_ind = torch.tensor(inputs['disp_2d_split_sec'])\n",
    "            cumsum_tril_indices = torch.tensor(np.int64(inputs['tril_indice']))\n",
    "            cumsum_tril_value_indices = torch.tensor(np.array(inputs['tril_value_indice'], dtype=np.int64))\n",
    "            click_2d_subindex = torch.tensor(inputs['click_sub_index_2d'])\n",
    "\n",
    "        else:\n",
    "            # define the inputs for val/tst here\n",
    "            # print (\"input_val\",inputs)\n",
    "            disp_current_feature = torch.tensor(inputs['feature_v'][index])\n",
    "            Xs_clicked = torch.tensor(inputs['feature_clicked_v'][index])\n",
    "            item_size = torch.tensor(inputs['news_cnt_short_v'][index])\n",
    "            section_length = torch.tensor(inputs['sec_cnt_v'][index])\n",
    "            disp_2d_split_sec_ind = torch.tensor(inputs['disp_2d_split_sec_v'][index])\n",
    "            cumsum_tril_indices = torch.tensor(np.int64(inputs['tril_ind_v'][index]))\n",
    "            cumsum_tril_value_indices = torch.tensor(np.array(inputs['tril_value_ind_v'][index], dtype=np.int64))\n",
    "            click_2d_subindex = torch.tensor(inputs['click_sub_index_2d_v'][index])\n",
    "\n",
    "        #feature_v가 9352 torch.Size([9352, 3952])\n",
    "        #유저들이 간단히 생각해서 본 영화의 총합이 9352개라는 뜻 (유저끼리는 중복있음)\n",
    "       # print(disp_current_feature.shape)\n",
    "        #2390,3952  torch.Size([2390, 3952])\n",
    "       # print(Xs_clicked.shape)\n",
    "        #null\n",
    "       # print(item_size.shape)\n",
    "        #null\n",
    "       # print(section_length.shape)\n",
    "        #click value torch.Size([2390])\n",
    "       # print(click_values.shape)\n",
    "        #torch.Size([9352, 2])\n",
    "        #print(disp_indices.shape)\n",
    "        #torch.Size([9352, 3952])\n",
    "        #print(disp_2d_split_sec_ind.shape)\n",
    "        #torch.Size([33865, 2])\n",
    "        #print(cumsum_tril_indices.shape)\n",
    "        #u_disp.shape torch.Size([9352, 1])\n",
    "        #print(cumsum_tril_value_indices.shape)\n",
    "        denseshape = [section_length, item_size]  # this wont work\n",
    "\n",
    "        click_history = [[] for _ in xrange(self.pw_dim)]\n",
    "\n",
    "        # pw\n",
    "        for ii in xrange(self.pw_dim):\n",
    "            position_weight = torch.ones(size=[self.band_size]).to(dtype=torch.float64) * 0.0001\n",
    "            # print (position_weight,cumsum_tril_value_indices)\n",
    "\n",
    "            cumsum_tril_value = position_weight[cumsum_tril_value_indices]\n",
    "            # tf.gather(position_weight, self.placeholder['cumsum_tril_value_indices'])\n",
    "          #  print (\"cumsum_tril_indices\",cumsum_tril_indices)\n",
    "           # print (\"cumsum_tril_value\",cumsum_tril_value)\n",
    "\n",
    "            cumsum_tril_matrix = torch.sparse.FloatTensor(cumsum_tril_indices.t(), cumsum_tril_value,\n",
    "                                                          [section_length, section_length]).to_dense()\n",
    "            # print (\"cumsum_tril_matrix\",cumsum_tril_matrix)\n",
    "            # print (\"Xs_clicked\",Xs_clicked.dtype)\n",
    "            # feature 행렬곱하는 부분\n",
    "\n",
    "            click_history[ii] = torch.matmul(cumsum_tril_matrix,\n",
    "                                             Xs_clicked.to(dtype=torch.float64))  # Xs_clicked: section by _f_dim\n",
    "\n",
    "        concat_history = torch.cat(click_history, axis=1)\n",
    "        #print(\"history: \",concat_history,concat_history.shape)\n",
    "        disp_history_feature = concat_history[disp_2d_split_sec_ind]\n",
    "\n",
    "        # (4) combine features\n",
    "        concat_disp_features = torch.reshape(torch.cat([disp_history_feature, disp_current_feature], axis=1),\n",
    "                                             [-1, self.f_dim * self.pw_dim + self.f_dim])\n",
    "        #  print(len(concat_disp_features))\n",
    "        # print(concat_disp_features.shape)\n",
    "        # (5) compute utility\n",
    "        # print (\"the in pu t shape s \",concat_disp_features.shape)\n",
    "        # reward\n",
    "        u_disp = self.mlp_model(concat_disp_features.float())\n",
    "        # net.apply(init_weights,sdv)\n",
    "        # (5) 백터로 나옴. 값은\n",
    "        # user가 본 영화 길이가 나오는듯\n",
    "        #print(u_disp)\n",
    "        exp_u_disp = torch.exp(u_disp)\n",
    "        # #torch exp\n",
    "        # [1,1,2,1,1, 1,1,1,1, 1,1,1]\n",
    "        # [0,0,0,0,0  1 1 1 1 ,2,2,2]->[5,4,3]\n",
    "        #     reward                     \n",
    "        return u_disp,disp_history_feature,concat_disp_features\n",
    "def segment_sum(data, segment_ids):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if not all(segment_ids[i] <= segment_ids[i + 1] for i in range(len(segment_ids) - 1)):\n",
    "        raise AssertionError(\"elements of segment_ids must be sorted\")\n",
    "\n",
    "    if len(segment_ids.shape) != 1:\n",
    "        raise AssertionError(\"segment_ids have be a 1-D tensor\")\n",
    "\n",
    "    if data.shape[0] != segment_ids.shape[0]:\n",
    "        raise AssertionError(\"segment_ids should be the same size as dimension 0 of input.\")\n",
    "\n",
    "    num_segments = len(torch.unique(segment_ids))\n",
    "    return unsorted_segment_sum(data, segment_ids, num_segments)\n",
    "\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    \"\"\"\n",
    "    Computes the sum along segments of a tensor. Analogous to tf.unsorted_segment_sum.\n",
    "\n",
    "    :param data: A tensor whose segments are to be summed.\n",
    "    :param segment_ids: The segment indices tensor.\n",
    "    :param num_segments: The number of segments.\n",
    "    :return: A tensor of same data type as the data argument.\n",
    "    \"\"\"\n",
    "    assert all([i in data.shape for i in segment_ids.shape]), \"segment_ids.shape should be a prefix of data.shape\"\n",
    "\n",
    "    # segment_ids is a 1-D tensor repeat it to have the same shape as data\n",
    "    if len(segment_ids.shape) == 1:\n",
    "        s = torch.prod(torch.tensor(data.shape[1:])).long()\n",
    "        segment_ids = segment_ids.repeat_interleave(s).view(segment_ids.shape[0], *data.shape[1:])\n",
    "\n",
    "    assert data.shape == segment_ids.shape, \"data.shape and segment_ids.shape should be equal\"\n",
    "\n",
    "    shape = [num_segments] + list(data.shape[1:])\n",
    "    tensor = torch.zeros(*shape).scatter_add(0, segment_ids, data.float())\n",
    "    tensor = tensor.type(data.dtype)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "\n",
    "def multithread_compute_vali( valid_data, model):\n",
    "    global vali_sum, vali_cnt\n",
    "\n",
    "    vali_sum = [0.0, 0.0, 0.0]\n",
    "    vali_cnt = 0\n",
    "    threads = []\n",
    "    for ii in xrange(10):\n",
    "        # print (\"got here\")\n",
    "        # print (dataset.model_type)\n",
    "        # print (\" [dataset.vali_user[ii]]\", [dataset.vali_user[ii]])\n",
    "        # valid_data = dataset.prepare_validation_data(1, [dataset.vali_user[15]]) # is a dict\n",
    "\n",
    "        # print (\"valid_data\",valid_data)\n",
    "        # sys.exit()\n",
    "\n",
    "        thread = threading.Thread(target=vali_eval, args=(1, ii, valid_data, model))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return vali_sum[0] / vali_cnt, vali_sum[1] / vali_cnt, vali_sum[2] / vali_cnt\n",
    "\n",
    "lock = threading.Lock()\n",
    "#thread 하나하나가 user다.\n",
    "\n",
    "def vali_eval(xx, ii, valid_data, model):\n",
    "    global vali_sum, vali_cnt\n",
    "    # print (\"dataset.vali_user\",dataset.vali_user)\n",
    "\n",
    "    # valid_data = dataset.prepare_validation_data(1, [dataset.vali_user[ii]]) # is a dict\n",
    "\n",
    "    # print (\"valid_data\",valid_data)\n",
    "    # sys.exit()\n",
    "    with torch.no_grad():\n",
    "        _, _, _, loss_sum, precision_1_sum, precision_2_sum, event_cnt,_ ,_= model(valid_data, index=ii)\n",
    "\n",
    "    lock.acquire()\n",
    "    vali_sum[0] += loss_sum\n",
    "    vali_sum[1] += precision_1_sum\n",
    "    vali_sum[2] += precision_2_sum\n",
    "    vali_cnt += event_cnt\n",
    "    lock.release()\n",
    "\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "\n",
    "def multithread_compute_test( test_data, model):\n",
    "    global test_sum, test_cnt\n",
    "\n",
    "    num_sets = 1 * 10\n",
    "\n",
    "    thread_dist = [[] for _ in xrange(10)]\n",
    "    for ii in xrange(num_sets):\n",
    "        thread_dist[ii % 10].append(ii)\n",
    "\n",
    "    test_sum = [0.0, 0.0, 0.0]\n",
    "    test_cnt = 0\n",
    "    threads = []\n",
    "    for ii in xrange(10):\n",
    "        thread = threading.Thread(target=test_eval, args=(1, thread_dist[ii], test_data, model))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    return test_sum[0] / test_cnt, test_sum[1] / test_cnt, test_sum[2] / test_cnt\n",
    "\n",
    "\n",
    "def test_eval(xx, thread_dist, test_data, model):\n",
    "    global test_sum, test_cnt\n",
    "    test_thread_eval = [0.0, 0.0, 0.0]\n",
    "    test_thread_cnt = 0\n",
    "    for ii in thread_dist:\n",
    "        with torch.no_grad():\n",
    "            _, _, _, loss_sum, precision_1_sum, precision_2_sum, event_cnt,_,_ = model(test_data, index=ii)\n",
    "\n",
    "        test_thread_eval[0] += loss_sum\n",
    "        test_thread_eval[1] += precision_1_sum\n",
    "        test_thread_eval[2] += precision_2_sum\n",
    "        test_thread_cnt += event_cnt\n",
    "\n",
    "    lock.acquire()\n",
    "    test_sum[0] += test_thread_eval[0]\n",
    "    test_sum[1] += test_thread_eval[1]\n",
    "    test_sum[2] += test_thread_eval[2]\n",
    "    test_cnt += test_thread_cnt\n",
    "    lock.release()\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    sd = 1e-3\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight)\n",
    "        m.weight.data.clamp_(-sd, sd)  # to mimic the normal clmaped weight initilization\n",
    "\n",
    "\n",
    "\n",
    "log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"%s, start\" % log_time)\n",
    "\n",
    "dataset = Dataset()\n",
    "log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"%s, load data completed\" % log_time)\n",
    "\n",
    "log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"%s, start prepare vali data\" % log_time)\n",
    "\n",
    "valid_data = dataset.prepare_validation_data(30, dataset.vali_user)\n",
    "log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"%s, prepare validation data, completed\" % log_time)\n",
    "model = UserModelPW(dataset.f_dim)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# optimizer = optim.Adam(\n",
    "#   [{'params': model.parameters(), 'lr': opts.learning_rate}])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "\n",
    "best_metric = [100000.0, 0.0, 0.0]\n",
    "\n",
    "vali_path = \"./save_dir/\" + '/'\n",
    "if not os.path.exists(vali_path):\n",
    "    os.makedirs(vali_path)\n",
    "\n",
    "for i in xrange(200):\n",
    "\n",
    "    #데이터 학습 qnetwork 학습\n",
    "    # model.train()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "    model.zero_grad()\n",
    "\n",
    "    training_user_nos = np.random.choice(dataset.train_user, 70, replace=False)\n",
    "    training_user = dataset.data_process_for_placeholder(training_user_nos)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data.clamp_(-1e0, 1e0)\n",
    "    loss, _, _, _, _, _, _ ,_,_= model(training_user, is_train=True)\n",
    "    # print (\"the loss is\",loss)\n",
    "    print(\"loss :\",loss)\n",
    "    losslist.append(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if np.mod(i, 10) == 0:\n",
    "        if i == 0:\n",
    "            log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s, start first iteration validation\" % log_time)\n",
    "\n",
    "    if np.mod(i, 10) == 0:\n",
    "        if i == 0:\n",
    "            log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s, start first iteration validation\" % log_time)\n",
    "        vali_loss_prc = multithread_compute_vali(valid_data, model)\n",
    "        if i == 0:\n",
    "            log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s, first iteration validation complete\" % log_time)\n",
    "\n",
    "        log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(\"%s: itr%d, vali: %.5f, %.5f, %.5f\" %\n",
    "                (log_time, i, vali_loss_prc[0], vali_loss_prc[1], vali_loss_prc[2]))\n",
    "\n",
    "        if vali_loss_prc[0] < best_metric[0]:\n",
    "            best_metric[0] = vali_loss_prc[0]\n",
    "            best_save_path = os.path.join(vali_path, 'best-loss')\n",
    "            torch.save(model.state_dict(), best_save_path)\n",
    "            # best_save_path = saver.save(sess, best_save_path)\n",
    "        if vali_loss_prc[1] > best_metric[1]:\n",
    "            best_metric[1] = vali_loss_prc[1]\n",
    "            best_save_path = os.path.join(vali_path, 'best-pre1')\n",
    "            torch.save(model.state_dict(), best_save_path)\n",
    "        if vali_loss_prc[2] > best_metric[2]:\n",
    "            best_metric[2] = vali_loss_prc[2]\n",
    "            best_save_path = os.path.join(vali_path, 'best-pre2')\n",
    "            torch.save(model.state_dict(), best_save_path)\n",
    "\n",
    "    log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"%s, iteration %d train complete\" % (log_time, i))\n",
    "\n",
    "\n",
    "log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"%s, data end\" % log_time)\n",
    "\n",
    "# test\n",
    "log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"%s, start prepare test data\" % log_time)\n",
    "\n",
    "test_data = dataset.prepare_validation_data(10, dataset.test_user)\n",
    "\n",
    "tmplosslist=losslist\n",
    "\n",
    "best_save_path = os.path.join(vali_path, 'best-loss')\n",
    "model.load_state_dict(torch.load(best_save_path))\n",
    "# saver.restore(sess, best_save_path)\n",
    "test_loss_prc = multithread_compute_test( test_data, model)\n",
    "vali_loss_prc = multithread_compute_vali( valid_data, model)\n",
    "print(\"test!!!loss!!!, test: %.5f, vali: %.5f\" % (test_loss_prc[0], vali_loss_prc[0]))\n",
    "\n",
    "best_save_path = os.path.join(vali_path, 'best-pre1')\n",
    "model.load_state_dict(torch.load(best_save_path))\n",
    "# saver.restore(sess, best_save_path)\n",
    "test_loss_prc = multithread_compute_test( test_data, model)\n",
    "vali_loss_prc = multithread_compute_vali( valid_data, model)\n",
    "print(\"test!!!pre1!!!, test: %.5f, vali: %.5f\" % (test_loss_prc[1], vali_loss_prc[1]))\n",
    "\n",
    "best_save_path = os.path.join(vali_path, 'best-pre2')\n",
    "model.load_state_dict(torch.load(best_save_path))\n",
    "# saver.restore(sess, best_save_path)\n",
    "test_loss_prc = multithread_compute_test( test_data, model)\n",
    "vali_loss_prc = multithread_compute_vali( valid_data, model)\n",
    "print(\"test!!!pre2!!!, test: %.5f, vali: %.5f\" % (test_loss_prc[2], vali_loss_prc[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c26f4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(tmplosslist)\n",
    "plt.show()\n",
    "plt.savefig(\"save.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97d2b05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [[2, 3123], [1, 2, 3]], [3123, 3]]\n",
      "0\n",
      "<__main__.Dataset object at 0x000001EC836C03A0>\n",
      "tests...\n",
      "loss : tensor(0.7389)\n",
      "tensor([[1.5540],\n",
      "        [1.4391],\n",
      "        [2.4719],\n",
      "        [1.5540],\n",
      "        [2.7440]])\n"
     ]
    }
   ],
   "source": [
    "mkrandhistory=[[[3],[[2,3123],[1,2,3]],[3123,3]]]\n",
    "#yelp\n",
    "#time=user\n",
    "#[123,1233,4,5,1]  dense_exp_disp_util?\n",
    "for mk in mkrandhistory:\n",
    "     print(mk) \n",
    "userdata=Dataset(train_data=False,userhistory=mkrandhistory)\n",
    "print(userdata)\n",
    "tests=userdata.data_process_for_placeholder([0])\n",
    "print(\"tests...\")\n",
    "with torch.no_grad():\n",
    "     loss, _, _, _, _, _, _ ,u_disps,_=  model(tests, is_train=True)\n",
    "print(u_disps)\n",
    "     #    print(sum_click_u_bar_ut)\n",
    "     #    loss=-sum_click_u_bar_ut+torch.log(sum_exp_disp_ubar_ut + 1)\n",
    "     #    print(torch.log(sum_exp_disp_ubar_ut + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432114db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3], [[2, 3123, 3], [0]], [3123, 0]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3676/535017839.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmkrandhistory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m      \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0muserdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muserhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmkrandhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtests\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_process_for_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "mkrandhistory=[[[3],[[2,3123,3],[0]],[3123,0]]]\n",
    "#yelp\n",
    "#time=user\n",
    "#[123,1233,4,5,1] \n",
    "for mk in mkrandhistory:\n",
    "     print(mk) \n",
    "userdata=Dataset(train_data=False,userhistory=mkrandhistory)\n",
    "print(userdata)\n",
    "tests=userdata.data_process_for_placeholder([0])\n",
    "print(\"tests...\")\n",
    "with torch.no_grad():\n",
    "     disp,concat,exp_disp= model.embedding(tests, is_train=True)\n",
    "     print(disp)\n",
    "     exp_u_disp = torch.exp(disp)\n",
    "     print(\"concat\",concat.shape)\n",
    "print(concats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c2bb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922])\n"
     ]
    }
   ],
   "source": [
    "#movieset에서 빈값이 있음\n",
    "#try부분\n",
    "#이거 채워야 진행이 될듯하네요.\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"ml-25m.csv\")\n",
    "df=df.drop(['movieId','genres'],axis=1)\n",
    "movies=df.loc[0:3922].to_dict('list')\n",
    "movies=movies['title']\n",
    "moviedict = {}\n",
    "movieset=[0 for i in range(3952)]\n",
    "idx2moviedict = {}\n",
    "\n",
    "movieid=0\n",
    "for i in range(len(movies)):\n",
    "    movieid=i\n",
    "    moviedict[movieid] = movies[i]\n",
    "    idx2moviedict[movies[i]] = movieid\n",
    "    movieset[movieid-1]=1\n",
    "print(moviedict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60bd5e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61f2cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cQ\n",
      "0\n",
      "0 : tensor(74)\n",
      "0\n",
      "1 : tensor(1041)\n",
      "0\n",
      "2 : tensor(32)\n",
      "0\n",
      "3 : tensor(2443)\n",
      "0\n",
      "4 : tensor(833)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2443)\n",
      "[tensor(2443)]\n",
      "1\n",
      "cQ\n",
      "0\n",
      "0 : tensor(72)\n",
      "0\n",
      "1 : tensor(1038)\n",
      "0\n",
      "2 : tensor(31)\n",
      "0\n",
      "3 : tensor(2440)\n",
      "0\n",
      "4 : tensor(831)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2440)\n",
      "[tensor(2443), tensor(2440)]\n",
      "2\n",
      "cQ\n",
      "0\n",
      "0 : tensor(70)\n",
      "0\n",
      "1 : tensor(1035)\n",
      "0\n",
      "2 : tensor(30)\n",
      "0\n",
      "3 : tensor(2435)\n",
      "0\n",
      "4 : tensor(827)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2435)\n",
      "[tensor(2443), tensor(2440), tensor(2435)]\n",
      "3\n",
      "cQ\n",
      "0\n",
      "0 : tensor(68)\n",
      "0\n",
      "1 : tensor(1031)\n",
      "0\n",
      "2 : tensor(29)\n",
      "0\n",
      "3 : tensor(2430)\n",
      "0\n",
      "4 : tensor(823)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(29)\n",
      "[tensor(2443), tensor(2440), tensor(2435), tensor(29)]\n",
      "4\n",
      "cQ\n",
      "0\n",
      "0 : tensor(66)\n",
      "0\n",
      "1 : tensor(1028)\n",
      "0\n",
      "2 : tensor(28)\n",
      "0\n",
      "3 : tensor(2427)\n",
      "0\n",
      "4 : tensor(821)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1028)\n",
      "[tensor(2443), tensor(2440), tensor(2435), tensor(29), tensor(1028)]\n",
      "5\n",
      "rand\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "1869\n",
      "[1869]\n",
      "6\n",
      "cQ\n",
      "0\n",
      "0 : tensor(64)\n",
      "0\n",
      "1 : tensor(1025)\n",
      "0\n",
      "2 : tensor(27)\n",
      "0\n",
      "3 : tensor(2422)\n",
      "0\n",
      "4 : tensor(818)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2422)\n",
      "[1869, tensor(2422)]\n",
      "7\n",
      "cQ\n",
      "0\n",
      "0 : tensor(62)\n",
      "0\n",
      "1 : tensor(1022)\n",
      "0\n",
      "2 : tensor(26)\n",
      "0\n",
      "3 : tensor(2418)\n",
      "0\n",
      "4 : tensor(815)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2418)\n",
      "[1869, tensor(2422), tensor(2418)]\n",
      "8\n",
      "cQ\n",
      "0\n",
      "0 : tensor(60)\n",
      "0\n",
      "1 : tensor(1019)\n",
      "0\n",
      "2 : tensor(25)\n",
      "0\n",
      "3 : tensor(2414)\n",
      "0\n",
      "4 : tensor(812)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1019)\n",
      "[1869, tensor(2422), tensor(2418), tensor(1019)]\n",
      "9\n",
      "cQ\n",
      "0\n",
      "0 : tensor(58)\n",
      "0\n",
      "1 : tensor(1015)\n",
      "0\n",
      "2 : tensor(24)\n",
      "0\n",
      "3 : tensor(2409)\n",
      "0\n",
      "4 : tensor(808)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2409)\n",
      "[1869, tensor(2422), tensor(2418), tensor(1019), tensor(2409)]\n",
      "10\n",
      "cQ\n",
      "0\n",
      "0 : tensor(56)\n",
      "0\n",
      "1 : tensor(1012)\n",
      "0\n",
      "2 : tensor(23)\n",
      "0\n",
      "3 : tensor(2405)\n",
      "0\n",
      "4 : tensor(805)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2405)\n",
      "[tensor(2405)]\n",
      "11\n",
      "cQ\n",
      "0\n",
      "0 : tensor(54)\n",
      "0\n",
      "1 : tensor(1009)\n",
      "0\n",
      "2 : tensor(22)\n",
      "0\n",
      "3 : tensor(2401)\n",
      "0\n",
      "4 : tensor(802)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(802)\n",
      "[tensor(2405), tensor(802)]\n",
      "12\n",
      "cQ\n",
      "0\n",
      "0 : tensor(52)\n",
      "0\n",
      "1 : tensor(1006)\n",
      "0\n",
      "2 : tensor(21)\n",
      "0\n",
      "3 : tensor(2397)\n",
      "0\n",
      "4 : tensor(799)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2397)\n",
      "[tensor(2405), tensor(802), tensor(2397)]\n",
      "13\n",
      "cQ\n",
      "0\n",
      "0 : tensor(50)\n",
      "0\n",
      "1 : tensor(1003)\n",
      "0\n",
      "2 : tensor(20)\n",
      "0\n",
      "3 : tensor(2393)\n",
      "0\n",
      "4 : tensor(796)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(50)\n",
      "[tensor(2405), tensor(802), tensor(2397), tensor(50)]\n",
      "14\n",
      "cQ\n",
      "0\n",
      "0 : tensor(48)\n",
      "0\n",
      "1 : tensor(999)\n",
      "0\n",
      "2 : tensor(19)\n",
      "0\n",
      "3 : tensor(2388)\n",
      "0\n",
      "4 : tensor(792)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(999)\n",
      "[tensor(2405), tensor(802), tensor(2397), tensor(50), tensor(999)]\n",
      "15\n",
      "cQ\n",
      "0\n",
      "0 : tensor(46)\n",
      "0\n",
      "1 : tensor(995)\n",
      "0\n",
      "2 : tensor(18)\n",
      "0\n",
      "3 : tensor(2383)\n",
      "0\n",
      "4 : tensor(788)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2383)\n",
      "[tensor(2383)]\n",
      "16\n",
      "cQ\n",
      "0\n",
      "0 : tensor(44)\n",
      "0\n",
      "1 : tensor(992)\n",
      "0\n",
      "2 : tensor(17)\n",
      "0\n",
      "3 : tensor(2379)\n",
      "0\n",
      "4 : tensor(785)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2379)\n",
      "[tensor(2383), tensor(2379)]\n",
      "17\n",
      "cQ\n",
      "0\n",
      "0 : tensor(42)\n",
      "0\n",
      "1 : tensor(990)\n",
      "0\n",
      "2 : tensor(16)\n",
      "0\n",
      "3 : tensor(2376)\n",
      "0\n",
      "4 : tensor(783)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(990)\n",
      "[tensor(2383), tensor(2379), tensor(990)]\n",
      "18\n",
      "cQ\n",
      "0\n",
      "0 : tensor(41)\n",
      "0\n",
      "1 : tensor(987)\n",
      "0\n",
      "2 : tensor(15)\n",
      "0\n",
      "3 : tensor(2372)\n",
      "0\n",
      "4 : tensor(780)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(987)\n",
      "[tensor(2383), tensor(2379), tensor(990), tensor(987)]\n",
      "19\n",
      "cQ\n",
      "0\n",
      "0 : tensor(39)\n",
      "0\n",
      "1 : tensor(984)\n",
      "0\n",
      "2 : tensor(14)\n",
      "0\n",
      "3 : tensor(2368)\n",
      "0\n",
      "4 : tensor(777)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(984)\n",
      "[tensor(2383), tensor(2379), tensor(990), tensor(987), tensor(984)]\n",
      "20\n",
      "cQ\n",
      "0\n",
      "0 : tensor(37)\n",
      "0\n",
      "1 : tensor(981)\n",
      "0\n",
      "2 : tensor(13)\n",
      "0\n",
      "3 : tensor(2364)\n",
      "0\n",
      "4 : tensor(774)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2364)\n",
      "[tensor(2364)]\n",
      "21\n",
      "cQ\n",
      "0\n",
      "0 : tensor(35)\n",
      "0\n",
      "1 : tensor(978)\n",
      "0\n",
      "2 : tensor(12)\n",
      "0\n",
      "3 : tensor(2360)\n",
      "0\n",
      "4 : tensor(771)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(12)\n",
      "[tensor(2364), tensor(12)]\n",
      "22\n",
      "cQ\n",
      "0\n",
      "0 : tensor(33)\n",
      "0\n",
      "1 : tensor(975)\n",
      "0\n",
      "2 : tensor(1539)\n",
      "0\n",
      "3 : tensor(2357)\n",
      "0\n",
      "4 : tensor(769)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(975)\n",
      "[tensor(2364), tensor(12), tensor(975)]\n",
      "23\n",
      "rand\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "608\n",
      "[tensor(2364), tensor(12), tensor(975), 608]\n",
      "24\n",
      "cQ\n",
      "0\n",
      "0 : tensor(32)\n",
      "0\n",
      "1 : tensor(974)\n",
      "0\n",
      "2 : tensor(1537)\n",
      "0\n",
      "3 : tensor(2355)\n",
      "0\n",
      "4 : tensor(768)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(974)\n",
      "[tensor(2364), tensor(12), tensor(975), 608, tensor(974)]\n",
      "25\n",
      "cQ\n",
      "0\n",
      "0 : tensor(32)\n",
      "0\n",
      "1 : tensor(973)\n",
      "0\n",
      "2 : tensor(1536)\n",
      "0\n",
      "3 : tensor(2354)\n",
      "0\n",
      "4 : tensor(767)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2354)\n",
      "[tensor(2354)]\n",
      "26\n",
      "cQ\n",
      "0\n",
      "0 : tensor(32)\n",
      "0\n",
      "1 : tensor(972)\n",
      "0\n",
      "2 : tensor(1534)\n",
      "0\n",
      "3 : tensor(2352)\n",
      "0\n",
      "4 : tensor(766)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(766)\n",
      "[tensor(2354), tensor(766)]\n",
      "27\n",
      "cQ\n",
      "0\n",
      "0 : tensor(32)\n",
      "0\n",
      "1 : tensor(971)\n",
      "0\n",
      "2 : tensor(1533)\n",
      "0\n",
      "3 : tensor(2350)\n",
      "0\n",
      "4 : tensor(764)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2350)\n",
      "[tensor(2354), tensor(766), tensor(2350)]\n",
      "28\n",
      "cQ\n",
      "0\n",
      "0 : tensor(32)\n",
      "0\n",
      "1 : tensor(969)\n",
      "0\n",
      "2 : tensor(1530)\n",
      "0\n",
      "3 : tensor(2348)\n",
      "0\n",
      "4 : tensor(763)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2348)\n",
      "[tensor(2354), tensor(766), tensor(2350), tensor(2348)]\n",
      "29\n",
      "cQ\n",
      "0\n",
      "0 : tensor(32)\n",
      "0\n",
      "1 : tensor(968)\n",
      "0\n",
      "2 : tensor(1529)\n",
      "0\n",
      "3 : tensor(2346)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2346)\n",
      "[tensor(2354), tensor(766), tensor(2350), tensor(2348), tensor(2346)]\n",
      "30\n",
      "[tensor(62), tensor(1022), tensor(26), tensor(2418), tensor(815)]\n",
      "0\n",
      "0\n",
      "[tensor(56), tensor(1012), tensor(23), tensor(2405), tensor(805)]\n",
      "0\n",
      "0\n",
      "[tensor(44), tensor(992), tensor(17), tensor(2379), tensor(785)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(972), tensor(1534), tensor(2352), tensor(766)]\n",
      "0\n",
      "0\n",
      "[tensor(37), tensor(981), tensor(13), tensor(2364), tensor(774)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(969), tensor(1530), tensor(2348), tensor(763)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(968), tensor(1529), tensor(2346), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(70), tensor(1035), tensor(30), tensor(2435), tensor(827)]\n",
      "0\n",
      "0\n",
      "[tensor(60), tensor(1019), tensor(25), tensor(2414), tensor(812)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(974), tensor(1537), tensor(2355), tensor(768)]\n",
      "0\n",
      "0\n",
      "[tensor(60), tensor(1019), tensor(25), tensor(2414), tensor(812)]\n",
      "0\n",
      "0\n",
      "[tensor(42), tensor(990), tensor(16), tensor(2376), tensor(783)]\n",
      "0\n",
      "0\n",
      "[tensor(39), tensor(984), tensor(14), tensor(2368), tensor(777)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(973), tensor(1536), tensor(2354), tensor(767)]\n",
      "0\n",
      "0\n",
      "[608, 1578, 1452, 2625, 1642]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(969), tensor(1530), tensor(2348), tensor(763)]\n",
      "0\n",
      "0\n",
      "[tensor(50), tensor(1003), tensor(20), tensor(2393), tensor(796)]\n",
      "0\n",
      "0\n",
      "[tensor(54), tensor(1009), tensor(22), tensor(2401), tensor(802)]\n",
      "0\n",
      "0\n",
      "[tensor(48), tensor(999), tensor(19), tensor(2388), tensor(792)]\n",
      "0\n",
      "0\n",
      "[tensor(58), tensor(1015), tensor(24), tensor(2409), tensor(808)]\n",
      "0\n",
      "0\n",
      "[tensor(42), tensor(990), tensor(16), tensor(2376), tensor(783)]\n",
      "0\n",
      "0\n",
      "[1620, 1869, 318, 3309, 217]\n",
      "0\n",
      "0\n",
      "[tensor(74), tensor(1041), tensor(32), tensor(2443), tensor(833)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(969), tensor(1530), tensor(2348), tensor(763)]\n",
      "0\n",
      "0\n",
      "[tensor(44), tensor(992), tensor(17), tensor(2379), tensor(785)]\n",
      "0\n",
      "0\n",
      "epi : 1\n",
      "score tensor([110.6393], grad_fn=<AddBackward0>)\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2335)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(2335)\n",
      "[tensor(2335)]\n",
      "31\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2334)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1437)\n",
      "[tensor(2335), tensor(1437)]\n",
      "32\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2334)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1437)\n",
      "[tensor(2335), tensor(1437), tensor(1437)]\n",
      "33\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2334)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1437)\n",
      "[tensor(2335), tensor(1437), tensor(1437), tensor(1437)]\n",
      "34\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2334)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1437)\n",
      "[tensor(2335), tensor(1437), tensor(1437), tensor(1437), tensor(1437)]\n",
      "35\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2334)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1437)\n",
      "[tensor(1437)]\n",
      "36\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2334)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1437)\n",
      "[tensor(1437), tensor(1437)]\n",
      "37\n",
      "rand\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "754\n",
      "[tensor(1437), tensor(1437), 754]\n",
      "38\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2334)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1437)\n",
      "[tensor(1437), tensor(1437), 754, tensor(1437)]\n",
      "39\n",
      "cQ\n",
      "0\n",
      "0 : tensor(2334)\n",
      "0\n",
      "1 : tensor(1157)\n",
      "0\n",
      "2 : tensor(1526)\n",
      "0\n",
      "3 : tensor(1437)\n",
      "0\n",
      "4 : tensor(761)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1437)\n",
      "[tensor(1437), tensor(1437), 754, tensor(1437), tensor(1437)]\n",
      "40\n",
      "[tensor(41), tensor(987), tensor(15), tensor(2372), tensor(780)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(973), tensor(1536), tensor(2354), tensor(767)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(2335), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(44), tensor(992), tensor(17), tensor(2379), tensor(785)]\n",
      "0\n",
      "0\n",
      "[tensor(52), tensor(1006), tensor(21), tensor(2397), tensor(799)]\n",
      "0\n",
      "0\n",
      "[tensor(44), tensor(992), tensor(17), tensor(2379), tensor(785)]\n",
      "0\n",
      "0\n",
      "[tensor(50), tensor(1003), tensor(20), tensor(2393), tensor(796)]\n",
      "0\n",
      "0\n",
      "[608, 1578, 1452, 2625, 1642]\n",
      "0\n",
      "0\n",
      "[1620, 1869, 318, 3309, 217]\n",
      "0\n",
      "0\n",
      "[tensor(46), tensor(995), tensor(18), tensor(2383), tensor(788)]\n",
      "0\n",
      "0\n",
      "[tensor(39), tensor(984), tensor(14), tensor(2368), tensor(777)]\n",
      "0\n",
      "0\n",
      "[tensor(50), tensor(1003), tensor(20), tensor(2393), tensor(796)]\n",
      "0\n",
      "0\n",
      "[tensor(2335), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[1620, 1869, 318, 3309, 217]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(2335), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(968), tensor(1529), tensor(2346), tensor(761)]\n",
      "0\n",
      "0\n",
      "[1620, 1869, 318, 3309, 217]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(969), tensor(1530), tensor(2348), tensor(763)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(974), tensor(1537), tensor(2355), tensor(768)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(42), tensor(990), tensor(16), tensor(2376), tensor(783)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(973), tensor(1536), tensor(2354), tensor(767)]\n",
      "0\n",
      "0\n",
      "epi : 1\n",
      "score tensor([31.6942], grad_fn=<AddBackward0>)\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1939)\n",
      "0\n",
      "1 : tensor(1156)\n",
      "0\n",
      "2 : tensor(1525)\n",
      "0\n",
      "3 : tensor(1519)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1525)\n",
      "[tensor(1525)]\n",
      "41\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1936)\n",
      "0\n",
      "1 : tensor(1153)\n",
      "0\n",
      "2 : tensor(1521)\n",
      "0\n",
      "3 : tensor(1515)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1521)\n",
      "[tensor(1525), tensor(1521)]\n",
      "42\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1933)\n",
      "0\n",
      "1 : tensor(1150)\n",
      "0\n",
      "2 : tensor(1519)\n",
      "0\n",
      "3 : tensor(1514)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1519)\n",
      "[tensor(1525), tensor(1521), tensor(1519)]\n",
      "43\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1931)\n",
      "0\n",
      "1 : tensor(1148)\n",
      "0\n",
      "2 : tensor(1517)\n",
      "0\n",
      "3 : tensor(1512)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1517)\n",
      "[tensor(1525), tensor(1521), tensor(1519), tensor(1517)]\n",
      "44\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1929)\n",
      "0\n",
      "1 : tensor(1146)\n",
      "0\n",
      "2 : tensor(1514)\n",
      "0\n",
      "3 : tensor(1509)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1514)\n",
      "[tensor(1525), tensor(1521), tensor(1519), tensor(1517), tensor(1514)]\n",
      "45\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1926)\n",
      "0\n",
      "1 : tensor(1143)\n",
      "0\n",
      "2 : tensor(1512)\n",
      "0\n",
      "3 : tensor(1507)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1512)\n",
      "[tensor(1512)]\n",
      "46\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1925)\n",
      "0\n",
      "1 : tensor(1142)\n",
      "0\n",
      "2 : tensor(1511)\n",
      "0\n",
      "3 : tensor(1506)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1511)\n",
      "[tensor(1512), tensor(1511)]\n",
      "47\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1923)\n",
      "0\n",
      "1 : tensor(1140)\n",
      "0\n",
      "2 : tensor(1509)\n",
      "0\n",
      "3 : tensor(1504)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1509)\n",
      "[tensor(1512), tensor(1511), tensor(1509)]\n",
      "48\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1922)\n",
      "0\n",
      "1 : tensor(1139)\n",
      "0\n",
      "2 : tensor(1508)\n",
      "0\n",
      "3 : tensor(1503)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1922)\n",
      "[tensor(1512), tensor(1511), tensor(1509), tensor(1922)]\n",
      "49\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1920)\n",
      "0\n",
      "1 : tensor(1137)\n",
      "0\n",
      "2 : tensor(1505)\n",
      "0\n",
      "3 : tensor(1500)\n",
      "0\n",
      "4 : tensor(154)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1505)\n",
      "[tensor(1512), tensor(1511), tensor(1509), tensor(1922), tensor(1505)]\n",
      "50\n",
      "[tensor(1926), tensor(1143), tensor(1512), tensor(1507), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(48), tensor(999), tensor(19), tensor(2388), tensor(792)]\n",
      "0\n",
      "0\n",
      "[608, 1578, 1452, 2625, 1642]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(44), tensor(992), tensor(17), tensor(2379), tensor(785)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1925), tensor(1142), tensor(1511), tensor(1506), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(56), tensor(1012), tensor(23), tensor(2405), tensor(805)]\n",
      "0\n",
      "0\n",
      "[tensor(37), tensor(981), tensor(13), tensor(2364), tensor(774)]\n",
      "0\n",
      "0\n",
      "[tensor(74), tensor(1041), tensor(32), tensor(2443), tensor(833)]\n",
      "0\n",
      "0\n",
      "[tensor(48), tensor(999), tensor(19), tensor(2388), tensor(792)]\n",
      "0\n",
      "0\n",
      "[tensor(42), tensor(990), tensor(16), tensor(2376), tensor(783)]\n",
      "0\n",
      "0\n",
      "[tensor(62), tensor(1022), tensor(26), tensor(2418), tensor(815)]\n",
      "0\n",
      "0\n",
      "[tensor(1923), tensor(1140), tensor(1509), tensor(1504), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(2335), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(969), tensor(1530), tensor(2348), tensor(763)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1923), tensor(1140), tensor(1509), tensor(1504), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(971), tensor(1533), tensor(2350), tensor(764)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(974), tensor(1537), tensor(2355), tensor(768)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(969), tensor(1530), tensor(2348), tensor(763)]\n",
      "0\n",
      "0\n",
      "[tensor(70), tensor(1035), tensor(30), tensor(2435), tensor(827)]\n",
      "0\n",
      "0\n",
      "[754, 3001, 3860, 1278, 199]\n",
      "0\n",
      "0\n",
      "[tensor(1929), tensor(1146), tensor(1514), tensor(1509), tensor(154)]\n",
      "0\n",
      "0\n",
      "epi : 1\n",
      "score tensor([50.0825], grad_fn=<AddBackward0>)\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1917)\n",
      "0\n",
      "1 : tensor(1134)\n",
      "0\n",
      "2 : tensor(1355)\n",
      "0\n",
      "3 : tensor(1497)\n",
      "0\n",
      "4 : tensor(2082)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1134)\n",
      "[tensor(1134)]\n",
      "51\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1913)\n",
      "0\n",
      "1 : tensor(1130)\n",
      "0\n",
      "2 : tensor(1351)\n",
      "0\n",
      "3 : tensor(1493)\n",
      "0\n",
      "4 : tensor(2079)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1913)\n",
      "[tensor(1134), tensor(1913)]\n",
      "52\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1910)\n",
      "0\n",
      "1 : tensor(1127)\n",
      "0\n",
      "2 : tensor(1347)\n",
      "0\n",
      "3 : tensor(1489)\n",
      "0\n",
      "4 : tensor(2074)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1127)\n",
      "[tensor(1134), tensor(1913), tensor(1127)]\n",
      "53\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1906)\n",
      "0\n",
      "1 : tensor(1123)\n",
      "0\n",
      "2 : tensor(1343)\n",
      "0\n",
      "3 : tensor(1485)\n",
      "0\n",
      "4 : tensor(2070)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1906)\n",
      "[tensor(1134), tensor(1913), tensor(1127), tensor(1906)]\n",
      "54\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1901)\n",
      "0\n",
      "1 : tensor(1118)\n",
      "0\n",
      "2 : tensor(1339)\n",
      "0\n",
      "3 : tensor(1481)\n",
      "0\n",
      "4 : tensor(2066)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1118)\n",
      "[tensor(1134), tensor(1913), tensor(1127), tensor(1906), tensor(1118)]\n",
      "55\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1897)\n",
      "0\n",
      "1 : tensor(1114)\n",
      "0\n",
      "2 : tensor(1335)\n",
      "0\n",
      "3 : tensor(1477)\n",
      "0\n",
      "4 : tensor(2062)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1335)\n",
      "[tensor(1335)]\n",
      "56\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1893)\n",
      "0\n",
      "1 : tensor(1110)\n",
      "0\n",
      "2 : tensor(1331)\n",
      "0\n",
      "3 : tensor(1473)\n",
      "0\n",
      "4 : tensor(2058)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1331)\n",
      "[tensor(1335), tensor(1331)]\n",
      "57\n",
      "rand\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "3049\n",
      "[tensor(1335), tensor(1331), 3049]\n",
      "58\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1888)\n",
      "0\n",
      "1 : tensor(1106)\n",
      "0\n",
      "2 : tensor(1327)\n",
      "0\n",
      "3 : tensor(1469)\n",
      "0\n",
      "4 : tensor(2053)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1469)\n",
      "[tensor(1335), tensor(1331), 3049, tensor(1469)]\n",
      "59\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1884)\n",
      "0\n",
      "1 : tensor(1102)\n",
      "0\n",
      "2 : tensor(1323)\n",
      "0\n",
      "3 : tensor(1465)\n",
      "0\n",
      "4 : tensor(2049)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1323)\n",
      "[tensor(1335), tensor(1331), 3049, tensor(1469), tensor(1323)]\n",
      "60\n",
      "[tensor(56), tensor(1012), tensor(23), tensor(2405), tensor(805)]\n",
      "0\n",
      "0\n",
      "[tensor(66), tensor(1028), tensor(28), tensor(2427), tensor(821)]\n",
      "0\n",
      "0\n",
      "[tensor(1888), tensor(1106), tensor(1327), tensor(1469), tensor(2053)]\n",
      "0\n",
      "0\n",
      "[tensor(35), tensor(978), tensor(12), tensor(2360), tensor(771)]\n",
      "0\n",
      "0\n",
      "[tensor(46), tensor(995), tensor(18), tensor(2383), tensor(788)]\n",
      "0\n",
      "0\n",
      "[tensor(37), tensor(981), tensor(13), tensor(2364), tensor(774)]\n",
      "0\n",
      "0\n",
      "[tensor(1917), tensor(1134), tensor(1355), tensor(1497), tensor(2082)]\n",
      "0\n",
      "0\n",
      "[tensor(1913), tensor(1130), tensor(1351), tensor(1493), tensor(2079)]\n",
      "0\n",
      "0\n",
      "[tensor(1884), tensor(1102), tensor(1323), tensor(1465), tensor(2049)]\n",
      "0\n",
      "0\n",
      "[tensor(46), tensor(995), tensor(18), tensor(2383), tensor(788)]\n",
      "0\n",
      "0\n",
      "[tensor(1926), tensor(1143), tensor(1512), tensor(1507), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(1893), tensor(1110), tensor(1331), tensor(1473), tensor(2058)]\n",
      "0\n",
      "0\n",
      "[tensor(1929), tensor(1146), tensor(1514), tensor(1509), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1884), tensor(1102), tensor(1323), tensor(1465), tensor(2049)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(971), tensor(1533), tensor(2350), tensor(764)]\n",
      "0\n",
      "0\n",
      "[tensor(52), tensor(1006), tensor(21), tensor(2397), tensor(799)]\n",
      "0\n",
      "0\n",
      "[tensor(48), tensor(999), tensor(19), tensor(2388), tensor(792)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1933), tensor(1150), tensor(1519), tensor(1514), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(969), tensor(1530), tensor(2348), tensor(763)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1910), tensor(1127), tensor(1347), tensor(1489), tensor(2074)]\n",
      "0\n",
      "0\n",
      "[tensor(1917), tensor(1134), tensor(1355), tensor(1497), tensor(2082)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(972), tensor(1534), tensor(2352), tensor(766)]\n",
      "0\n",
      "0\n",
      "epi : 2\n",
      "score tensor([35.3840], grad_fn=<AddBackward0>)\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1880)\n",
      "0\n",
      "1 : tensor(1620)\n",
      "0\n",
      "2 : tensor(1681)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2046)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(639)\n",
      "[tensor(639)]\n",
      "61\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1877)\n",
      "0\n",
      "1 : tensor(1617)\n",
      "0\n",
      "2 : tensor(1678)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2042)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(639)\n",
      "[tensor(639), tensor(639)]\n",
      "62\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1873)\n",
      "0\n",
      "1 : tensor(1613)\n",
      "0\n",
      "2 : tensor(1674)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2039)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1873)\n",
      "[tensor(639), tensor(639), tensor(1873)]\n",
      "63\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1870)\n",
      "0\n",
      "1 : tensor(1610)\n",
      "0\n",
      "2 : tensor(1671)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2036)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1671)\n",
      "[tensor(639), tensor(639), tensor(1873), tensor(1671)]\n",
      "64\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1867)\n",
      "0\n",
      "1 : tensor(1607)\n",
      "0\n",
      "2 : tensor(1668)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2033)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1607)\n",
      "[tensor(639), tensor(639), tensor(1873), tensor(1671), tensor(1607)]\n",
      "65\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1864)\n",
      "0\n",
      "1 : tensor(1604)\n",
      "0\n",
      "2 : tensor(1665)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2029)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1665)\n",
      "[tensor(1665)]\n",
      "66\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1860)\n",
      "0\n",
      "1 : tensor(1600)\n",
      "0\n",
      "2 : tensor(1661)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2026)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1600)\n",
      "[tensor(1665), tensor(1600)]\n",
      "67\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1857)\n",
      "0\n",
      "1 : tensor(1597)\n",
      "0\n",
      "2 : tensor(1658)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2023)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1857)\n",
      "[tensor(1665), tensor(1600), tensor(1857)]\n",
      "68\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1854)\n",
      "0\n",
      "1 : tensor(1594)\n",
      "0\n",
      "2 : tensor(1655)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2020)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(639)\n",
      "[tensor(1665), tensor(1600), tensor(1857), tensor(639)]\n",
      "69\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1851)\n",
      "0\n",
      "1 : tensor(1591)\n",
      "0\n",
      "2 : tensor(1652)\n",
      "0\n",
      "3 : tensor(639)\n",
      "0\n",
      "4 : tensor(2017)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1851)\n",
      "[tensor(1665), tensor(1600), tensor(1857), tensor(639), tensor(1851)]\n",
      "70\n",
      "[tensor(1901), tensor(1118), tensor(1339), tensor(1481), tensor(2066)]\n",
      "0\n",
      "0\n",
      "[tensor(58), tensor(1015), tensor(24), tensor(2409), tensor(808)]\n",
      "0\n",
      "0\n",
      "[tensor(1893), tensor(1110), tensor(1331), tensor(1473), tensor(2058)]\n",
      "0\n",
      "0\n",
      "[tensor(1917), tensor(1134), tensor(1355), tensor(1497), tensor(2082)]\n",
      "0\n",
      "0\n",
      "[tensor(1873), tensor(1613), tensor(1674), tensor(639), tensor(2039)]\n",
      "0\n",
      "0\n",
      "[tensor(1877), tensor(1617), tensor(1678), tensor(639), tensor(2042)]\n",
      "0\n",
      "0\n",
      "[tensor(1925), tensor(1142), tensor(1511), tensor(1506), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1884), tensor(1102), tensor(1323), tensor(1465), tensor(2049)]\n",
      "0\n",
      "0\n",
      "[tensor(58), tensor(1015), tensor(24), tensor(2409), tensor(808)]\n",
      "0\n",
      "0\n",
      "[tensor(1926), tensor(1143), tensor(1512), tensor(1507), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(1893), tensor(1110), tensor(1331), tensor(1473), tensor(2058)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(973), tensor(1536), tensor(2354), tensor(767)]\n",
      "0\n",
      "0\n",
      "[tensor(39), tensor(984), tensor(14), tensor(2368), tensor(777)]\n",
      "0\n",
      "0\n",
      "[tensor(46), tensor(995), tensor(18), tensor(2383), tensor(788)]\n",
      "0\n",
      "0\n",
      "[tensor(1936), tensor(1153), tensor(1521), tensor(1515), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(1880), tensor(1620), tensor(1681), tensor(639), tensor(2046)]\n",
      "0\n",
      "0\n",
      "[tensor(41), tensor(987), tensor(15), tensor(2372), tensor(780)]\n",
      "0\n",
      "0\n",
      "[tensor(1870), tensor(1610), tensor(1671), tensor(639), tensor(2036)]\n",
      "0\n",
      "0\n",
      "[tensor(35), tensor(978), tensor(12), tensor(2360), tensor(771)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(973), tensor(1536), tensor(2354), tensor(767)]\n",
      "0\n",
      "0\n",
      "[tensor(1867), tensor(1607), tensor(1668), tensor(639), tensor(2033)]\n",
      "0\n",
      "0\n",
      "[tensor(56), tensor(1012), tensor(23), tensor(2405), tensor(805)]\n",
      "0\n",
      "0\n",
      "[tensor(39), tensor(984), tensor(14), tensor(2368), tensor(777)]\n",
      "0\n",
      "0\n",
      "[tensor(54), tensor(1009), tensor(22), tensor(2401), tensor(802)]\n",
      "0\n",
      "0\n",
      "epi : 2\n",
      "score tensor([32.6377], grad_fn=<AddBackward0>)\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1848)\n",
      "0\n",
      "1 : tensor(1588)\n",
      "0\n",
      "2 : tensor(1649)\n",
      "0\n",
      "3 : tensor(1987)\n",
      "0\n",
      "4 : tensor(2012)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1848)\n",
      "[tensor(1848)]\n",
      "71\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1843)\n",
      "0\n",
      "1 : tensor(1583)\n",
      "0\n",
      "2 : tensor(1644)\n",
      "0\n",
      "3 : tensor(1983)\n",
      "0\n",
      "4 : tensor(2008)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1843)\n",
      "[tensor(1848), tensor(1843)]\n",
      "72\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1839)\n",
      "0\n",
      "1 : tensor(1579)\n",
      "0\n",
      "2 : tensor(1640)\n",
      "0\n",
      "3 : tensor(1979)\n",
      "0\n",
      "4 : tensor(2004)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1839)\n",
      "[tensor(1848), tensor(1843), tensor(1839)]\n",
      "73\n",
      "rand\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "3869\n",
      "[tensor(1848), tensor(1843), tensor(1839), 3869]\n",
      "74\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1835)\n",
      "0\n",
      "1 : tensor(1576)\n",
      "0\n",
      "2 : tensor(1637)\n",
      "0\n",
      "3 : tensor(1976)\n",
      "0\n",
      "4 : tensor(2001)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1637)\n",
      "[tensor(1848), tensor(1843), tensor(1839), 3869, tensor(1637)]\n",
      "75\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1832)\n",
      "0\n",
      "1 : tensor(1572)\n",
      "0\n",
      "2 : tensor(1633)\n",
      "0\n",
      "3 : tensor(1972)\n",
      "0\n",
      "4 : tensor(1997)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1633)\n",
      "[tensor(1633)]\n",
      "76\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1828)\n",
      "0\n",
      "1 : tensor(1568)\n",
      "0\n",
      "2 : tensor(1629)\n",
      "0\n",
      "3 : tensor(1967)\n",
      "0\n",
      "4 : tensor(1992)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1992)\n",
      "[tensor(1633), tensor(1992)]\n",
      "77\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1823)\n",
      "0\n",
      "1 : tensor(1564)\n",
      "0\n",
      "2 : tensor(1626)\n",
      "0\n",
      "3 : tensor(1965)\n",
      "0\n",
      "4 : tensor(1990)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1626)\n",
      "[tensor(1633), tensor(1992), tensor(1626)]\n",
      "78\n",
      "rand\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "1539\n",
      "[tensor(1633), tensor(1992), tensor(1626), 1539]\n",
      "79\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1821)\n",
      "0\n",
      "1 : tensor(1561)\n",
      "0\n",
      "2 : tensor(1622)\n",
      "0\n",
      "3 : tensor(1961)\n",
      "0\n",
      "4 : tensor(1986)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1986)\n",
      "[tensor(1633), tensor(1992), tensor(1626), 1539, tensor(1986)]\n",
      "80\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1867), tensor(1607), tensor(1668), tensor(639), tensor(2033)]\n",
      "0\n",
      "0\n",
      "[tensor(1936), tensor(1153), tensor(1521), tensor(1515), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(1922), tensor(1139), tensor(1508), tensor(1503), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(2335), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1926), tensor(1143), tensor(1512), tensor(1507), tensor(154)]\n",
      "0\n",
      "0\n",
      "[1539, 2151, 3748, 2736, 106]\n",
      "0\n",
      "0\n",
      "[tensor(44), tensor(992), tensor(17), tensor(2379), tensor(785)]\n",
      "0\n",
      "0\n",
      "[tensor(32), tensor(969), tensor(1530), tensor(2348), tensor(763)]\n",
      "0\n",
      "0\n",
      "[tensor(70), tensor(1035), tensor(30), tensor(2435), tensor(827)]\n",
      "0\n",
      "0\n",
      "[tensor(1922), tensor(1139), tensor(1508), tensor(1503), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(1901), tensor(1118), tensor(1339), tensor(1481), tensor(2066)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(66), tensor(1028), tensor(28), tensor(2427), tensor(821)]\n",
      "0\n",
      "0\n",
      "[tensor(52), tensor(1006), tensor(21), tensor(2397), tensor(799)]\n",
      "0\n",
      "0\n",
      "[tensor(1926), tensor(1143), tensor(1512), tensor(1507), tensor(154)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(50), tensor(1003), tensor(20), tensor(2393), tensor(796)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(2334), tensor(1157), tensor(1526), tensor(1437), tensor(761)]\n",
      "0\n",
      "0\n",
      "[tensor(1877), tensor(1617), tensor(1678), tensor(639), tensor(2042)]\n",
      "0\n",
      "0\n",
      "[tensor(46), tensor(995), tensor(18), tensor(2383), tensor(788)]\n",
      "0\n",
      "0\n",
      "[tensor(1926), tensor(1143), tensor(1512), tensor(1507), tensor(154)]\n",
      "0\n",
      "0\n",
      "[1539, 2151, 3748, 2736, 106]\n",
      "0\n",
      "0\n",
      "epi : 3\n",
      "score tensor([36.9172], grad_fn=<AddBackward0>)\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1817)\n",
      "0\n",
      "1 : tensor(1558)\n",
      "0\n",
      "2 : tensor(1620)\n",
      "0\n",
      "3 : tensor(60)\n",
      "0\n",
      "4 : tensor(1985)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1558)\n",
      "[tensor(1558)]\n",
      "81\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1816)\n",
      "0\n",
      "1 : tensor(1557)\n",
      "0\n",
      "2 : tensor(1619)\n",
      "0\n",
      "3 : tensor(60)\n",
      "0\n",
      "4 : tensor(1983)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(60)\n",
      "[tensor(1558), tensor(60)]\n",
      "82\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1815)\n",
      "0\n",
      "1 : tensor(1555)\n",
      "0\n",
      "2 : tensor(1617)\n",
      "0\n",
      "3 : tensor(60)\n",
      "0\n",
      "4 : tensor(1982)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1555)\n",
      "[tensor(1558), tensor(60), tensor(1555)]\n",
      "83\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1813)\n",
      "0\n",
      "1 : tensor(1554)\n",
      "0\n",
      "2 : tensor(1616)\n",
      "0\n",
      "3 : tensor(60)\n",
      "0\n",
      "4 : tensor(1980)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(60)\n",
      "[tensor(1558), tensor(60), tensor(1555), tensor(60)]\n",
      "84\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1811)\n",
      "0\n",
      "1 : tensor(1552)\n",
      "0\n",
      "2 : tensor(1614)\n",
      "0\n",
      "3 : tensor(60)\n",
      "0\n",
      "4 : tensor(1978)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1552)\n",
      "[tensor(1558), tensor(60), tensor(1555), tensor(60), tensor(1552)]\n",
      "85\n",
      "cQ\n",
      "0\n",
      "0 : tensor(1809)\n",
      "0\n",
      "1 : tensor(1549)\n",
      "0\n",
      "2 : tensor(1611)\n",
      "0\n",
      "3 : tensor(60)\n",
      "0\n",
      "4 : tensor(1975)\n",
      "0\n",
      "torch.Size([5, 1])\n",
      "tensor(1549)\n",
      "[tensor(1549)]\n",
      "86\n",
      "cQ\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "#moviedict를 만듬\n",
    "\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 50000\n",
    "batch_size    = 5\n",
    "#[0,[action],[state]]\n",
    "#1. 유저가 있음.\n",
    "#유저는 탐색을 해서, 유저를 트레이닝셋에서 가져옴\n",
    "#파라마티는 너가 추천을 해줄 갯수 5개로 합니다\n",
    "#s=[]인 유저로 시작\n",
    "#5개를 추천해줘야함. 추천받아서 \n",
    "#to choose a set I of k items based on user state s\n",
    "#유저 시스템은 유저 상태에서 기반해 k로 이루어진 I를 추천해주고 싶어함.\n",
    "#상태 s(유저가 본 영화)를 제외함 x\n",
    "#그리고 유저가 본 영화를 제외하고, \n",
    "# 3952-x개의 영화속에서 영화를 하나 뽑아서, a1*로 함\n",
    "#3951-x개의 영화속에서,\n",
    "#disp가 분포인데...모든 영화를 넣어버리고\n",
    "#그 영화 feature에서  disp를 가져와서\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst,jlst  = [], [], [], [],[]\n",
    "        \n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime ,j= transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append(a)\n",
    "            r_lst.append(r)\n",
    "            s_prime_lst.append(s_prime)\n",
    "            jlst.append(j)\n",
    "\n",
    "        return s_lst,a_lst, r_lst, s_prime_lst,jlst\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc11 = nn.Linear(19760, 256)\n",
    "        self.fc12 = nn.Linear(256, 256)\n",
    "        self.fc13 = nn.Linear(256, 1)        \n",
    "        #395x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc11(x))\n",
    "        x = F.relu(self.fc12(x))\n",
    "        x = self.fc13(x)\n",
    "        return x\n",
    "\n",
    "def train(q1,q2,q3,q4,q5, q_target1,q_target2,q_target3,q_target4,q_target5, memory, optimizer1,optimizer2,optimizer3,optimizer4,optimizer5):\n",
    "    for i in range(5):\n",
    "        loss=0\n",
    "        s_lst,a_lst, r_lst, s_prime_lst,jlst = memory.sample(batch_size)\n",
    "        for k in range(5): \n",
    "            print(a_lst[k])\n",
    "            sa=[[[0],[],[]]]\n",
    "            sa[0][1].append([a_lst[k][0],a_lst[k][2],a_lst[k][4]])\n",
    "            sa[0][1].append([a_lst[k][1],a_lst[k][3]])\n",
    "            sa[0][2]=[a_lst[k][0],a_lst[k][1]]\n",
    "            sa=Dataset(train_data=False,userhistory=sa)\n",
    "            datas=sa.data_process_for_placeholder([0])       \n",
    "         # print(datas)\n",
    "            with torch.no_grad():\n",
    "        #_,disp,_= model.embedding(datas, is_train=True)\n",
    "                _, _,concat= model.embedding(datas, is_train=True)\n",
    "            concat=concat.float()\n",
    "\n",
    "            tmpmovieset2=movieset\n",
    "            for s in s_prime_lst[k]:\n",
    "                tmpmovieset2[s]=0\n",
    "            u=[[0,[[idx] for idx,x in enumerate(tmpmovieset2) if x==1 ],[idx for idx,x in enumerate(tmpmovieset2) if x==1]]]\n",
    "            sa=Dataset(train_data=False,userhistory=u)\n",
    "            datas2=sa.data_process_for_placeholder([0])       \n",
    "         # print(datas)\n",
    "            with torch.no_grad():\n",
    "        #_,disp,_= model.embedding(datas, is_train=True)\n",
    "                _, _,concat2= model.embedding(datas2, is_train=True)\n",
    "            concat2=concat2.float()\n",
    "\n",
    "            if jlst[k]==0:\n",
    "                q_out=q1.forward(concat)\n",
    "                q_a=q_out[k]\n",
    "                max_q_prime=q_target1(concat2).max()\n",
    "                target=r_lst[k]+gamma*max_q_prime\n",
    "                loss =(target-q_a)**2            \n",
    "                optimizer1.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer1.step()\n",
    "            if jlst[k]==1:\n",
    "                q_out=q2.forward(concat)\n",
    "                q_a=q_out[k]\n",
    "                #qa=q_out.topk(2)[1]\n",
    "                max_q_prime=q_target2(concat2).max()\n",
    "                target=r_lst[k]+gamma*max_q_prime\n",
    "                loss2 =(target-q_a)**2            \n",
    "                optimizer2.zero_grad()\n",
    "                loss2.backward(retain_graph=True)\n",
    "                optimizer2.step()\n",
    "            if jlst[k]==2:\n",
    "                q_out=q3.forward(concat)\n",
    "                q_a=q_out[k]\n",
    "                #qa=q_out.topk(3)[2]\n",
    "                max_q_prime=q_target3(concat2).max()\n",
    "                target=r_lst[k]+gamma*max_q_prime\n",
    "                loss3 =(target-q_a)**2            \n",
    "                optimizer3.zero_grad()\n",
    "                loss3.backward(retain_graph=True)\n",
    "                optimizer3.step()\n",
    "            if jlst[k]==3:\n",
    "                q_out=q4.forward(concat)\n",
    "                q_a=q_out[k]\n",
    "               # qa=q_out.topk(4)[3]\n",
    "                max_q_prime=q_target4(concat2).max()\n",
    "                target=r_lst[k]+gamma*max_q_prime\n",
    "                loss4 =(target-q_a)**2            \n",
    "                optimizer4.zero_grad()\n",
    "                loss4.backward(retain_graph=True)\n",
    "                optimizer4.step()\n",
    "            if jlst[k]==4:\n",
    "                q_out=q5.forward(concat)\n",
    "                q_a=q_out[k]\n",
    "                #qa=q_out.topk(5)[4]\n",
    "                max_q_prime=q_target5(concat2).max()\n",
    "                target=r_lst[k]+gamma*max_q_prime\n",
    "                loss5 =(target-q_a)**2            \n",
    "                optimizer5.zero_grad()\n",
    "                loss5.backward(retain_graph=True)\n",
    "                optimizer5.step()\n",
    "        #실제 선택한 action q_out\n",
    "\n",
    "\n",
    "#sa user가 이미 본 movie\n",
    "#데이터셋 [1,1,1,1 ] 3923\n",
    "def casadeQ(saa,q1,q2,q3,q4,q5):\n",
    "    astar=[]\n",
    "    #movienum리스트를 3923차원으로 만들고, 값을 죄다 1로 채워넣은 다음, 안되는 건 0으로 만드는 방식\n",
    "        #state에는 자신이 본 영화만 1\n",
    "    tmpmovieset=movieset\n",
    "    for s in saa:\n",
    "        tmpmovieset[s]=0\n",
    "    for i in range(5):\n",
    "\n",
    "        u=[[0,[[idx] for idx,x in enumerate(tmpmovieset) if x==1 ],[idx for idx,x in enumerate(tmpmovieset) if x==1]]]\n",
    "    #u 유저가 이미 본걸 뺀 영화 전부를 넣음 3800\n",
    "        sa=Dataset(train_data=False,userhistory=u)\n",
    "        datas=sa.data_process_for_placeholder([0])       \n",
    "   # print(datas)\n",
    "        with torch.no_grad():\n",
    "        #_,disp,_= model.embedding(datas, is_train=True)\n",
    "        \n",
    "            _, _,concat= model.embedding(datas, is_train=True)\n",
    "        #embedding\n",
    "        concat=concat.float()\n",
    "   # print(concat.type)\n",
    "   # print(concat.shape)\n",
    "    \n",
    "        #현상태에 기반해 새로운 인덱스를 만듬\n",
    "        #q값이 최대인 것을 tmpmovieset에서 하나 뽑고 tmpmovieset에서 추천한거 삭제...\n",
    "        #i값에 따라 쓰는 파라미터를 변화시켜야됨(3923,3923-1,3923-2,3923-3,3923-4)\n",
    "        #그럼 len(obs)=i일때는 3923-i-j로 일반화가능\n",
    "        if i==0:\n",
    "            out=q1.forward(concat)\n",
    "            #print(out.shape)\n",
    "            #print(out)\n",
    "            aj=torch.argmax(out)\n",
    "        if i==1:\n",
    "            out=q2.forward(concat)\n",
    "            aj=torch.argmax(out)\n",
    "        if i==2:\n",
    "            out=q3.forward(concat)\n",
    "            aj=torch.argmax(out)\n",
    "        if i==3:               \n",
    "            out=q4.forward(concat)\n",
    "            aj=torch.argmax(out)\n",
    "        if i==4:\n",
    "            out=q5.forward(concat)\n",
    "            aj=torch.argmax(out)\n",
    "\n",
    "        astar.append(aj)\n",
    "        print(i,\":\",aj)\n",
    "        tmpmovieset[aj]=0\n",
    "    return astar\n",
    "\n",
    "\n",
    "def main():\n",
    "    q1 = Qnet()\n",
    "    q2 = Qnet()\n",
    "    q3 = Qnet()\n",
    "    q4 = Qnet()\n",
    "    q5 = Qnet()\n",
    "    q_target1 = Qnet()\n",
    "    q_target2 = Qnet()\n",
    "    q_target3 = Qnet()\n",
    "    q_target4 = Qnet()\n",
    "    q_target5 = Qnet()\n",
    "    q_target1.load_state_dict(q1.state_dict())\n",
    "    q_target2.load_state_dict(q2.state_dict())\n",
    "    q_target3.load_state_dict(q3.state_dict())\n",
    "    q_target4.load_state_dict(q4.state_dict())\n",
    "    q_target5.load_state_dict(q5.state_dict())\n",
    "\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    score = 0.0  \n",
    "    optimizer1 = optim.Adam(q1.parameters(), lr=learning_rate)\n",
    "    optimizer2 = optim.Adam(q2.parameters(), lr=learning_rate)\n",
    "    optimizer3 = optim.Adam(q3.parameters(), lr=learning_rate)\n",
    "    optimizer4 = optim.Adam(q4.parameters(), lr=learning_rate)\n",
    "    optimizer5 = optim.Adam(q5.parameters(), lr=learning_rate)\n",
    "    for n_epi in range(30):\n",
    "        epsilon =max(0.01, 0.1 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "        # #state 비우기 같은 느낌, user1명에 대해 \n",
    "        # dataset=Dataset()\n",
    "        # training_user_nos = np.random.choice(dataset.train_user, 10, replace=False)\n",
    "        # training_user = dataset.data_process_for_placeholder(training_user_nos)\n",
    "        for t in range(5):\n",
    "            s=[]\n",
    "            for j in range(5):\n",
    "                coin = random.random()\n",
    "                if coin<epsilon:\n",
    "                    print(\"rand\")\n",
    "                    #movielist에서 무작위로 뽑아서 추천해줌\n",
    "                    tmp = list(moviedict.keys())\n",
    "                    randlist = random.sample(tmp,5)\n",
    "                else:\n",
    "                    print(\"cQ\")\n",
    "                    randlist=casadeQ(s,q1,q2,q3,q4,q5)\n",
    "                #모델에 action을 넣어서 ... st,at,r(s,t),s(t+1)을 구해야되는데\n",
    "                # st를 임베딩을 해야  \n",
    "                #movieset에서 빈값이 있음...\n",
    "                \n",
    "                sa=[[[0],[],[]]]\n",
    "                sa[0][1].append([randlist[0],randlist[2],randlist[4]])\n",
    "                sa[0][1].append([randlist[1],randlist[3]])\n",
    "                sa[0][2]=[randlist[0],randlist[1]]\n",
    "                \n",
    "                sa=Dataset(train_data=False,userhistory=sa)\n",
    "                tests=sa.data_process_for_placeholder([0])\n",
    "                u_disp, _, _,= model.embedding(tests, is_train=True)\n",
    "                    #[1,2,3,4,5]\n",
    "                    #[?,?,5,?,?]\n",
    "                #disp는 선택한 값의 reward가 담겨져 있음.\n",
    "                #r은 disp중에 max\n",
    "                print(u_disp.shape)\n",
    "                r=max(u_disp)\n",
    "                score+=r\n",
    "                #s_prime은 그 R값의 인덱스가 해당하는 영화의므로, randlist에 가서 찾아 s_prime으로 한다.\n",
    "                s_p=randlist[torch.argmax(u_disp)]\n",
    "                print(randlist[torch.argmax(u_disp)])\n",
    "                s_t=s+[s_p]\n",
    "                print(s_t)\n",
    "                #transition=[현상태,액션(추천목록) ,reward,s_t ]\n",
    "                transition=[s,randlist,r,s_t,j]\n",
    "                memory.put(transition)\n",
    "                s=s_t\n",
    "                print(memory.size())\n",
    "\n",
    "                if memory.size()%10==0 and n_epi!=0:\n",
    "                    train(q1,q2,q3,q4,q5, q_target1,q_target2,q_target3,q_target4,q_target5, memory, optimizer1,optimizer2,optimizer3,optimizer4,optimizer5)\n",
    "                if memory.size()%10==0 and n_epi!=0:\n",
    "                    q_target1.load_state_dict(q1.state_dict())\n",
    "                    q_target2.load_state_dict(q2.state_dict())\n",
    "                    q_target3.load_state_dict(q3.state_dict())\n",
    "                    q_target4.load_state_dict(q4.state_dict())\n",
    "                    q_target5.load_state_dict(q5.state_dict())\n",
    "                    print(\"epi :\",n_epi)\n",
    "                    print(\"score\",score)\n",
    "                    #print(\"n_episode :{}, score : {:.1f}, eps : {:.1f}%\".format(n_epi, score/print_interval,  )\n",
    "                    score = 0.0            \n",
    "\n",
    "        \n",
    "           \n",
    "main()\n",
    "#250번에 72분...?\n",
    "#영화가 3952개인 모델인데도 이렇게 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53964d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0], [[0, 2, 4], [1, 3]], [0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "randlist=[0,1,2,3,4]\n",
    "sa=[[[0],[],[]]]\n",
    "sa[0][1].append([randlist[0],randlist[2],randlist[4]])\n",
    "sa[0][1].append([randlist[1],randlist[3]])\n",
    "sa[0][2]=[randlist[0],randlist[1]]\n",
    "print(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3022b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[35]\n",
      "[36]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "[45]\n",
      "[46]\n",
      "[47]\n",
      "[48]\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "[52]\n",
      "[53]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[57]\n",
      "[58]\n",
      "[59]\n",
      "[60]\n",
      "[61]\n",
      "[62]\n",
      "[63]\n",
      "[64]\n",
      "[65]\n",
      "[66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n",
      "[76]\n",
      "[77]\n",
      "[78]\n",
      "[79]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[84]\n",
      "[85]\n",
      "[86]\n",
      "[87]\n",
      "[88]\n",
      "[89]\n",
      "[91]\n",
      "[92]\n",
      "[93]\n",
      "[94]\n",
      "[95]\n",
      "[96]\n",
      "[97]\n",
      "[98]\n",
      "[99]\n",
      "[100]\n",
      "[101]\n",
      "[102]\n",
      "[103]\n",
      "[104]\n",
      "[105]\n",
      "[106]\n",
      "[107]\n",
      "[108]\n",
      "[109]\n",
      "[110]\n",
      "[111]\n",
      "[112]\n",
      "[113]\n",
      "[114]\n",
      "[115]\n",
      "[116]\n",
      "[117]\n",
      "[118]\n",
      "[119]\n",
      "[120]\n",
      "[121]\n",
      "[122]\n",
      "[123]\n",
      "[124]\n",
      "[125]\n",
      "[126]\n",
      "[127]\n",
      "[128]\n",
      "[129]\n",
      "[130]\n",
      "[131]\n",
      "[132]\n",
      "[133]\n",
      "[134]\n",
      "[135]\n",
      "[136]\n",
      "[137]\n",
      "[138]\n",
      "[139]\n",
      "[140]\n",
      "[141]\n",
      "[142]\n",
      "[143]\n",
      "[144]\n",
      "[145]\n",
      "[146]\n",
      "[147]\n",
      "[148]\n",
      "[149]\n",
      "[150]\n",
      "[151]\n",
      "[152]\n",
      "[153]\n",
      "[154]\n",
      "[155]\n",
      "[156]\n",
      "[157]\n",
      "[158]\n",
      "[159]\n",
      "[160]\n",
      "[161]\n",
      "[162]\n",
      "[163]\n",
      "[164]\n",
      "[165]\n",
      "[166]\n",
      "[167]\n",
      "[168]\n",
      "[169]\n",
      "[170]\n",
      "[171]\n",
      "[172]\n",
      "[173]\n",
      "[174]\n",
      "[175]\n",
      "[176]\n",
      "[177]\n",
      "[178]\n",
      "[179]\n",
      "[180]\n",
      "[181]\n",
      "[182]\n",
      "[183]\n",
      "[184]\n",
      "[185]\n",
      "[186]\n",
      "[187]\n",
      "[188]\n",
      "[189]\n",
      "[190]\n",
      "[191]\n",
      "[192]\n",
      "[193]\n",
      "[194]\n",
      "[195]\n",
      "[196]\n",
      "[197]\n",
      "[198]\n",
      "[199]\n",
      "[200]\n",
      "[201]\n",
      "[202]\n",
      "[203]\n",
      "[204]\n",
      "[205]\n",
      "[206]\n",
      "[207]\n",
      "[208]\n",
      "[209]\n",
      "[210]\n",
      "[211]\n",
      "[212]\n",
      "[213]\n",
      "[214]\n",
      "[215]\n",
      "[216]\n",
      "[217]\n",
      "[218]\n",
      "[219]\n",
      "[221]\n",
      "[222]\n",
      "[223]\n",
      "[224]\n",
      "[225]\n",
      "[226]\n",
      "[227]\n",
      "[228]\n",
      "[229]\n",
      "[230]\n",
      "[231]\n",
      "[232]\n",
      "[233]\n",
      "[234]\n",
      "[235]\n",
      "[236]\n",
      "[237]\n",
      "[238]\n",
      "[239]\n",
      "[240]\n",
      "[241]\n",
      "[242]\n",
      "[243]\n",
      "[244]\n",
      "[245]\n",
      "[246]\n",
      "[247]\n",
      "[248]\n",
      "[249]\n",
      "[250]\n",
      "[251]\n",
      "[252]\n",
      "[253]\n",
      "[254]\n",
      "[255]\n",
      "[256]\n",
      "[257]\n",
      "[258]\n",
      "[259]\n",
      "[260]\n",
      "[261]\n",
      "[262]\n",
      "[263]\n",
      "[264]\n",
      "[265]\n",
      "[266]\n",
      "[267]\n",
      "[268]\n",
      "[269]\n",
      "[270]\n",
      "[271]\n",
      "[272]\n",
      "[273]\n",
      "[274]\n",
      "[275]\n",
      "[276]\n",
      "[277]\n",
      "[278]\n",
      "[279]\n",
      "[280]\n",
      "[281]\n",
      "[282]\n",
      "[283]\n",
      "[284]\n",
      "[285]\n",
      "[286]\n",
      "[287]\n",
      "[288]\n",
      "[289]\n",
      "[291]\n",
      "[292]\n",
      "[293]\n",
      "[294]\n",
      "[295]\n",
      "[296]\n",
      "[297]\n",
      "[298]\n",
      "[299]\n",
      "[300]\n",
      "[301]\n",
      "[302]\n",
      "[303]\n",
      "[304]\n",
      "[305]\n",
      "[306]\n",
      "[307]\n",
      "[308]\n",
      "[309]\n",
      "[310]\n",
      "[311]\n",
      "[312]\n",
      "[313]\n",
      "[314]\n",
      "[315]\n",
      "[316]\n",
      "[317]\n",
      "[318]\n",
      "[319]\n",
      "[320]\n",
      "[321]\n",
      "[323]\n",
      "[324]\n",
      "[325]\n",
      "[326]\n",
      "[327]\n",
      "[328]\n",
      "[329]\n",
      "[330]\n",
      "[331]\n",
      "[332]\n",
      "[333]\n",
      "[334]\n",
      "[335]\n",
      "[336]\n",
      "[337]\n",
      "[338]\n",
      "[339]\n",
      "[340]\n",
      "[341]\n",
      "[342]\n",
      "[343]\n",
      "[344]\n",
      "[345]\n",
      "[346]\n",
      "[347]\n",
      "[348]\n",
      "[349]\n",
      "[350]\n",
      "[351]\n",
      "[352]\n",
      "[353]\n",
      "[354]\n",
      "[355]\n",
      "[356]\n",
      "[357]\n",
      "[358]\n",
      "[359]\n",
      "[360]\n",
      "[361]\n",
      "[362]\n",
      "[363]\n",
      "[364]\n",
      "[365]\n",
      "[366]\n",
      "[367]\n",
      "[368]\n",
      "[369]\n",
      "[370]\n",
      "[371]\n",
      "[372]\n",
      "[373]\n",
      "[374]\n",
      "[375]\n",
      "[376]\n",
      "[377]\n",
      "[378]\n",
      "[379]\n",
      "[380]\n",
      "[381]\n",
      "[382]\n",
      "[383]\n",
      "[384]\n",
      "[385]\n",
      "[386]\n",
      "[387]\n",
      "[388]\n",
      "[389]\n",
      "[390]\n",
      "[391]\n",
      "[392]\n",
      "[393]\n",
      "[394]\n",
      "[395]\n",
      "[396]\n",
      "[397]\n",
      "[398]\n",
      "[399]\n",
      "[400]\n",
      "[401]\n",
      "[402]\n",
      "[403]\n",
      "[404]\n",
      "[405]\n",
      "[406]\n",
      "[407]\n",
      "[408]\n",
      "[409]\n",
      "[410]\n",
      "[411]\n",
      "[412]\n",
      "[413]\n",
      "[414]\n",
      "[415]\n",
      "[416]\n",
      "[417]\n",
      "[418]\n",
      "[419]\n",
      "[420]\n",
      "[421]\n",
      "[422]\n",
      "[423]\n",
      "[424]\n",
      "[425]\n",
      "[426]\n",
      "[427]\n",
      "[428]\n",
      "[429]\n",
      "[430]\n",
      "[431]\n",
      "[432]\n",
      "[433]\n",
      "[434]\n",
      "[435]\n",
      "[436]\n",
      "[437]\n",
      "[438]\n",
      "[439]\n",
      "[440]\n",
      "[441]\n",
      "[442]\n",
      "[443]\n",
      "[444]\n",
      "[445]\n",
      "[446]\n",
      "[447]\n",
      "[448]\n",
      "[449]\n",
      "[450]\n",
      "[451]\n",
      "[452]\n",
      "[453]\n",
      "[454]\n",
      "[455]\n",
      "[456]\n",
      "[457]\n",
      "[458]\n",
      "[459]\n",
      "[460]\n",
      "[461]\n",
      "[462]\n",
      "[463]\n",
      "[464]\n",
      "[465]\n",
      "[466]\n",
      "[467]\n",
      "[468]\n",
      "[469]\n",
      "[470]\n",
      "[471]\n",
      "[472]\n",
      "[473]\n",
      "[474]\n",
      "[475]\n",
      "[476]\n",
      "[477]\n",
      "[478]\n",
      "[479]\n",
      "[480]\n",
      "[481]\n",
      "[482]\n",
      "[483]\n",
      "[484]\n",
      "[485]\n",
      "[486]\n",
      "[487]\n",
      "[488]\n",
      "[489]\n",
      "[490]\n",
      "[491]\n",
      "[492]\n",
      "[493]\n",
      "[494]\n",
      "[495]\n",
      "[496]\n",
      "[497]\n",
      "[498]\n",
      "[499]\n",
      "[500]\n",
      "[501]\n",
      "[502]\n",
      "[503]\n",
      "[504]\n",
      "[505]\n",
      "[506]\n",
      "[507]\n",
      "[508]\n",
      "[509]\n",
      "[510]\n",
      "[511]\n",
      "[512]\n",
      "[513]\n",
      "[514]\n",
      "[515]\n",
      "[516]\n",
      "[517]\n",
      "[518]\n",
      "[519]\n",
      "[520]\n",
      "[521]\n",
      "[522]\n",
      "[523]\n",
      "[524]\n",
      "[525]\n",
      "[526]\n",
      "[527]\n",
      "[528]\n",
      "[529]\n",
      "[530]\n",
      "[531]\n",
      "[532]\n",
      "[533]\n",
      "[534]\n",
      "[535]\n",
      "[536]\n",
      "[537]\n",
      "[538]\n",
      "[539]\n",
      "[540]\n",
      "[541]\n",
      "[542]\n",
      "[543]\n",
      "[545]\n",
      "[546]\n",
      "[547]\n",
      "[548]\n",
      "[549]\n",
      "[550]\n",
      "[551]\n",
      "[552]\n",
      "[553]\n",
      "[554]\n",
      "[555]\n",
      "[557]\n",
      "[558]\n",
      "[559]\n",
      "[560]\n",
      "[561]\n",
      "[562]\n",
      "[563]\n",
      "[564]\n",
      "[565]\n",
      "[566]\n",
      "[567]\n",
      "[568]\n",
      "[569]\n",
      "[570]\n",
      "[571]\n",
      "[572]\n",
      "[573]\n",
      "[574]\n",
      "[575]\n",
      "[576]\n",
      "[578]\n",
      "[579]\n",
      "[580]\n",
      "[581]\n",
      "[582]\n",
      "[583]\n",
      "[584]\n",
      "[585]\n",
      "[586]\n",
      "[587]\n",
      "[588]\n",
      "[589]\n",
      "[590]\n",
      "[591]\n",
      "[592]\n",
      "[593]\n",
      "[594]\n",
      "[595]\n",
      "[596]\n",
      "[597]\n",
      "[598]\n",
      "[599]\n",
      "[600]\n",
      "[601]\n",
      "[602]\n",
      "[603]\n",
      "[604]\n",
      "[605]\n",
      "[606]\n",
      "[607]\n",
      "[608]\n",
      "[609]\n",
      "[610]\n",
      "[611]\n",
      "[612]\n",
      "[613]\n",
      "[614]\n",
      "[615]\n",
      "[616]\n",
      "[617]\n",
      "[618]\n",
      "[619]\n",
      "[620]\n",
      "[622]\n",
      "[624]\n",
      "[625]\n",
      "[626]\n",
      "[627]\n",
      "[628]\n",
      "[629]\n",
      "[630]\n",
      "[631]\n",
      "[632]\n",
      "[633]\n",
      "[634]\n",
      "[635]\n",
      "[636]\n",
      "[637]\n",
      "[638]\n",
      "[639]\n",
      "[640]\n",
      "[641]\n",
      "[642]\n",
      "[643]\n",
      "[644]\n",
      "[646]\n",
      "[647]\n",
      "[648]\n",
      "[649]\n",
      "[650]\n",
      "[651]\n",
      "[652]\n",
      "[653]\n",
      "[654]\n",
      "[655]\n",
      "[656]\n",
      "[657]\n",
      "[658]\n",
      "[659]\n",
      "[660]\n",
      "[661]\n",
      "[662]\n",
      "[663]\n",
      "[664]\n",
      "[665]\n",
      "[666]\n",
      "[667]\n",
      "[669]\n",
      "[670]\n",
      "[671]\n",
      "[672]\n",
      "[673]\n",
      "[674]\n",
      "[675]\n",
      "[677]\n",
      "[678]\n",
      "[679]\n",
      "[680]\n",
      "[681]\n",
      "[682]\n",
      "[683]\n",
      "[684]\n",
      "[686]\n",
      "[687]\n",
      "[689]\n",
      "[690]\n",
      "[691]\n",
      "[692]\n",
      "[693]\n",
      "[694]\n",
      "[695]\n",
      "[696]\n",
      "[697]\n",
      "[698]\n",
      "[699]\n",
      "[700]\n",
      "[701]\n",
      "[702]\n",
      "[703]\n",
      "[704]\n",
      "[705]\n",
      "[706]\n",
      "[707]\n",
      "[708]\n",
      "[709]\n",
      "[710]\n",
      "[711]\n",
      "[712]\n",
      "[713]\n",
      "[714]\n",
      "[715]\n",
      "[716]\n",
      "[717]\n",
      "[718]\n",
      "[719]\n",
      "[720]\n",
      "[721]\n",
      "[722]\n",
      "[723]\n",
      "[724]\n",
      "[725]\n",
      "[726]\n",
      "[727]\n",
      "[728]\n",
      "[729]\n",
      "[730]\n",
      "[731]\n",
      "[732]\n",
      "[733]\n",
      "[734]\n",
      "[735]\n",
      "[736]\n",
      "[737]\n",
      "[738]\n",
      "[740]\n",
      "[741]\n",
      "[742]\n",
      "[743]\n",
      "[744]\n",
      "[745]\n",
      "[746]\n",
      "[747]\n",
      "[748]\n",
      "[749]\n",
      "[750]\n",
      "[751]\n",
      "[752]\n",
      "[753]\n",
      "[754]\n",
      "[755]\n",
      "[756]\n",
      "[757]\n",
      "[758]\n",
      "[759]\n",
      "[760]\n",
      "[761]\n",
      "[762]\n",
      "[763]\n",
      "[764]\n",
      "[765]\n",
      "[766]\n",
      "[767]\n",
      "[768]\n",
      "[769]\n",
      "[770]\n",
      "[771]\n",
      "[772]\n",
      "[773]\n",
      "[774]\n",
      "[775]\n",
      "[776]\n",
      "[777]\n",
      "[778]\n",
      "[779]\n",
      "[780]\n",
      "[781]\n",
      "[782]\n",
      "[783]\n",
      "[784]\n",
      "[785]\n",
      "[786]\n",
      "[787]\n",
      "[788]\n",
      "[789]\n",
      "[790]\n",
      "[791]\n",
      "[792]\n",
      "[793]\n",
      "[794]\n",
      "[795]\n",
      "[796]\n",
      "[797]\n",
      "[798]\n",
      "[799]\n",
      "[800]\n",
      "[801]\n",
      "[802]\n",
      "[803]\n",
      "[804]\n",
      "[805]\n",
      "[806]\n",
      "[807]\n",
      "[808]\n",
      "[809]\n",
      "[811]\n",
      "[812]\n",
      "[813]\n",
      "[814]\n",
      "[815]\n",
      "[817]\n",
      "[818]\n",
      "[819]\n",
      "[820]\n",
      "[821]\n",
      "[822]\n",
      "[823]\n",
      "[824]\n",
      "[825]\n",
      "[826]\n",
      "[827]\n",
      "[828]\n",
      "[829]\n",
      "[830]\n",
      "[831]\n",
      "[832]\n",
      "[833]\n",
      "[834]\n",
      "[835]\n",
      "[836]\n",
      "[837]\n",
      "[838]\n",
      "[839]\n",
      "[840]\n",
      "[841]\n",
      "[842]\n",
      "[843]\n",
      "[844]\n",
      "[845]\n",
      "[846]\n",
      "[847]\n",
      "[848]\n",
      "[849]\n",
      "[850]\n",
      "[851]\n",
      "[852]\n",
      "[853]\n",
      "[854]\n",
      "[855]\n",
      "[856]\n",
      "[857]\n",
      "[858]\n",
      "[859]\n",
      "[860]\n",
      "[861]\n",
      "[863]\n",
      "[864]\n",
      "[865]\n",
      "[866]\n",
      "[867]\n",
      "[868]\n",
      "[869]\n",
      "[870]\n",
      "[871]\n",
      "[872]\n",
      "[873]\n",
      "[874]\n",
      "[875]\n",
      "[876]\n",
      "[877]\n",
      "[878]\n",
      "[879]\n",
      "[880]\n",
      "[881]\n",
      "[883]\n",
      "[884]\n",
      "[885]\n",
      "[886]\n",
      "[888]\n",
      "[889]\n",
      "[890]\n",
      "[891]\n",
      "[892]\n",
      "[893]\n",
      "[894]\n",
      "[895]\n",
      "[896]\n",
      "[897]\n",
      "[898]\n",
      "[899]\n",
      "[900]\n",
      "[901]\n",
      "[902]\n",
      "[903]\n",
      "[904]\n",
      "[905]\n",
      "[906]\n",
      "[907]\n",
      "[908]\n",
      "[909]\n",
      "[910]\n",
      "[911]\n",
      "[912]\n",
      "[913]\n",
      "[914]\n",
      "[915]\n",
      "[916]\n",
      "[917]\n",
      "[918]\n",
      "[919]\n",
      "[920]\n",
      "[921]\n",
      "[922]\n",
      "[923]\n",
      "[924]\n",
      "[925]\n",
      "[926]\n",
      "[927]\n",
      "[928]\n",
      "[929]\n",
      "[930]\n",
      "[931]\n",
      "[932]\n",
      "[933]\n",
      "[934]\n",
      "[935]\n",
      "[936]\n",
      "[937]\n",
      "[938]\n",
      "[939]\n",
      "[940]\n",
      "[941]\n",
      "[942]\n",
      "[943]\n",
      "[944]\n",
      "[945]\n",
      "[946]\n",
      "[947]\n",
      "[948]\n",
      "[949]\n",
      "[950]\n",
      "[951]\n",
      "[952]\n",
      "[953]\n",
      "[954]\n",
      "[955]\n",
      "[956]\n",
      "[957]\n",
      "[958]\n",
      "[959]\n",
      "[960]\n",
      "[961]\n",
      "[962]\n",
      "[963]\n",
      "[964]\n",
      "[965]\n",
      "[966]\n",
      "[967]\n",
      "[968]\n",
      "[969]\n",
      "[970]\n",
      "[971]\n",
      "[972]\n",
      "[973]\n",
      "[974]\n",
      "[975]\n",
      "[976]\n",
      "[978]\n",
      "[979]\n",
      "[980]\n",
      "[981]\n",
      "[982]\n",
      "[983]\n",
      "[984]\n",
      "[985]\n",
      "[986]\n",
      "[987]\n",
      "[988]\n",
      "[989]\n",
      "[990]\n",
      "[991]\n",
      "[992]\n",
      "[993]\n",
      "[995]\n",
      "[996]\n",
      "[997]\n",
      "[998]\n",
      "[999]\n",
      "[1000]\n",
      "[1001]\n",
      "[1002]\n",
      "[1003]\n",
      "[1004]\n",
      "[1005]\n",
      "[1006]\n",
      "[1007]\n",
      "[1008]\n",
      "[1009]\n",
      "[1010]\n",
      "[1011]\n",
      "[1012]\n",
      "[1013]\n",
      "[1014]\n",
      "[1015]\n",
      "[1016]\n",
      "[1017]\n",
      "[1018]\n",
      "[1019]\n",
      "[1020]\n",
      "[1021]\n",
      "[1022]\n",
      "[1023]\n",
      "[1024]\n",
      "[1025]\n",
      "[1026]\n",
      "[1027]\n",
      "[1028]\n",
      "[1029]\n",
      "[1030]\n",
      "[1031]\n",
      "[1032]\n",
      "[1033]\n",
      "[1034]\n",
      "[1035]\n",
      "[1036]\n",
      "[1037]\n",
      "[1038]\n",
      "[1039]\n",
      "[1040]\n",
      "[1041]\n",
      "[1042]\n",
      "[1043]\n",
      "[1044]\n",
      "[1045]\n",
      "[1046]\n",
      "[1048]\n",
      "[1049]\n",
      "[1050]\n",
      "[1051]\n",
      "[1052]\n",
      "[1053]\n",
      "[1054]\n",
      "[1055]\n",
      "[1056]\n",
      "[1057]\n",
      "[1058]\n",
      "[1059]\n",
      "[1060]\n",
      "[1061]\n",
      "[1062]\n",
      "[1064]\n",
      "[1065]\n",
      "[1066]\n",
      "[1067]\n",
      "[1068]\n",
      "[1069]\n",
      "[1070]\n",
      "[1072]\n",
      "[1074]\n",
      "[1075]\n",
      "[1076]\n",
      "[1077]\n",
      "[1078]\n",
      "[1079]\n",
      "[1080]\n",
      "[1081]\n",
      "[1082]\n",
      "[1083]\n",
      "[1084]\n",
      "[1085]\n",
      "[1086]\n",
      "[1087]\n",
      "[1088]\n",
      "[1089]\n",
      "[1090]\n",
      "[1091]\n",
      "[1092]\n",
      "[1093]\n",
      "[1094]\n",
      "[1095]\n",
      "[1096]\n",
      "[1097]\n",
      "[1098]\n",
      "[1099]\n",
      "[1100]\n",
      "[1101]\n",
      "[1102]\n",
      "[1103]\n",
      "[1104]\n",
      "[1105]\n",
      "[1106]\n",
      "[1108]\n",
      "[1109]\n",
      "[1110]\n",
      "[1111]\n",
      "[1112]\n",
      "[1113]\n",
      "[1114]\n",
      "[1115]\n",
      "[1116]\n",
      "[1117]\n",
      "[1118]\n",
      "[1119]\n",
      "[1120]\n",
      "[1121]\n",
      "[1122]\n",
      "[1123]\n",
      "[1124]\n",
      "[1125]\n",
      "[1126]\n",
      "[1127]\n",
      "[1128]\n",
      "[1129]\n",
      "[1130]\n",
      "[1131]\n",
      "[1132]\n",
      "[1133]\n",
      "[1134]\n",
      "[1135]\n",
      "[1136]\n",
      "[1137]\n",
      "[1138]\n",
      "[1139]\n",
      "[1140]\n",
      "[1141]\n",
      "[1142]\n",
      "[1143]\n",
      "[1144]\n",
      "[1145]\n",
      "[1146]\n",
      "[1147]\n",
      "[1148]\n",
      "[1149]\n",
      "[1150]\n",
      "[1151]\n",
      "[1152]\n",
      "[1153]\n",
      "[1154]\n",
      "[1155]\n",
      "[1156]\n",
      "[1157]\n",
      "[1158]\n",
      "[1159]\n",
      "[1160]\n",
      "[1161]\n",
      "[1162]\n",
      "[1163]\n",
      "[1164]\n",
      "[1165]\n",
      "[1166]\n",
      "[1167]\n",
      "[1168]\n",
      "[1169]\n",
      "[1170]\n",
      "[1171]\n",
      "[1172]\n",
      "[1173]\n",
      "[1174]\n",
      "[1175]\n",
      "[1176]\n",
      "[1177]\n",
      "[1178]\n",
      "[1179]\n",
      "[1180]\n",
      "[1182]\n",
      "[1183]\n",
      "[1184]\n",
      "[1185]\n",
      "[1186]\n",
      "[1187]\n",
      "[1188]\n",
      "[1189]\n",
      "[1190]\n",
      "[1191]\n",
      "[1192]\n",
      "[1193]\n",
      "[1195]\n",
      "[1196]\n",
      "[1197]\n",
      "[1198]\n",
      "[1199]\n",
      "[1200]\n",
      "[1201]\n",
      "[1202]\n",
      "[1203]\n",
      "[1205]\n",
      "[1206]\n",
      "[1207]\n",
      "[1208]\n",
      "[1209]\n",
      "[1210]\n",
      "[1211]\n",
      "[1212]\n",
      "[1213]\n",
      "[1214]\n",
      "[1215]\n",
      "[1216]\n",
      "[1217]\n",
      "[1218]\n",
      "[1219]\n",
      "[1220]\n",
      "[1221]\n",
      "[1222]\n",
      "[1223]\n",
      "[1224]\n",
      "[1225]\n",
      "[1226]\n",
      "[1227]\n",
      "[1229]\n",
      "[1230]\n",
      "[1231]\n",
      "[1232]\n",
      "[1233]\n",
      "[1234]\n",
      "[1235]\n",
      "[1236]\n",
      "[1237]\n",
      "[1239]\n",
      "[1240]\n",
      "[1241]\n",
      "[1242]\n",
      "[1243]\n",
      "[1244]\n",
      "[1245]\n",
      "[1246]\n",
      "[1247]\n",
      "[1248]\n",
      "[1249]\n",
      "[1250]\n",
      "[1251]\n",
      "[1252]\n",
      "[1253]\n",
      "[1254]\n",
      "[1255]\n",
      "[1256]\n",
      "[1257]\n",
      "[1258]\n",
      "[1259]\n",
      "[1260]\n",
      "[1261]\n",
      "[1262]\n",
      "[1263]\n",
      "[1264]\n",
      "[1265]\n",
      "[1266]\n",
      "[1267]\n",
      "[1268]\n",
      "[1269]\n",
      "[1270]\n",
      "[1271]\n",
      "[1272]\n",
      "[1273]\n",
      "[1274]\n",
      "[1275]\n",
      "[1276]\n",
      "[1277]\n",
      "[1278]\n",
      "[1279]\n",
      "[1280]\n",
      "[1281]\n",
      "[1282]\n",
      "[1283]\n",
      "[1284]\n",
      "[1285]\n",
      "[1286]\n",
      "[1287]\n",
      "[1288]\n",
      "[1289]\n",
      "[1290]\n",
      "[1291]\n",
      "[1292]\n",
      "[1294]\n",
      "[1295]\n",
      "[1296]\n",
      "[1297]\n",
      "[1298]\n",
      "[1299]\n",
      "[1300]\n",
      "[1301]\n",
      "[1302]\n",
      "[1303]\n",
      "[1304]\n",
      "[1305]\n",
      "[1306]\n",
      "[1307]\n",
      "[1308]\n",
      "[1309]\n",
      "[1310]\n",
      "[1311]\n",
      "[1312]\n",
      "[1313]\n",
      "[1314]\n",
      "[1315]\n",
      "[1316]\n",
      "[1317]\n",
      "[1318]\n",
      "[1319]\n",
      "[1320]\n",
      "[1321]\n",
      "[1322]\n",
      "[1323]\n",
      "[1324]\n",
      "[1325]\n",
      "[1326]\n",
      "[1327]\n",
      "[1328]\n",
      "[1329]\n",
      "[1330]\n",
      "[1331]\n",
      "[1332]\n",
      "[1333]\n",
      "[1334]\n",
      "[1335]\n",
      "[1336]\n",
      "[1338]\n",
      "[1339]\n",
      "[1340]\n",
      "[1341]\n",
      "[1342]\n",
      "[1343]\n",
      "[1344]\n",
      "[1345]\n",
      "[1346]\n",
      "[1347]\n",
      "[1348]\n",
      "[1349]\n",
      "[1350]\n",
      "[1351]\n",
      "[1352]\n",
      "[1353]\n",
      "[1354]\n",
      "[1355]\n",
      "[1356]\n",
      "[1357]\n",
      "[1358]\n",
      "[1359]\n",
      "[1360]\n",
      "[1362]\n",
      "[1363]\n",
      "[1364]\n",
      "[1365]\n",
      "[1366]\n",
      "[1367]\n",
      "[1368]\n",
      "[1369]\n",
      "[1370]\n",
      "[1371]\n",
      "[1372]\n",
      "[1373]\n",
      "[1374]\n",
      "[1375]\n",
      "[1376]\n",
      "[1377]\n",
      "[1378]\n",
      "[1379]\n",
      "[1380]\n",
      "[1381]\n",
      "[1382]\n",
      "[1383]\n",
      "[1384]\n",
      "[1385]\n",
      "[1386]\n",
      "[1387]\n",
      "[1388]\n",
      "[1389]\n",
      "[1390]\n",
      "[1391]\n",
      "[1392]\n",
      "[1393]\n",
      "[1394]\n",
      "[1395]\n",
      "[1396]\n",
      "[1397]\n",
      "[1398]\n",
      "[1399]\n",
      "[1400]\n",
      "[1403]\n",
      "[1404]\n",
      "[1405]\n",
      "[1406]\n",
      "[1407]\n",
      "[1408]\n",
      "[1409]\n",
      "[1410]\n",
      "[1411]\n",
      "[1412]\n",
      "[1413]\n",
      "[1414]\n",
      "[1415]\n",
      "[1416]\n",
      "[1418]\n",
      "[1419]\n",
      "[1420]\n",
      "[1421]\n",
      "[1422]\n",
      "[1423]\n",
      "[1424]\n",
      "[1425]\n",
      "[1426]\n",
      "[1427]\n",
      "[1428]\n",
      "[1429]\n",
      "[1430]\n",
      "[1431]\n",
      "[1432]\n",
      "[1433]\n",
      "[1435]\n",
      "[1436]\n",
      "[1437]\n",
      "[1438]\n",
      "[1439]\n",
      "[1440]\n",
      "[1441]\n",
      "[1442]\n",
      "[1443]\n",
      "[1444]\n",
      "[1445]\n",
      "[1446]\n",
      "[1447]\n",
      "[1448]\n",
      "[1449]\n",
      "[1452]\n",
      "[1453]\n",
      "[1454]\n",
      "[1455]\n",
      "[1456]\n",
      "[1457]\n",
      "[1458]\n",
      "[1459]\n",
      "[1460]\n",
      "[1461]\n",
      "[1462]\n",
      "[1463]\n",
      "[1464]\n",
      "[1465]\n",
      "[1466]\n",
      "[1467]\n",
      "[1469]\n",
      "[1470]\n",
      "[1471]\n",
      "[1472]\n",
      "[1473]\n",
      "[1474]\n",
      "[1475]\n",
      "[1476]\n",
      "[1478]\n",
      "[1479]\n",
      "[1481]\n",
      "[1482]\n",
      "[1483]\n",
      "[1484]\n",
      "[1485]\n",
      "[1486]\n",
      "[1487]\n",
      "[1488]\n",
      "[1489]\n",
      "[1492]\n",
      "[1494]\n",
      "[1495]\n",
      "[1496]\n",
      "[1497]\n",
      "[1498]\n",
      "[1499]\n",
      "[1500]\n",
      "[1501]\n",
      "[1502]\n",
      "[1503]\n",
      "[1506]\n",
      "[1507]\n",
      "[1508]\n",
      "[1509]\n",
      "[1510]\n",
      "[1512]\n",
      "[1513]\n",
      "[1514]\n",
      "[1515]\n",
      "[1516]\n",
      "[1517]\n",
      "[1518]\n",
      "[1519]\n",
      "[1521]\n",
      "[1522]\n",
      "[1523]\n",
      "[1524]\n",
      "[1525]\n",
      "[1526]\n",
      "[1527]\n",
      "[1528]\n",
      "[1530]\n",
      "[1531]\n",
      "[1532]\n",
      "[1533]\n",
      "[1534]\n",
      "[1536]\n",
      "[1537]\n",
      "[1538]\n",
      "[1540]\n",
      "[1541]\n",
      "[1542]\n",
      "[1543]\n",
      "[1544]\n",
      "[1545]\n",
      "[1546]\n",
      "[1547]\n",
      "[1548]\n",
      "[1549]\n",
      "[1550]\n",
      "[1551]\n",
      "[1552]\n",
      "[1553]\n",
      "[1554]\n",
      "[1555]\n",
      "[1556]\n",
      "[1557]\n",
      "[1558]\n",
      "[1560]\n",
      "[1561]\n",
      "[1562]\n",
      "[1563]\n",
      "[1564]\n",
      "[1565]\n",
      "[1566]\n",
      "[1567]\n",
      "[1568]\n",
      "[1569]\n",
      "[1570]\n",
      "[1571]\n",
      "[1572]\n",
      "[1573]\n",
      "[1574]\n",
      "[1576]\n",
      "[1577]\n",
      "[1578]\n",
      "[1579]\n",
      "[1580]\n",
      "[1581]\n",
      "[1582]\n",
      "[1583]\n",
      "[1584]\n",
      "[1585]\n",
      "[1586]\n",
      "[1587]\n",
      "[1588]\n",
      "[1589]\n",
      "[1590]\n",
      "[1591]\n",
      "[1592]\n",
      "[1593]\n",
      "[1594]\n",
      "[1595]\n",
      "[1596]\n",
      "[1597]\n",
      "[1598]\n",
      "[1599]\n",
      "[1600]\n",
      "[1601]\n",
      "[1602]\n",
      "[1603]\n",
      "[1604]\n",
      "[1605]\n",
      "[1607]\n",
      "[1608]\n",
      "[1609]\n",
      "[1610]\n",
      "[1611]\n",
      "[1612]\n",
      "[1613]\n",
      "[1614]\n",
      "[1615]\n",
      "[1616]\n",
      "[1618]\n",
      "[1619]\n",
      "[1620]\n",
      "[1621]\n",
      "[1622]\n",
      "[1623]\n",
      "[1624]\n",
      "[1625]\n",
      "[1626]\n",
      "[1627]\n",
      "[1628]\n",
      "[1629]\n",
      "[1630]\n",
      "[1631]\n",
      "[1632]\n",
      "[1634]\n",
      "[1635]\n",
      "[1638]\n",
      "[1639]\n",
      "[1640]\n",
      "[1641]\n",
      "[1642]\n",
      "[1643]\n",
      "[1644]\n",
      "[1645]\n",
      "[1646]\n",
      "[1647]\n",
      "[1648]\n",
      "[1649]\n",
      "[1650]\n",
      "[1651]\n",
      "[1652]\n",
      "[1653]\n",
      "[1654]\n",
      "[1655]\n",
      "[1656]\n",
      "[1657]\n",
      "[1658]\n",
      "[1659]\n",
      "[1660]\n",
      "[1661]\n",
      "[1662]\n",
      "[1663]\n",
      "[1664]\n",
      "[1665]\n",
      "[1666]\n",
      "[1667]\n",
      "[1668]\n",
      "[1669]\n",
      "[1670]\n",
      "[1671]\n",
      "[1672]\n",
      "[1673]\n",
      "[1674]\n",
      "[1675]\n",
      "[1676]\n",
      "[1677]\n",
      "[1678]\n",
      "[1679]\n",
      "[1680]\n",
      "[1681]\n",
      "[1682]\n",
      "[1683]\n",
      "[1684]\n",
      "[1685]\n",
      "[1686]\n",
      "[1687]\n",
      "[1688]\n",
      "[1689]\n",
      "[1691]\n",
      "[1692]\n",
      "[1693]\n",
      "[1694]\n",
      "[1695]\n",
      "[1696]\n",
      "[1697]\n",
      "[1698]\n",
      "[1700]\n",
      "[1701]\n",
      "[1702]\n",
      "[1703]\n",
      "[1704]\n",
      "[1705]\n",
      "[1706]\n",
      "[1707]\n",
      "[1708]\n",
      "[1710]\n",
      "[1712]\n",
      "[1713]\n",
      "[1714]\n",
      "[1715]\n",
      "[1716]\n",
      "[1717]\n",
      "[1718]\n",
      "[1719]\n",
      "[1720]\n",
      "[1721]\n",
      "[1722]\n",
      "[1723]\n",
      "[1724]\n",
      "[1725]\n",
      "[1726]\n",
      "[1727]\n",
      "[1728]\n",
      "[1729]\n",
      "[1730]\n",
      "[1731]\n",
      "[1732]\n",
      "[1733]\n",
      "[1734]\n",
      "[1737]\n",
      "[1738]\n",
      "[1739]\n",
      "[1741]\n",
      "[1742]\n",
      "[1743]\n",
      "[1745]\n",
      "[1746]\n",
      "[1747]\n",
      "[1748]\n",
      "[1749]\n",
      "[1751]\n",
      "[1752]\n",
      "[1753]\n",
      "[1754]\n",
      "[1755]\n",
      "[1756]\n",
      "[1758]\n",
      "[1759]\n",
      "[1761]\n",
      "[1763]\n",
      "[1764]\n",
      "[1766]\n",
      "[1767]\n",
      "[1768]\n",
      "[1769]\n",
      "[1770]\n",
      "[1771]\n",
      "[1772]\n",
      "[1773]\n",
      "[1775]\n",
      "[1776]\n",
      "[1778]\n",
      "[1779]\n",
      "[1780]\n",
      "[1781]\n",
      "[1782]\n",
      "[1783]\n",
      "[1784]\n",
      "[1786]\n",
      "[1787]\n",
      "[1788]\n",
      "[1790]\n",
      "[1791]\n",
      "[1792]\n",
      "[1793]\n",
      "[1794]\n",
      "[1795]\n",
      "[1796]\n",
      "[1797]\n",
      "[1798]\n",
      "[1800]\n",
      "[1803]\n",
      "[1804]\n",
      "[1805]\n",
      "[1806]\n",
      "[1808]\n",
      "[1809]\n",
      "[1810]\n",
      "[1811]\n",
      "[1813]\n",
      "[1814]\n",
      "[1815]\n",
      "[1816]\n",
      "[1818]\n",
      "[1819]\n",
      "[1820]\n",
      "[1821]\n",
      "[1823]\n",
      "[1824]\n",
      "[1825]\n",
      "[1826]\n",
      "[1828]\n",
      "[1829]\n",
      "[1830]\n",
      "[1831]\n",
      "[1832]\n",
      "[1833]\n",
      "[1834]\n",
      "[1835]\n",
      "[1836]\n",
      "[1838]\n",
      "[1839]\n",
      "[1840]\n",
      "[1841]\n",
      "[1842]\n",
      "[1843]\n",
      "[1844]\n",
      "[1845]\n",
      "[1846]\n",
      "[1847]\n",
      "[1848]\n",
      "[1849]\n",
      "[1850]\n",
      "[1851]\n",
      "[1852]\n",
      "[1853]\n",
      "[1854]\n",
      "[1855]\n",
      "[1856]\n",
      "[1857]\n",
      "[1858]\n",
      "[1859]\n",
      "[1860]\n",
      "[1861]\n",
      "[1862]\n",
      "[1863]\n",
      "[1864]\n",
      "[1865]\n",
      "[1866]\n",
      "[1868]\n",
      "[1869]\n",
      "[1870]\n",
      "[1871]\n",
      "[1872]\n",
      "[1873]\n",
      "[1874]\n",
      "[1875]\n",
      "[1876]\n",
      "[1877]\n",
      "[1878]\n",
      "[1879]\n",
      "[1880]\n",
      "[1881]\n",
      "[1882]\n",
      "[1883]\n",
      "[1884]\n",
      "[1885]\n",
      "[1886]\n",
      "[1887]\n",
      "[1888]\n",
      "[1889]\n",
      "[1890]\n",
      "[1891]\n",
      "[1892]\n",
      "[1893]\n",
      "[1894]\n",
      "[1895]\n",
      "[1896]\n",
      "[1897]\n",
      "[1898]\n",
      "[1899]\n",
      "[1900]\n",
      "[1901]\n",
      "[1902]\n",
      "[1903]\n",
      "[1904]\n",
      "[1905]\n",
      "[1906]\n",
      "[1907]\n",
      "[1908]\n",
      "[1909]\n",
      "[1910]\n",
      "[1911]\n",
      "[1912]\n",
      "[1913]\n",
      "[1914]\n",
      "[1915]\n",
      "[1916]\n",
      "[1917]\n",
      "[1918]\n",
      "[1919]\n",
      "[1920]\n",
      "[1921]\n",
      "[1922]\n",
      "[1923]\n",
      "[1924]\n",
      "[1925]\n",
      "[1926]\n",
      "[1927]\n",
      "[1928]\n",
      "[1929]\n",
      "[1930]\n",
      "[1931]\n",
      "[1932]\n",
      "[1933]\n",
      "[1934]\n",
      "[1935]\n",
      "[1936]\n",
      "[1937]\n",
      "[1938]\n",
      "[1939]\n",
      "[1940]\n",
      "[1941]\n",
      "[1942]\n",
      "[1943]\n",
      "[1944]\n",
      "[1945]\n",
      "[1946]\n",
      "[1947]\n",
      "[1948]\n",
      "[1949]\n",
      "[1950]\n",
      "[1951]\n",
      "[1952]\n",
      "[1953]\n",
      "[1954]\n",
      "[1955]\n",
      "[1956]\n",
      "[1957]\n",
      "[1958]\n",
      "[1959]\n",
      "[1960]\n",
      "[1961]\n",
      "[1962]\n",
      "[1963]\n",
      "[1964]\n",
      "[1965]\n",
      "[1966]\n",
      "[1967]\n",
      "[1968]\n",
      "[1969]\n",
      "[1970]\n",
      "[1971]\n",
      "[1972]\n",
      "[1973]\n",
      "[1974]\n",
      "[1975]\n",
      "[1976]\n",
      "[1977]\n",
      "[1978]\n",
      "[1979]\n",
      "[1980]\n",
      "[1981]\n",
      "[1982]\n",
      "[1983]\n",
      "[1984]\n",
      "[1985]\n",
      "[1986]\n",
      "[1987]\n",
      "[1988]\n",
      "[1989]\n",
      "[1990]\n",
      "[1991]\n",
      "[1992]\n",
      "[1993]\n",
      "[1994]\n",
      "[1995]\n",
      "[1996]\n",
      "[1997]\n",
      "[1998]\n",
      "[1999]\n",
      "[2000]\n",
      "[2001]\n",
      "[2002]\n",
      "[2003]\n",
      "[2004]\n",
      "[2005]\n",
      "[2006]\n",
      "[2007]\n",
      "[2008]\n",
      "[2009]\n",
      "[2010]\n",
      "[2011]\n",
      "[2012]\n",
      "[2013]\n",
      "[2014]\n",
      "[2015]\n",
      "[2016]\n",
      "[2017]\n",
      "[2018]\n",
      "[2019]\n",
      "[2020]\n",
      "[2021]\n",
      "[2022]\n",
      "[2023]\n",
      "[2024]\n",
      "[2025]\n",
      "[2026]\n",
      "[2027]\n",
      "[2028]\n",
      "[2029]\n",
      "[2030]\n",
      "[2031]\n",
      "[2032]\n",
      "[2033]\n",
      "[2034]\n",
      "[2035]\n",
      "[2036]\n",
      "[2037]\n",
      "[2038]\n",
      "[2039]\n",
      "[2040]\n",
      "[2041]\n",
      "[2042]\n",
      "[2043]\n",
      "[2044]\n",
      "[2045]\n",
      "[2046]\n",
      "[2047]\n",
      "[2048]\n",
      "[2049]\n",
      "[2050]\n",
      "[2051]\n",
      "[2052]\n",
      "[2053]\n",
      "[2054]\n",
      "[2055]\n",
      "[2056]\n",
      "[2057]\n",
      "[2058]\n",
      "[2059]\n",
      "[2060]\n",
      "[2061]\n",
      "[2062]\n",
      "[2063]\n",
      "[2064]\n",
      "[2065]\n",
      "[2066]\n",
      "[2067]\n",
      "[2068]\n",
      "[2069]\n",
      "[2070]\n",
      "[2071]\n",
      "[2072]\n",
      "[2073]\n",
      "[2074]\n",
      "[2075]\n",
      "[2076]\n",
      "[2077]\n",
      "[2078]\n",
      "[2079]\n",
      "[2080]\n",
      "[2081]\n",
      "[2082]\n",
      "[2083]\n",
      "[2084]\n",
      "[2085]\n",
      "[2086]\n",
      "[2087]\n",
      "[2088]\n",
      "[2089]\n",
      "[2090]\n",
      "[2092]\n",
      "[2093]\n",
      "[2094]\n",
      "[2095]\n",
      "[2096]\n",
      "[2097]\n",
      "[2098]\n",
      "[2099]\n",
      "[2100]\n",
      "[2101]\n",
      "[2102]\n",
      "[2103]\n",
      "[2104]\n",
      "[2105]\n",
      "[2106]\n",
      "[2107]\n",
      "[2108]\n",
      "[2109]\n",
      "[2110]\n",
      "[2111]\n",
      "[2112]\n",
      "[2113]\n",
      "[2114]\n",
      "[2115]\n",
      "[2116]\n",
      "[2117]\n",
      "[2118]\n",
      "[2119]\n",
      "[2120]\n",
      "[2121]\n",
      "[2122]\n",
      "[2123]\n",
      "[2124]\n",
      "[2125]\n",
      "[2126]\n",
      "[2127]\n",
      "[2128]\n",
      "[2129]\n",
      "[2130]\n",
      "[2131]\n",
      "[2132]\n",
      "[2133]\n",
      "[2134]\n",
      "[2135]\n",
      "[2136]\n",
      "[2137]\n",
      "[2138]\n",
      "[2139]\n",
      "[2140]\n",
      "[2141]\n",
      "[2142]\n",
      "[2143]\n",
      "[2144]\n",
      "[2145]\n",
      "[2146]\n",
      "[2147]\n",
      "[2148]\n",
      "[2149]\n",
      "[2150]\n",
      "[2151]\n",
      "[2152]\n",
      "[2153]\n",
      "[2154]\n",
      "[2155]\n",
      "[2156]\n",
      "[2157]\n",
      "[2158]\n",
      "[2159]\n",
      "[2160]\n",
      "[2161]\n",
      "[2162]\n",
      "[2163]\n",
      "[2164]\n",
      "[2165]\n",
      "[2166]\n",
      "[2167]\n",
      "[2168]\n",
      "[2169]\n",
      "[2170]\n",
      "[2171]\n",
      "[2172]\n",
      "[2173]\n",
      "[2174]\n",
      "[2175]\n",
      "[2176]\n",
      "[2177]\n",
      "[2178]\n",
      "[2179]\n",
      "[2180]\n",
      "[2181]\n",
      "[2182]\n",
      "[2183]\n",
      "[2184]\n",
      "[2185]\n",
      "[2186]\n",
      "[2187]\n",
      "[2188]\n",
      "[2189]\n",
      "[2190]\n",
      "[2191]\n",
      "[2192]\n",
      "[2193]\n",
      "[2194]\n",
      "[2195]\n",
      "[2196]\n",
      "[2197]\n",
      "[2198]\n",
      "[2199]\n",
      "[2200]\n",
      "[2201]\n",
      "[2202]\n",
      "[2203]\n",
      "[2204]\n",
      "[2205]\n",
      "[2206]\n",
      "[2207]\n",
      "[2208]\n",
      "[2209]\n",
      "[2210]\n",
      "[2211]\n",
      "[2212]\n",
      "[2213]\n",
      "[2214]\n",
      "[2215]\n",
      "[2216]\n",
      "[2217]\n",
      "[2218]\n",
      "[2219]\n",
      "[2220]\n",
      "[2221]\n",
      "[2222]\n",
      "[2223]\n",
      "[2224]\n",
      "[2225]\n",
      "[2226]\n",
      "[2228]\n",
      "[2229]\n",
      "[2230]\n",
      "[2231]\n",
      "[2232]\n",
      "[2233]\n",
      "[2234]\n",
      "[2235]\n",
      "[2236]\n",
      "[2237]\n",
      "[2238]\n",
      "[2239]\n",
      "[2240]\n",
      "[2241]\n",
      "[2242]\n",
      "[2243]\n",
      "[2244]\n",
      "[2245]\n",
      "[2246]\n",
      "[2247]\n",
      "[2248]\n",
      "[2249]\n",
      "[2250]\n",
      "[2251]\n",
      "[2252]\n",
      "[2253]\n",
      "[2254]\n",
      "[2255]\n",
      "[2256]\n",
      "[2257]\n",
      "[2258]\n",
      "[2259]\n",
      "[2260]\n",
      "[2261]\n",
      "[2262]\n",
      "[2263]\n",
      "[2264]\n",
      "[2265]\n",
      "[2266]\n",
      "[2267]\n",
      "[2268]\n",
      "[2269]\n",
      "[2270]\n",
      "[2271]\n",
      "[2272]\n",
      "[2273]\n",
      "[2274]\n",
      "[2275]\n",
      "[2276]\n",
      "[2277]\n",
      "[2278]\n",
      "[2279]\n",
      "[2280]\n",
      "[2281]\n",
      "[2282]\n",
      "[2283]\n",
      "[2284]\n",
      "[2285]\n",
      "[2286]\n",
      "[2287]\n",
      "[2288]\n",
      "[2289]\n",
      "[2290]\n",
      "[2291]\n",
      "[2292]\n",
      "[2293]\n",
      "[2294]\n",
      "[2295]\n",
      "[2296]\n",
      "[2297]\n",
      "[2298]\n",
      "[2299]\n",
      "[2300]\n",
      "[2301]\n",
      "[2302]\n",
      "[2303]\n",
      "[2304]\n",
      "[2305]\n",
      "[2306]\n",
      "[2307]\n",
      "[2308]\n",
      "[2309]\n",
      "[2310]\n",
      "[2311]\n",
      "[2312]\n",
      "[2313]\n",
      "[2314]\n",
      "[2315]\n",
      "[2316]\n",
      "[2317]\n",
      "[2318]\n",
      "[2319]\n",
      "[2320]\n",
      "[2321]\n",
      "[2322]\n",
      "[2323]\n",
      "[2324]\n",
      "[2325]\n",
      "[2326]\n",
      "[2327]\n",
      "[2328]\n",
      "[2329]\n",
      "[2330]\n",
      "[2331]\n",
      "[2332]\n",
      "[2333]\n",
      "[2334]\n",
      "[2335]\n",
      "[2336]\n",
      "[2337]\n",
      "[2338]\n",
      "[2339]\n",
      "[2340]\n",
      "[2341]\n",
      "[2342]\n",
      "[2343]\n",
      "[2344]\n",
      "[2345]\n",
      "[2346]\n",
      "[2347]\n",
      "[2348]\n",
      "[2349]\n",
      "[2350]\n",
      "[2351]\n",
      "[2352]\n",
      "[2353]\n",
      "[2354]\n",
      "[2355]\n",
      "[2356]\n",
      "[2357]\n",
      "[2358]\n",
      "[2359]\n",
      "[2360]\n",
      "[2361]\n",
      "[2362]\n",
      "[2363]\n",
      "[2364]\n",
      "[2365]\n",
      "[2366]\n",
      "[2367]\n",
      "[2368]\n",
      "[2369]\n",
      "[2370]\n",
      "[2371]\n",
      "[2372]\n",
      "[2373]\n",
      "[2374]\n",
      "[2375]\n",
      "[2376]\n",
      "[2377]\n",
      "[2378]\n",
      "[2379]\n",
      "[2380]\n",
      "[2381]\n",
      "[2382]\n",
      "[2383]\n",
      "[2384]\n",
      "[2385]\n",
      "[2386]\n",
      "[2387]\n",
      "[2388]\n",
      "[2389]\n",
      "[2390]\n",
      "[2391]\n",
      "[2392]\n",
      "[2393]\n",
      "[2394]\n",
      "[2395]\n",
      "[2396]\n",
      "[2397]\n",
      "[2398]\n",
      "[2399]\n",
      "[2400]\n",
      "[2401]\n",
      "[2402]\n",
      "[2403]\n",
      "[2404]\n",
      "[2405]\n",
      "[2406]\n",
      "[2407]\n",
      "[2408]\n",
      "[2409]\n",
      "[2410]\n",
      "[2411]\n",
      "[2412]\n",
      "[2413]\n",
      "[2414]\n",
      "[2415]\n",
      "[2416]\n",
      "[2417]\n",
      "[2418]\n",
      "[2419]\n",
      "[2420]\n",
      "[2421]\n",
      "[2422]\n",
      "[2423]\n",
      "[2424]\n",
      "[2425]\n",
      "[2426]\n",
      "[2427]\n",
      "[2428]\n",
      "[2429]\n",
      "[2430]\n",
      "[2431]\n",
      "[2432]\n",
      "[2433]\n",
      "[2434]\n",
      "[2435]\n",
      "[2436]\n",
      "[2437]\n",
      "[2438]\n",
      "[2439]\n",
      "[2440]\n",
      "[2441]\n",
      "[2442]\n",
      "[2443]\n",
      "[2444]\n",
      "[2445]\n",
      "[2446]\n",
      "[2447]\n",
      "[2448]\n",
      "[2449]\n",
      "[2450]\n",
      "[2451]\n",
      "[2452]\n",
      "[2453]\n",
      "[2454]\n",
      "[2455]\n",
      "[2456]\n",
      "[2457]\n",
      "[2458]\n",
      "[2459]\n",
      "[2460]\n",
      "[2461]\n",
      "[2462]\n",
      "[2463]\n",
      "[2464]\n",
      "[2465]\n",
      "[2466]\n",
      "[2467]\n",
      "[2468]\n",
      "[2469]\n",
      "[2470]\n",
      "[2471]\n",
      "[2472]\n",
      "[2473]\n",
      "[2474]\n",
      "[2475]\n",
      "[2476]\n",
      "[2477]\n",
      "[2478]\n",
      "[2479]\n",
      "[2480]\n",
      "[2481]\n",
      "[2482]\n",
      "[2483]\n",
      "[2484]\n",
      "[2485]\n",
      "[2486]\n",
      "[2487]\n",
      "[2488]\n",
      "[2489]\n",
      "[2490]\n",
      "[2491]\n",
      "[2492]\n",
      "[2493]\n",
      "[2494]\n",
      "[2495]\n",
      "[2496]\n",
      "[2497]\n",
      "[2498]\n",
      "[2499]\n",
      "[2500]\n",
      "[2501]\n",
      "[2502]\n",
      "[2503]\n",
      "[2504]\n",
      "[2505]\n",
      "[2506]\n",
      "[2507]\n",
      "[2508]\n",
      "[2509]\n",
      "[2510]\n",
      "[2511]\n",
      "[2512]\n",
      "[2513]\n",
      "[2514]\n",
      "[2515]\n",
      "[2516]\n",
      "[2517]\n",
      "[2518]\n",
      "[2519]\n",
      "[2520]\n",
      "[2521]\n",
      "[2522]\n",
      "[2523]\n",
      "[2524]\n",
      "[2525]\n",
      "[2526]\n",
      "[2527]\n",
      "[2528]\n",
      "[2529]\n",
      "[2530]\n",
      "[2531]\n",
      "[2532]\n",
      "[2533]\n",
      "[2534]\n",
      "[2535]\n",
      "[2536]\n",
      "[2537]\n",
      "[2538]\n",
      "[2539]\n",
      "[2540]\n",
      "[2541]\n",
      "[2542]\n",
      "[2543]\n",
      "[2544]\n",
      "[2545]\n",
      "[2546]\n",
      "[2547]\n",
      "[2548]\n",
      "[2549]\n",
      "[2550]\n",
      "[2551]\n",
      "[2552]\n",
      "[2553]\n",
      "[2554]\n",
      "[2555]\n",
      "[2556]\n",
      "[2557]\n",
      "[2558]\n",
      "[2559]\n",
      "[2560]\n",
      "[2561]\n",
      "[2562]\n",
      "[2563]\n",
      "[2564]\n",
      "[2565]\n",
      "[2566]\n",
      "[2567]\n",
      "[2568]\n",
      "[2569]\n",
      "[2570]\n",
      "[2571]\n",
      "[2572]\n",
      "[2573]\n",
      "[2574]\n",
      "[2575]\n",
      "[2576]\n",
      "[2577]\n",
      "[2578]\n",
      "[2579]\n",
      "[2580]\n",
      "[2581]\n",
      "[2582]\n",
      "[2583]\n",
      "[2584]\n",
      "[2585]\n",
      "[2586]\n",
      "[2587]\n",
      "[2588]\n",
      "[2589]\n",
      "[2590]\n",
      "[2591]\n",
      "[2592]\n",
      "[2593]\n",
      "[2594]\n",
      "[2595]\n",
      "[2596]\n",
      "[2597]\n",
      "[2598]\n",
      "[2599]\n",
      "[2600]\n",
      "[2601]\n",
      "[2602]\n",
      "[2603]\n",
      "[2604]\n",
      "[2605]\n",
      "[2606]\n",
      "[2607]\n",
      "[2608]\n",
      "[2609]\n",
      "[2610]\n",
      "[2611]\n",
      "[2612]\n",
      "[2613]\n",
      "[2614]\n",
      "[2615]\n",
      "[2616]\n",
      "[2617]\n",
      "[2618]\n",
      "[2619]\n",
      "[2620]\n",
      "[2621]\n",
      "[2622]\n",
      "[2623]\n",
      "[2624]\n",
      "[2625]\n",
      "[2626]\n",
      "[2627]\n",
      "[2628]\n",
      "[2629]\n",
      "[2630]\n",
      "[2631]\n",
      "[2632]\n",
      "[2633]\n",
      "[2634]\n",
      "[2635]\n",
      "[2636]\n",
      "[2637]\n",
      "[2638]\n",
      "[2639]\n",
      "[2640]\n",
      "[2641]\n",
      "[2642]\n",
      "[2643]\n",
      "[2645]\n",
      "[2646]\n",
      "[2647]\n",
      "[2648]\n",
      "[2649]\n",
      "[2650]\n",
      "[2651]\n",
      "[2652]\n",
      "[2653]\n",
      "[2654]\n",
      "[2655]\n",
      "[2656]\n",
      "[2657]\n",
      "[2658]\n",
      "[2659]\n",
      "[2660]\n",
      "[2661]\n",
      "[2662]\n",
      "[2663]\n",
      "[2664]\n",
      "[2665]\n",
      "[2666]\n",
      "[2667]\n",
      "[2668]\n",
      "[2669]\n",
      "[2670]\n",
      "[2671]\n",
      "[2672]\n",
      "[2673]\n",
      "[2674]\n",
      "[2675]\n",
      "[2676]\n",
      "[2677]\n",
      "[2678]\n",
      "[2679]\n",
      "[2680]\n",
      "[2681]\n",
      "[2682]\n",
      "[2683]\n",
      "[2684]\n",
      "[2685]\n",
      "[2686]\n",
      "[2687]\n",
      "[2688]\n",
      "[2689]\n",
      "[2690]\n",
      "[2691]\n",
      "[2692]\n",
      "[2693]\n",
      "[2694]\n",
      "[2695]\n",
      "[2696]\n",
      "[2697]\n",
      "[2698]\n",
      "[2699]\n",
      "[2700]\n",
      "[2701]\n",
      "[2702]\n",
      "[2703]\n",
      "[2704]\n",
      "[2705]\n",
      "[2706]\n",
      "[2707]\n",
      "[2708]\n",
      "[2709]\n",
      "[2710]\n",
      "[2711]\n",
      "[2712]\n",
      "[2713]\n",
      "[2714]\n",
      "[2715]\n",
      "[2716]\n",
      "[2717]\n",
      "[2718]\n",
      "[2719]\n",
      "[2720]\n",
      "[2721]\n",
      "[2722]\n",
      "[2723]\n",
      "[2724]\n",
      "[2725]\n",
      "[2726]\n",
      "[2727]\n",
      "[2728]\n",
      "[2729]\n",
      "[2730]\n",
      "[2731]\n",
      "[2732]\n",
      "[2733]\n",
      "[2734]\n",
      "[2735]\n",
      "[2736]\n",
      "[2737]\n",
      "[2738]\n",
      "[2739]\n",
      "[2740]\n",
      "[2741]\n",
      "[2742]\n",
      "[2743]\n",
      "[2744]\n",
      "[2745]\n",
      "[2746]\n",
      "[2747]\n",
      "[2748]\n",
      "[2749]\n",
      "[2750]\n",
      "[2751]\n",
      "[2752]\n",
      "[2753]\n",
      "[2754]\n",
      "[2755]\n",
      "[2756]\n",
      "[2757]\n",
      "[2758]\n",
      "[2759]\n",
      "[2760]\n",
      "[2761]\n",
      "[2762]\n",
      "[2763]\n",
      "[2764]\n",
      "[2765]\n",
      "[2766]\n",
      "[2767]\n",
      "[2768]\n",
      "[2769]\n",
      "[2770]\n",
      "[2771]\n",
      "[2772]\n",
      "[2773]\n",
      "[2774]\n",
      "[2775]\n",
      "[2776]\n",
      "[2777]\n",
      "[2778]\n",
      "[2779]\n",
      "[2780]\n",
      "[2781]\n",
      "[2782]\n",
      "[2783]\n",
      "[2784]\n",
      "[2785]\n",
      "[2786]\n",
      "[2787]\n",
      "[2788]\n",
      "[2789]\n",
      "[2790]\n",
      "[2791]\n",
      "[2792]\n",
      "[2793]\n",
      "[2794]\n",
      "[2795]\n",
      "[2796]\n",
      "[2797]\n",
      "[2798]\n",
      "[2799]\n",
      "[2800]\n",
      "[2801]\n",
      "[2802]\n",
      "[2803]\n",
      "[2804]\n",
      "[2805]\n",
      "[2806]\n",
      "[2807]\n",
      "[2808]\n",
      "[2809]\n",
      "[2810]\n",
      "[2811]\n",
      "[2812]\n",
      "[2813]\n",
      "[2814]\n",
      "[2815]\n",
      "[2816]\n",
      "[2817]\n",
      "[2818]\n",
      "[2819]\n",
      "[2820]\n",
      "[2821]\n",
      "[2822]\n",
      "[2823]\n",
      "[2824]\n",
      "[2825]\n",
      "[2826]\n",
      "[2827]\n",
      "[2828]\n",
      "[2829]\n",
      "[2830]\n",
      "[2831]\n",
      "[2832]\n",
      "[2833]\n",
      "[2834]\n",
      "[2835]\n",
      "[2836]\n",
      "[2837]\n",
      "[2838]\n",
      "[2839]\n",
      "[2840]\n",
      "[2841]\n",
      "[2842]\n",
      "[2843]\n",
      "[2844]\n",
      "[2845]\n",
      "[2846]\n",
      "[2847]\n",
      "[2848]\n",
      "[2849]\n",
      "[2850]\n",
      "[2851]\n",
      "[2852]\n",
      "[2853]\n",
      "[2854]\n",
      "[2855]\n",
      "[2856]\n",
      "[2857]\n",
      "[2858]\n",
      "[2859]\n",
      "[2860]\n",
      "[2861]\n",
      "[2862]\n",
      "[2863]\n",
      "[2864]\n",
      "[2865]\n",
      "[2866]\n",
      "[2867]\n",
      "[2868]\n",
      "[2869]\n",
      "[2870]\n",
      "[2871]\n",
      "[2872]\n",
      "[2873]\n",
      "[2874]\n",
      "[2875]\n",
      "[2876]\n",
      "[2877]\n",
      "[2878]\n",
      "[2879]\n",
      "[2880]\n",
      "[2881]\n",
      "[2882]\n",
      "[2883]\n",
      "[2884]\n",
      "[2885]\n",
      "[2886]\n",
      "[2887]\n",
      "[2888]\n",
      "[2889]\n",
      "[2890]\n",
      "[2891]\n",
      "[2892]\n",
      "[2893]\n",
      "[2894]\n",
      "[2895]\n",
      "[2896]\n",
      "[2897]\n",
      "[2898]\n",
      "[2899]\n",
      "[2900]\n",
      "[2901]\n",
      "[2902]\n",
      "[2903]\n",
      "[2904]\n",
      "[2905]\n",
      "[2906]\n",
      "[2907]\n",
      "[2908]\n",
      "[2909]\n",
      "[2910]\n",
      "[2911]\n",
      "[2912]\n",
      "[2913]\n",
      "[2914]\n",
      "[2915]\n",
      "[2916]\n",
      "[2917]\n",
      "[2918]\n",
      "[2919]\n",
      "[2920]\n",
      "[2921]\n",
      "[2922]\n",
      "[2923]\n",
      "[2924]\n",
      "[2925]\n",
      "[2926]\n",
      "[2927]\n",
      "[2928]\n",
      "[2929]\n",
      "[2930]\n",
      "[2931]\n",
      "[2932]\n",
      "[2933]\n",
      "[2934]\n",
      "[2935]\n",
      "[2936]\n",
      "[2937]\n",
      "[2938]\n",
      "[2939]\n",
      "[2940]\n",
      "[2941]\n",
      "[2942]\n",
      "[2943]\n",
      "[2944]\n",
      "[2945]\n",
      "[2946]\n",
      "[2947]\n",
      "[2948]\n",
      "[2949]\n",
      "[2950]\n",
      "[2951]\n",
      "[2952]\n",
      "[2953]\n",
      "[2954]\n",
      "[2955]\n",
      "[2956]\n",
      "[2957]\n",
      "[2958]\n",
      "[2959]\n",
      "[2960]\n",
      "[2961]\n",
      "[2962]\n",
      "[2963]\n",
      "[2964]\n",
      "[2965]\n",
      "[2966]\n",
      "[2967]\n",
      "[2968]\n",
      "[2969]\n",
      "[2970]\n",
      "[2971]\n",
      "[2972]\n",
      "[2973]\n",
      "[2974]\n",
      "[2975]\n",
      "[2976]\n",
      "[2977]\n",
      "[2978]\n",
      "[2979]\n",
      "[2980]\n",
      "[2981]\n",
      "[2982]\n",
      "[2983]\n",
      "[2984]\n",
      "[2985]\n",
      "[2986]\n",
      "[2987]\n",
      "[2988]\n",
      "[2989]\n",
      "[2990]\n",
      "[2991]\n",
      "[2992]\n",
      "[2993]\n",
      "[2994]\n",
      "[2995]\n",
      "[2996]\n",
      "[2997]\n",
      "[2998]\n",
      "[2999]\n",
      "[3000]\n",
      "[3001]\n",
      "[3002]\n",
      "[3003]\n",
      "[3004]\n",
      "[3005]\n",
      "[3006]\n",
      "[3007]\n",
      "[3008]\n",
      "[3009]\n",
      "[3010]\n",
      "[3011]\n",
      "[3012]\n",
      "[3013]\n",
      "[3014]\n",
      "[3015]\n",
      "[3016]\n",
      "[3017]\n",
      "[3018]\n",
      "[3019]\n",
      "[3020]\n",
      "[3021]\n",
      "[3022]\n",
      "[3023]\n",
      "[3024]\n",
      "[3025]\n",
      "[3027]\n",
      "[3028]\n",
      "[3029]\n",
      "[3030]\n",
      "[3031]\n",
      "[3032]\n",
      "[3033]\n",
      "[3034]\n",
      "[3035]\n",
      "[3036]\n",
      "[3037]\n",
      "[3038]\n",
      "[3039]\n",
      "[3040]\n",
      "[3041]\n",
      "[3042]\n",
      "[3043]\n",
      "[3044]\n",
      "[3045]\n",
      "[3046]\n",
      "[3047]\n",
      "[3048]\n",
      "[3049]\n",
      "[3050]\n",
      "[3051]\n",
      "[3052]\n",
      "[3053]\n",
      "[3054]\n",
      "[3055]\n",
      "[3056]\n",
      "[3057]\n",
      "[3058]\n",
      "[3059]\n",
      "[3060]\n",
      "[3061]\n",
      "[3062]\n",
      "[3063]\n",
      "[3064]\n",
      "[3065]\n",
      "[3066]\n",
      "[3067]\n",
      "[3068]\n",
      "[3069]\n",
      "[3070]\n",
      "[3071]\n",
      "[3072]\n",
      "[3073]\n",
      "[3074]\n",
      "[3075]\n",
      "[3076]\n",
      "[3077]\n",
      "[3078]\n",
      "[3079]\n",
      "[3080]\n",
      "[3081]\n",
      "[3082]\n",
      "[3083]\n",
      "[3084]\n",
      "[3085]\n",
      "[3086]\n",
      "[3087]\n",
      "[3088]\n",
      "[3089]\n",
      "[3090]\n",
      "[3091]\n",
      "[3092]\n",
      "[3093]\n",
      "[3094]\n",
      "[3095]\n",
      "[3096]\n",
      "[3097]\n",
      "[3098]\n",
      "[3099]\n",
      "[3100]\n",
      "[3101]\n",
      "[3102]\n",
      "[3103]\n",
      "[3104]\n",
      "[3105]\n",
      "[3106]\n",
      "[3107]\n",
      "[3108]\n",
      "[3109]\n",
      "[3110]\n",
      "[3111]\n",
      "[3112]\n",
      "[3113]\n",
      "[3114]\n",
      "[3115]\n",
      "[3116]\n",
      "[3117]\n",
      "[3118]\n",
      "[3119]\n",
      "[3120]\n",
      "[3121]\n",
      "[3122]\n",
      "[3123]\n",
      "[3124]\n",
      "[3125]\n",
      "[3126]\n",
      "[3127]\n",
      "[3128]\n",
      "[3129]\n",
      "[3130]\n",
      "[3131]\n",
      "[3132]\n",
      "[3133]\n",
      "[3134]\n",
      "[3135]\n",
      "[3136]\n",
      "[3137]\n",
      "[3138]\n",
      "[3139]\n",
      "[3140]\n",
      "[3141]\n",
      "[3142]\n",
      "[3143]\n",
      "[3144]\n",
      "[3145]\n",
      "[3146]\n",
      "[3147]\n",
      "[3148]\n",
      "[3149]\n",
      "[3150]\n",
      "[3151]\n",
      "[3152]\n",
      "[3153]\n",
      "[3154]\n",
      "[3155]\n",
      "[3156]\n",
      "[3157]\n",
      "[3158]\n",
      "[3159]\n",
      "[3160]\n",
      "[3161]\n",
      "[3162]\n",
      "[3163]\n",
      "[3164]\n",
      "[3165]\n",
      "[3166]\n",
      "[3167]\n",
      "[3168]\n",
      "[3169]\n",
      "[3170]\n",
      "[3171]\n",
      "[3172]\n",
      "[3173]\n",
      "[3174]\n",
      "[3175]\n",
      "[3176]\n",
      "[3177]\n",
      "[3178]\n",
      "[3179]\n",
      "[3180]\n",
      "[3181]\n",
      "[3182]\n",
      "[3183]\n",
      "[3184]\n",
      "[3185]\n",
      "[3186]\n",
      "[3187]\n",
      "[3188]\n",
      "[3189]\n",
      "[3190]\n",
      "[3191]\n",
      "[3192]\n",
      "[3193]\n",
      "[3194]\n",
      "[3195]\n",
      "[3196]\n",
      "[3197]\n",
      "[3198]\n",
      "[3199]\n",
      "[3200]\n",
      "[3201]\n",
      "[3202]\n",
      "[3203]\n",
      "[3204]\n",
      "[3205]\n",
      "[3206]\n",
      "[3207]\n",
      "[3208]\n",
      "[3209]\n",
      "[3210]\n",
      "[3211]\n",
      "[3212]\n",
      "[3213]\n",
      "[3214]\n",
      "[3215]\n",
      "[3216]\n",
      "[3217]\n",
      "[3218]\n",
      "[3219]\n",
      "[3220]\n",
      "[3221]\n",
      "[3222]\n",
      "[3223]\n",
      "[3224]\n",
      "[3225]\n",
      "[3226]\n",
      "[3227]\n",
      "[3228]\n",
      "[3229]\n",
      "[3230]\n",
      "[3231]\n",
      "[3232]\n",
      "[3233]\n",
      "[3234]\n",
      "[3235]\n",
      "[3236]\n",
      "[3237]\n",
      "[3238]\n",
      "[3239]\n",
      "[3240]\n",
      "[3241]\n",
      "[3242]\n",
      "[3243]\n",
      "[3244]\n",
      "[3245]\n",
      "[3246]\n",
      "[3247]\n",
      "[3248]\n",
      "[3249]\n",
      "[3250]\n",
      "[3251]\n",
      "[3252]\n",
      "[3253]\n",
      "[3254]\n",
      "[3255]\n",
      "[3256]\n",
      "[3257]\n",
      "[3258]\n",
      "[3259]\n",
      "[3260]\n",
      "[3261]\n",
      "[3262]\n",
      "[3263]\n",
      "[3264]\n",
      "[3265]\n",
      "[3266]\n",
      "[3267]\n",
      "[3268]\n",
      "[3269]\n",
      "[3270]\n",
      "[3271]\n",
      "[3272]\n",
      "[3273]\n",
      "[3274]\n",
      "[3275]\n",
      "[3276]\n",
      "[3277]\n",
      "[3278]\n",
      "[3279]\n",
      "[3280]\n",
      "[3281]\n",
      "[3282]\n",
      "[3283]\n",
      "[3284]\n",
      "[3285]\n",
      "[3286]\n",
      "[3287]\n",
      "[3288]\n",
      "[3289]\n",
      "[3290]\n",
      "[3291]\n",
      "[3292]\n",
      "[3293]\n",
      "[3294]\n",
      "[3295]\n",
      "[3296]\n",
      "[3297]\n",
      "[3298]\n",
      "[3299]\n",
      "[3300]\n",
      "[3301]\n",
      "[3302]\n",
      "[3303]\n",
      "[3304]\n",
      "[3305]\n",
      "[3306]\n",
      "[3307]\n",
      "[3308]\n",
      "[3309]\n",
      "[3310]\n",
      "[3311]\n",
      "[3312]\n",
      "[3313]\n",
      "[3314]\n",
      "[3315]\n",
      "[3316]\n",
      "[3317]\n",
      "[3318]\n",
      "[3319]\n",
      "[3320]\n",
      "[3321]\n",
      "[3322]\n",
      "[3323]\n",
      "[3324]\n",
      "[3325]\n",
      "[3326]\n",
      "[3327]\n",
      "[3328]\n",
      "[3329]\n",
      "[3330]\n",
      "[3331]\n",
      "[3332]\n",
      "[3333]\n",
      "[3334]\n",
      "[3335]\n",
      "[3336]\n",
      "[3337]\n",
      "[3338]\n",
      "[3339]\n",
      "[3340]\n",
      "[3341]\n",
      "[3342]\n",
      "[3343]\n",
      "[3344]\n",
      "[3345]\n",
      "[3346]\n",
      "[3347]\n",
      "[3348]\n",
      "[3349]\n",
      "[3350]\n",
      "[3351]\n",
      "[3352]\n",
      "[3353]\n",
      "[3354]\n",
      "[3356]\n",
      "[3357]\n",
      "[3358]\n",
      "[3359]\n",
      "[3360]\n",
      "[3361]\n",
      "[3362]\n",
      "[3363]\n",
      "[3364]\n",
      "[3366]\n",
      "[3367]\n",
      "[3368]\n",
      "[3369]\n",
      "[3370]\n",
      "[3371]\n",
      "[3372]\n",
      "[3373]\n",
      "[3374]\n",
      "[3375]\n",
      "[3376]\n",
      "[3377]\n",
      "[3378]\n",
      "[3379]\n",
      "[3380]\n",
      "[3381]\n",
      "[3382]\n",
      "[3383]\n",
      "[3384]\n",
      "[3385]\n",
      "[3386]\n",
      "[3387]\n",
      "[3388]\n",
      "[3389]\n",
      "[3390]\n",
      "[3391]\n",
      "[3392]\n",
      "[3393]\n",
      "[3394]\n",
      "[3395]\n",
      "[3396]\n",
      "[3397]\n",
      "[3398]\n",
      "[3399]\n",
      "[3400]\n",
      "[3401]\n",
      "[3402]\n",
      "[3403]\n",
      "[3404]\n",
      "[3405]\n",
      "[3406]\n",
      "[3407]\n",
      "[3408]\n",
      "[3409]\n",
      "[3410]\n",
      "[3411]\n",
      "[3412]\n",
      "[3413]\n",
      "[3414]\n",
      "[3416]\n",
      "[3417]\n",
      "[3418]\n",
      "[3419]\n",
      "[3420]\n",
      "[3421]\n",
      "[3422]\n",
      "[3423]\n",
      "[3424]\n",
      "[3425]\n",
      "[3426]\n",
      "[3427]\n",
      "[3428]\n",
      "[3429]\n",
      "[3430]\n",
      "[3431]\n",
      "[3432]\n",
      "[3433]\n",
      "[3434]\n",
      "[3435]\n",
      "[3436]\n",
      "[3437]\n",
      "[3438]\n",
      "[3439]\n",
      "[3440]\n",
      "[3441]\n",
      "[3442]\n",
      "[3443]\n",
      "[3444]\n",
      "[3445]\n",
      "[3446]\n",
      "[3447]\n",
      "[3448]\n",
      "[3449]\n",
      "[3450]\n",
      "[3451]\n",
      "[3452]\n",
      "[3453]\n",
      "[3454]\n",
      "[3455]\n",
      "[3456]\n",
      "[3457]\n",
      "[3458]\n",
      "[3459]\n",
      "[3460]\n",
      "[3461]\n",
      "[3462]\n",
      "[3464]\n",
      "[3465]\n",
      "[3466]\n",
      "[3467]\n",
      "[3468]\n",
      "[3469]\n",
      "[3470]\n",
      "[3471]\n",
      "[3472]\n",
      "[3473]\n",
      "[3474]\n",
      "[3475]\n",
      "[3476]\n",
      "[3477]\n",
      "[3478]\n",
      "[3479]\n",
      "[3480]\n",
      "[3482]\n",
      "[3483]\n",
      "[3484]\n",
      "[3485]\n",
      "[3486]\n",
      "[3487]\n",
      "[3488]\n",
      "[3489]\n",
      "[3490]\n",
      "[3491]\n",
      "[3492]\n",
      "[3493]\n",
      "[3494]\n",
      "[3495]\n",
      "[3496]\n",
      "[3497]\n",
      "[3498]\n",
      "[3499]\n",
      "[3500]\n",
      "[3501]\n",
      "[3502]\n",
      "[3503]\n",
      "[3504]\n",
      "[3505]\n",
      "[3506]\n",
      "[3507]\n",
      "[3508]\n",
      "[3509]\n",
      "[3510]\n",
      "[3511]\n",
      "[3512]\n",
      "[3513]\n",
      "[3514]\n",
      "[3515]\n",
      "[3516]\n",
      "[3517]\n",
      "[3518]\n",
      "[3519]\n",
      "[3520]\n",
      "[3521]\n",
      "[3522]\n",
      "[3523]\n",
      "[3524]\n",
      "[3525]\n",
      "[3526]\n",
      "[3527]\n",
      "[3528]\n",
      "[3529]\n",
      "[3530]\n",
      "[3532]\n",
      "[3533]\n",
      "[3534]\n",
      "[3535]\n",
      "[3536]\n",
      "[3537]\n",
      "[3538]\n",
      "[3539]\n",
      "[3540]\n",
      "[3541]\n",
      "[3542]\n",
      "[3543]\n",
      "[3544]\n",
      "[3545]\n",
      "[3546]\n",
      "[3547]\n",
      "[3548]\n",
      "[3549]\n",
      "[3550]\n",
      "[3551]\n",
      "[3552]\n",
      "[3553]\n",
      "[3554]\n",
      "[3555]\n",
      "[3556]\n",
      "[3557]\n",
      "[3558]\n",
      "[3559]\n",
      "[3560]\n",
      "[3561]\n",
      "[3562]\n",
      "[3563]\n",
      "[3564]\n",
      "[3565]\n",
      "[3566]\n",
      "[3567]\n",
      "[3568]\n",
      "[3569]\n",
      "[3570]\n",
      "[3571]\n",
      "[3572]\n",
      "[3573]\n",
      "[3574]\n",
      "[3575]\n",
      "[3576]\n",
      "[3577]\n",
      "[3578]\n",
      "[3579]\n",
      "[3580]\n",
      "[3581]\n",
      "[3582]\n",
      "[3583]\n",
      "[3584]\n",
      "[3585]\n",
      "[3586]\n",
      "[3587]\n",
      "[3588]\n",
      "[3589]\n",
      "[3590]\n",
      "[3591]\n",
      "[3592]\n",
      "[3593]\n",
      "[3594]\n",
      "[3595]\n",
      "[3596]\n",
      "[3597]\n",
      "[3598]\n",
      "[3599]\n",
      "[3600]\n",
      "[3601]\n",
      "[3602]\n",
      "[3603]\n",
      "[3604]\n",
      "[3605]\n",
      "[3606]\n",
      "[3607]\n",
      "[3608]\n",
      "[3609]\n",
      "[3610]\n",
      "[3611]\n",
      "[3612]\n",
      "[3613]\n",
      "[3614]\n",
      "[3615]\n",
      "[3616]\n",
      "[3617]\n",
      "[3618]\n",
      "[3619]\n",
      "[3620]\n",
      "[3621]\n",
      "[3622]\n",
      "[3623]\n",
      "[3624]\n",
      "[3625]\n",
      "[3626]\n",
      "[3627]\n",
      "[3628]\n",
      "[3629]\n",
      "[3630]\n",
      "[3631]\n",
      "[3632]\n",
      "[3633]\n",
      "[3634]\n",
      "[3635]\n",
      "[3636]\n",
      "[3637]\n",
      "[3638]\n",
      "[3639]\n",
      "[3640]\n",
      "[3641]\n",
      "[3642]\n",
      "[3643]\n",
      "[3644]\n",
      "[3645]\n",
      "[3646]\n",
      "[3647]\n",
      "[3648]\n",
      "[3649]\n",
      "[3650]\n",
      "[3651]\n",
      "[3652]\n",
      "[3653]\n",
      "[3654]\n",
      "[3655]\n",
      "[3656]\n",
      "[3657]\n",
      "[3658]\n",
      "[3661]\n",
      "[3662]\n",
      "[3663]\n",
      "[3664]\n",
      "[3665]\n",
      "[3666]\n",
      "[3667]\n",
      "[3668]\n",
      "[3669]\n",
      "[3670]\n",
      "[3671]\n",
      "[3672]\n",
      "[3673]\n",
      "[3674]\n",
      "[3675]\n",
      "[3676]\n",
      "[3677]\n",
      "[3678]\n",
      "[3679]\n",
      "[3680]\n",
      "[3681]\n",
      "[3682]\n",
      "[3683]\n",
      "[3684]\n",
      "[3685]\n",
      "[3686]\n",
      "[3687]\n",
      "[3688]\n",
      "[3689]\n",
      "[3690]\n",
      "[3691]\n",
      "[3692]\n",
      "[3693]\n",
      "[3694]\n",
      "[3695]\n",
      "[3696]\n",
      "[3697]\n",
      "[3698]\n",
      "[3699]\n",
      "[3700]\n",
      "[3701]\n",
      "[3702]\n",
      "[3703]\n",
      "[3704]\n",
      "[3705]\n",
      "[3706]\n",
      "[3707]\n",
      "[3708]\n",
      "[3709]\n",
      "[3710]\n",
      "[3711]\n",
      "[3712]\n",
      "[3713]\n",
      "[3714]\n",
      "[3715]\n",
      "[3716]\n",
      "[3717]\n",
      "[3718]\n",
      "[3719]\n",
      "[3720]\n",
      "[3721]\n",
      "[3722]\n",
      "[3723]\n",
      "[3724]\n",
      "[3725]\n",
      "[3726]\n",
      "[3727]\n",
      "[3728]\n",
      "[3729]\n",
      "[3730]\n",
      "[3731]\n",
      "[3732]\n",
      "[3733]\n",
      "[3734]\n",
      "[3735]\n",
      "[3736]\n",
      "[3737]\n",
      "[3738]\n",
      "[3739]\n",
      "[3740]\n",
      "[3741]\n",
      "[3742]\n",
      "[3743]\n",
      "[3744]\n",
      "[3745]\n",
      "[3746]\n",
      "[3747]\n",
      "[3748]\n",
      "[3749]\n",
      "[3750]\n",
      "[3751]\n",
      "[3752]\n",
      "[3753]\n",
      "[3754]\n",
      "[3755]\n",
      "[3756]\n",
      "[3757]\n",
      "[3758]\n",
      "[3759]\n",
      "[3760]\n",
      "[3761]\n",
      "[3762]\n",
      "[3763]\n",
      "[3764]\n",
      "[3765]\n",
      "[3766]\n",
      "[3767]\n",
      "[3768]\n",
      "[3769]\n",
      "[3770]\n",
      "[3771]\n",
      "[3772]\n",
      "[3773]\n",
      "[3774]\n",
      "[3775]\n",
      "[3776]\n",
      "[3777]\n",
      "[3778]\n",
      "[3779]\n",
      "[3780]\n",
      "[3781]\n",
      "[3782]\n",
      "[3783]\n",
      "[3784]\n",
      "[3785]\n",
      "[3786]\n",
      "[3787]\n",
      "[3788]\n",
      "[3789]\n",
      "[3790]\n",
      "[3791]\n",
      "[3792]\n",
      "[3793]\n",
      "[3794]\n",
      "[3795]\n",
      "[3796]\n",
      "[3797]\n",
      "[3798]\n",
      "[3799]\n",
      "[3800]\n",
      "[3801]\n",
      "[3802]\n",
      "[3803]\n",
      "[3804]\n",
      "[3805]\n",
      "[3806]\n",
      "[3807]\n",
      "[3808]\n",
      "[3809]\n",
      "[3810]\n",
      "[3811]\n",
      "[3812]\n",
      "[3813]\n",
      "[3815]\n",
      "[3816]\n",
      "[3817]\n",
      "[3818]\n",
      "[3819]\n",
      "[3820]\n",
      "[3821]\n",
      "[3822]\n",
      "[3823]\n",
      "[3824]\n",
      "[3825]\n",
      "[3826]\n",
      "[3827]\n",
      "[3828]\n",
      "[3829]\n",
      "[3830]\n",
      "[3831]\n",
      "[3832]\n",
      "[3833]\n",
      "[3834]\n",
      "[3835]\n",
      "[3836]\n",
      "[3837]\n",
      "[3838]\n",
      "[3839]\n",
      "[3840]\n",
      "[3842]\n",
      "[3843]\n",
      "[3844]\n",
      "[3845]\n",
      "[3846]\n",
      "[3847]\n",
      "[3848]\n",
      "[3849]\n",
      "[3850]\n",
      "[3851]\n",
      "[3852]\n",
      "[3853]\n",
      "[3854]\n",
      "[3855]\n",
      "[3856]\n",
      "[3857]\n",
      "[3858]\n",
      "[3859]\n",
      "[3860]\n",
      "[3861]\n",
      "[3862]\n",
      "[3863]\n",
      "[3864]\n",
      "[3865]\n",
      "[3866]\n",
      "[3867]\n",
      "[3868]\n",
      "[3869]\n",
      "[3870]\n",
      "[3871]\n",
      "[3872]\n",
      "[3873]\n",
      "[3874]\n",
      "[3875]\n",
      "[3876]\n",
      "[3877]\n",
      "[3878]\n",
      "[3879]\n",
      "[3880]\n",
      "[3881]\n",
      "[3882]\n",
      "[3883]\n",
      "[3884]\n",
      "[3885]\n",
      "[3886]\n",
      "[3887]\n",
      "[3888]\n",
      "[3889]\n",
      "[3890]\n",
      "[3891]\n",
      "[3892]\n",
      "[3893]\n",
      "[3894]\n",
      "[3895]\n",
      "[3896]\n",
      "[3897]\n",
      "[3898]\n",
      "[3899]\n",
      "[3900]\n",
      "[3901]\n",
      "[3902]\n",
      "[3903]\n",
      "[3904]\n",
      "[3905]\n",
      "[3906]\n",
      "[3907]\n",
      "[3908]\n",
      "[3909]\n",
      "[3910]\n",
      "[3911]\n",
      "[3912]\n",
      "[3913]\n",
      "[3914]\n",
      "[3915]\n",
      "[3916]\n",
      "[3917]\n",
      "[3918]\n",
      "[3919]\n",
      "[3920]\n",
      "[3921]\n",
      "[3922]\n",
      "[3923]\n",
      "[3924]\n",
      "[3925]\n",
      "[3926]\n",
      "[3927]\n",
      "[3928]\n",
      "[3929]\n",
      "[3930]\n",
      "[3931]\n",
      "[3932]\n",
      "[3933]\n",
      "[3935]\n",
      "[3936]\n",
      "[3937]\n",
      "[3938]\n",
      "[3939]\n",
      "[3940]\n",
      "[3941]\n",
      "[3942]\n",
      "[3943]\n",
      "[3944]\n",
      "[3945]\n",
      "[3946]\n",
      "[3947]\n",
      "[3948]\n",
      "[3949]\n",
      "[3950]\n",
      "[3951]\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3676/3608358714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muserhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mdatas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_process_for_placeholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m    \u001b[1;31m# print(datas)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3676/1038374200.py\u001b[0m in \u001b[0;36mdata_process_for_placeholder\u001b[1;34m(self, user_set)\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# 0,kk=0이면 (0+0+1,0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                     \u001b[1;31m#[1,0]....[6,0..]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                     \u001b[0mt_indice\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msec_cnt_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msec_cnt_x\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m               \u001b[1;31m#  for t in range(len(t_indice)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m               \u001b[1;31m#      print(t_indice[t])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3676/1038374200.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# 0,kk=0이면 (0+0+1,0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                     \u001b[1;31m#[1,0]....[6,0..]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                     \u001b[0mt_indice\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msec_cnt_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msec_cnt_x\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m               \u001b[1;31m#  for t in range(len(t_indice)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m               \u001b[1;31m#      print(t_indice[t])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tmpmovieset=movieset\n",
    "mkrandhistory=[[[3],[[2,3123],[1,2,3]],[3123,3]]]\n",
    "\n",
    "u=[[0],[[idx] for idx,x in enumerate(tmpmovieset) if x==1 ],[idx for idx,x in enumerate(tmpmovieset) if x==1]]\n",
    "for uu in u:\n",
    "    for uuu in uu[1]:\n",
    "        print(uuu)\n",
    "print(\")\")\n",
    "sa=Dataset(train_data=False,userhistory=u)\n",
    "datas=sa.data_process_for_placeholder([0])       \n",
    "   # print(datas)\n",
    "with torch.no_grad():\n",
    "    reward,disp_history_feature,user= model.embedding(datas, is_train=True)\n",
    "    #데이터셋 리스트를 만들어서 model의 disp값을 받아와야할듯?\n",
    "disp=disp.float()\n",
    "print(disp_history_feature.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
